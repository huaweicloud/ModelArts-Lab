# 物体检测算法综述
## 1. 引论
计算机视觉领域，存在一系列研究问题，其中比较基础的包括图像分类、物体检测、实例分割、语义分割。图像分割任务目的在于识别给定图像的物体语义类别，比如是一只猫还是一只狗。物体检测任务更进一步，不仅需要检测出物体的类别，还要获得物体位置的边界框信息，同时需要对一个图像检测多个类别实例。语义分割在上述基础上更进一步，对于物体位置的信息需要更细粒度的表述，不满足于通过BBox表示位置，而是要具体到每个像素所对应的类别，但是并不需要区分同一类别的不同实例。更为细致的则是实例分割，需要区分一个像素属于同一类别的哪一实例，比如某一像素属于猫1，另一像素属于猫2。<br>
在深度学习应用到object detection之前，物体检测技术所采用的pipeline基本上是三段论，即候选区域生成、特征向量提取、区域分类。第一步，候选区域生成的目的在于搜索图像中可能含有物体的区域即ROI，一个非常符合直觉的方式是用滑移窗口来检测整个图像，为了捕获多尺度的信息，通常采用图像缩放和多尺度滑移窗。第二步，在图像的每个区域借助滑移窗通过一些低层视觉特征(如SIFT，Haar，HOG，SURF)抽取出固定长度的特征向量以捕获图像对应区域的语义信息。第三步，对于每一区域通过区域分类器赋予类别标签，通常采用one-vs-all SVM进行分类。<br>
然而，传统的方法由于存在一些局限性，包括大量候选区域的计算冗余性、分类过程的假阳性、窗口尺度人为设计难以完美匹配不同物体、基于低层的视觉信息设计的特征难以捕获复杂语义信息、整个pipeline分割独立难以获得全局最优解等一系列问题，在2008~2012年的Pascal VOC物体检测上进展艰难，逐渐转变为了基于深度学习框架。其基本思想是通过卷积神经网络直接从原始像素生成层级的语义信息代替手工特征提取，借助大算力和大数据获得更好的特征表达。<br>
## 2. 物体检测的正式描述
物体检测主要包含两个任务，分别是识别(即物体分类)和定位(即位置回归)。物体检测器需要将特定类别的物体以准确的位置和正确的标签信息从背景中识别出来。广义来讲，针对边界框BBox定位的Object Detection和基于像素Mask定位的Semantic Segmentation和Instance Segmentation都属于物体检测的范畴。狭义上，只限定为Object Detection，然而，其实在技术上具有很大程度的相似性。<br>
下面给出物体检测的数学形式：<br>
对于给定带标注的图像集合![f1](https://user-images.githubusercontent.com/30947306/63493345-ffd5b680-c4ed-11e9-86a5-d9fc4012fc7f.png)，其中标注形式如下![f2](https://user-images.githubusercontent.com/30947306/63493352-02381080-c4ee-11e9-97bd-bd9ff8f27dbd.png)，其中![f3](https://user-images.githubusercontent.com/30947306/63493353-02d0a700-c4ee-11e9-82ea-73e9c3f5953d.png)，包括![f4](https://user-images.githubusercontent.com/30947306/63493662-a621bc00-c4ee-11e9-9e61-35f4d23391bd.png)个物体，![f5](https://user-images.githubusercontent.com/30947306/63493665-a7eb7f80-c4ee-11e9-85ba-c530b5dfb8e7.png)为第i张图![f6](https://user-images.githubusercontent.com/30947306/63493732-c81b3e80-c4ee-11e9-9fab-34e0460f18c1.png)
中的第j个物体的BBox或是像素Mask。检测器是为了获得![f7](https://user-images.githubusercontent.com/30947306/63493360-049a6a80-c4ee-11e9-8dac-a6dacfa7d40a.png)，即在给定图像![f6](https://user-images.githubusercontent.com/30947306/63493732-c81b3e80-c4ee-11e9-9fab-34e0460f18c1.png)的条件下给出图像中每个检测到物体的类别和位置信息。
## 3. 物体检测的主要范式
目前主要物体检测的范式包含two-stage detector和one-stage detector。对于two-stage detector的范式而言，分为两个阶段，第一个阶段生成稀疏的候选区域，第二个阶段通过卷积神经网络对于生成的候选区域进行特征编码，然后进行物体类别分类。对于one-stage detector的范式而言，没有单独的候选区域生成过程，而是将每个位置都认为是可能存在物体的区域，然后将每个ROI分类为背景或是目标。一般来讲，two-stage detector范式在准确率上有优势，而one-stage detector在推理速度上具有优势，实时性高。
### 3.1 Two-Stage Detector
Two-stage detector范式将整个检测任务分解为两个阶段，主要包括(1)候选区域生成(2)对候选区域进行预测。在候选区域生成阶段，检测器将识别出图像中潜在有可能有物体的区域。其核心想法是以较高的召回率提出候选区域使得图片中所有的物体至少被包含在一个候选区域内。在第二阶段内，采用深度学习的模型对于候选区域进行分类，贴上正确的类别标签，类别标签包括背景和所有的物体类别。除此之外，还会对于生成的候选区域的原始位置信息进行修正调整。下面介绍一些主要的two-stage物体检测器。<br>
R-CNN，2014年Grishick提出，是two-stage detector的先驱。相较于传统框架的SegDPM，R-CNN一下子在Pascal VOC2010数据集上，提升了超过10% mAP，一骑绝尘。R-CNN的pipeline主要包含三个部件：1)候选区生成2)特征抽取3)区域分类。RCNN的架构如下图所示。对于每一张图像，R-CNN通过启发式算法Selective Search生成了约2000个稀疏的候选区。SS算法被设计用来拒绝那些可以被轻易识别为背景的区域。紧接着，每个候选区域被截取出来并resize为固定大小送入深度卷积网络编码为固定维度的特征向量，并将之作为one-vs-all SVM分类器的输入。最后，BBox回归器，利用抽取出的特征作为输入对于检测框进行回归以获得紧致的边界框。在trick方面，卷积权重采用ImageNet预训练权重，只将最后的全连接层进行初始化以适应物体检测。然而，R-CNN面临着一些缺陷，包括1）每个候选区的特征由一个深度神经网络分别进行抽取，导致大量重复计算，计算时间长2）pipeline中的三步相对独立，难以进行端到端的优化获得全局最优解3）SS算法依赖于低层的视觉特征，难以从复杂语义抽取高质量的候选区。另外，由于SS算法原因难以采用GPU加速。<br>
![fig1](https://user-images.githubusercontent.com/30947306/63493827-031d7200-c4ef-11e9-98d0-44c191a92688.png)<br>
针对R-CNN的缺点，何凯明提出了SPP-net，相比于R-CNN截取出候选区并独立送入CNN，SPP-net直接通过深度卷积网络对整幅图像计算特征图，并通过空间金字塔pooling(SPP)层从特征图上抽取固定长度的特征向量。SPP-Net的整体架构如下图所示。SPP层将整个特征图划分为NxN的网格，对于每个网格进行池化操作以生成特征向量，由不同N下抽取出的特征向量，拼接形成对于区域的表示，抽取出的特征进一步送入区域分类器和BBox回归器。相比于R-CNN， SPP-Net能够对于图像不同长宽比和尺度的区域进行处理而无需resize，避免了信息loss和几何扭曲。但是SPP-net仍然不是一个end-to-end解决方案，难以获得全局最优。另一方面，SPP层难以通过BP算法传递梯度信息，导致SPP层之前的参数都被固定，大幅限制了框架的学习能力。<br>
![fig2](https://user-images.githubusercontent.com/30947306/63493937-3fe96900-c4ef-11e9-978d-587b38815dd6.png)<br>
为了解决这些问题，Girshick提出了Fast RCNN。Fast RCNN网络架构如下图所示。Fast RCNN同样对于整幅图像计算特征图并在特征图上抽取固定维度的区域特征向量。但是与SPP-Net不同的是，Fast RCNN使用ROI池化来抽取区域特征。ROI池化是SPP层的一个特例，只采用单个尺度N对候选区进行分割，从而使得BP算法能够进行。另一方面，在特征抽取后在进入回归器和分类器之前，将特征向量送入一系列全连接层。分类器致力于区分C+1个类别，包括背景1个，物体分类C个。回归器以4个实值参数来表示BBox，以改进BBox的位置信息。Fast RCNN中，特征抽取、区域分类、边界框回归等步骤实现了端对端的优化，同时不需要额外的缓存空间存储特征。<br>
![fig3](https://user-images.githubusercontent.com/30947306/63493938-411a9600-c4ef-11e9-879f-7356d0b35177.png)<br>
尽管Fast RCNN相比于SPP-Net获得了长足的进步，但是对于候选区的生成仍然依赖于Selective Search和Edge Boxes等传统算法，高度依赖于低层的视觉特征，而不是从数据中习得，Faster-RCNN的提出致力于解决这个问题。Faster RCNN的网络架构如下图所示。Faster RCNN 提出了RPN网络，RPN是一个全卷积网络，实现任意大小图像入，在特征图上生成一系列物体候选区域，通过一个nxn的滑移窗在特征图上滑移，获取每一位置的特征向量，其后特征向量输入两个并行的分支。一个分支为物体回归层，用以判断候选内是否有物体。另一分支为BBox回归层。最后将上述结果送入最后的分类与回归层进行物体的检测。通过RPN网络，将候选区的生成也采用数据驱动的方式实现，Faster RCNN在多个公开数据集上获得了SOTA的结果。<br>
![fig4](https://user-images.githubusercontent.com/30947306/63493941-424bc300-c4ef-11e9-95ae-82a858a6f598.png)<br>
然而，Faster-RCNN只在不同区域特征抽取上共享了计算，对于最后的区域分类步骤仍然是每个特征向量单独经过一系列FC层，这种模式在存在大量候选区的情况下造成了大量额外计算。Dai为了解决这个问题提出了R-FCN，共享区域分类步骤内的计算代价。R-FCN生成了位置敏感度图将不同类别之间的相对位置信息进行了编码，并进一步通过位置敏感ROI池化层抽取带有空间信息的区域特征。<br>
![fig5](https://user-images.githubusercontent.com/30947306/63493942-437cf000-c4ef-11e9-8e3f-e3ab55139d6f.png)<br>
### 3.2 One-Stage Detector
上一小结主要介绍了一下主要的两阶段检测器。区别于Two-Stage Detector算法将整个物体检测过程分割为两个部分即候选区生成和区域分类，One-Stage Detector并没有独立的候选区域生成阶段，而是将图像中所有位置都认为是潜在物体存在的位置，并且将每个ROI区域分类为前景或是物体。<br>
YOLO是One-Stage Detector经典结构，其网络结构如下图所示。YOLO将物体检测作为一个回归问题，并将整个图像空间上分成固定数量的网格单元。每个网格单元都做为候选区检测是否出现了一个或多个物体。在最初版本的实现中，每个单元认为是包含了至多两个物体的中心。对于每个单元，综合该位置是否有物体，边界框的坐标和尺寸、物体分类等信息进行预测。借助小心设计的轻量级框架，YOLO可以达到45FPS以上的实时预测特性。然而，YOLO也存在一些问题，包括1）对于每个位置至多只能检测两个物体，对于小物体和高密度物体的检测存在困难2）只利用最后的特征图进行预测，难以应对多尺度和不同长宽比的需求。上述问题在YOLO2中，进行了改进。YOLO2中采用更强的backbone net来捕获更精细的信息，同时受SSD启发，借助k-mean距离方法从训练数据中定义先验的锚框替代原先认为指定的方法。同时，加入了BN层，和多尺度训练技巧，在SSD后再次获得SOTA。<br>
![fig6](https://user-images.githubusercontent.com/30947306/63493943-437cf000-c4ef-11e9-8901-be85d7532cf3.png)<br>
为了解决YOLO的这些局限性，2016年Liu提出了SSD框架。SSD网络架构如下图所示。SSD架构同样将图像划分为网格，但是对于每个网格单元，生成了一系列不同尺度和长框比的锚点框来离散输出空间的边界框。每个锚点框由回归器生成的4个实值变量描述，并被分类器赋予了C+1个类别的分类概率。另外，SSD在多个特征图上预测物体，每个特征图负责不同尺度物体的检测。为了能够检测大物体和增大感受野，在SSD中的backbone网络中加入了额外的卷积层提取特征图。在训练技巧上为了避免大量负面的候选区主导训练梯度，采用难例挖掘，同时配合密集的数据增强来改善检测性能。<br>
![fig7](https://user-images.githubusercontent.com/30947306/63493944-44158680-c4ef-11e9-8e1b-67b4c1f4cece.png)<br>
但是，由于缺乏候选区域生成这一过程来过滤易于检测的负样例，前景和背景的类别不平衡是one-stage detector架构中的一个严重问题。RetinaNet的提出正是为了处理这样一种类别不均衡问题。RetinaNet的网络结构如下图所示。RetinaNet使用在交叉熵损失函数基础上改进得到的focal loss来抑制易于检测到的负样例的梯度。另外，RetinaNet使用FPN特征金字塔网络来在不同的特征图层检测多尺度物体，上述focal loss的改进相比于原有的难例挖掘技术在性能上有了极大提升。<br>
![fig8](https://user-images.githubusercontent.com/30947306/63493950-44ae1d00-c4ef-11e9-974b-b6845cbd8cec.png)<br>
上面的框架，无论是YOLO，SSD，还是RetinaNet都需要设计锚框来训练检测器。因此，便有学者提出了无锚点框的物体检测方法。核心思想是预测边界框的关键点，而不是试图将一个物体放入锚点框内。CornerNet是一个比较新的无锚框化的物体检测方法，直接预测物体的一对角点。CornerNet的网络架构如下图所示。对于每个特征图，预测heatmap、pair embedding和offsets。Heatmap计算输出预测角点信息，可以用维度为H x W的feature map，其中C表示目标的类别（没有背景图）这个特征图的每个channel都是一个mask，mask的范围是0~1。embeddings的主要作用是预测corner点做group，也就是说检测检测出的左上角和右下角是否是同一目标的。offsets：用来对预测框的位置进行微调，因为点映射到feature map时会有量化误差。由于无需手工设计anchor来匹配物体，CornerNet在MSCOCO数据集上的结果获得了显著提高。<br>
![fig9](https://user-images.githubusercontent.com/30947306/63493959-4aa3fe00-c4ef-11e9-9869-cfa9d1297a10.png)<br>
## 4. 附录
### 4.1 物体检测中的常见指标
**相关概念**

|符号                           | 含义 
|:-:                             | :-:
|TP(true positive)              |	实际是正例，预测为正例
|FP(false positive)             |	实际为负例，预测为正例
|TN(true negative)              |	实际为负例，预测为负例
|FN(false negative)             |	实际为正例，预测为负例
|TPR(true positive rate)        |	真正率又称为：灵敏度(sensitivity)
|TFR(true negative rate)	       | 真负率又称为：特指度(specificity)
|FNR(false negative rate)	      | 假负率又称为：虚警率(False Alarm)
|FPR(false positive rate)	      | 假正率
|R(recall)	                     | 检测正确正样例 / 所有正样例
|P(Precision)                   |	检测正确正样例 / 预测正样例
|IoU(Intersection over Union)   |	预测框与ground truth的交集和并集的比值，也被称为Jaccard指数
|准确率-召回率曲线（P-R曲线）     |	以召回率为横坐标，精确率为纵坐标，用不同的阀值，统计出一组不同阀值下的精确率和召回率。
|AP (average precision)         |	P-R曲线下的面积
|mAP (mean average precision)   |	多个类别AP的平均值
|ROC曲线                         |	用不同的阀值，统计出一组不同阀值下的TPR（真阳率）和FPR（假阳率）的关系
|AUC (Area Under Curve)         |	ROC曲线下的面积
|FPS (Frames Per Second)        |	每秒处理图像的帧数
|FLOPS                          |	每秒浮点运算次数、每秒峰值速度

**相关计算公式：<br>**
![formula1](https://user-images.githubusercontent.com/30947306/63494296-fbaa9880-c4ef-11e9-8f96-5d261df9c599.png)<br>
![formula2](https://user-images.githubusercontent.com/30947306/63494297-fbaa9880-c4ef-11e9-945c-b87cdc0a1a29.png)<br>
### 4.2 物体检测中的概念补充
**Selective Search**
- 使用一种过分割手段，将图像分割成小区域（1k~2k）
- 查看现有的小区域，按照合并规则合并可能性最高的相邻两个区域。重复，直到整张图像合并成一个区域
- 给出所有曾经存在过的区域，所谓候选区域

**Bounding Box（bbox）**
bbox是包含物体的最小矩形，目标物体应在最小矩形内部。输出一组(x,y,w,h)，其中x, y 代表bbox左上角（或其他固定点，可自定义；w, h 代表bbox的宽和高；每一组(x,y,w,h)可以唯一确定一个定位框。

**非极大值抑制(Non-Maximum Suppression/NMS)**
非极大值抑制就是把不是极大值的抑制掉，在物体检测上，就是对一个目标有多个标定框，使用极大值抑制算法滤掉多余的标定框。<br>
具体的方法是：先假设有多个矩形框，比如6个，根据分类器的类别分类概率做排序，假设从小到大属于某一物体的概率分别为A、B、C、D、E、F。
- 从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值
- 假设B、D与F的重叠度超过阈值，那么就扔掉B、D,并标记第一个矩形框F是我们保留下来的
- 从剩下的矩形框A、C、E中，选择概率最大的E,然后判断E与A、C的重叠度,重叠度大于一定的阈值,那么就扔掉；并标记E是我们保留下来的第二个矩形框。就这样一直重复,找到所有被保留下来的矩形框。

### 4.3 常见框架性能比较
精度上：RFCN的准确度是最高的。(VOC2007+voc2012数据集训练，mAP的计算方式是VOC2012)<br>
![fig10](https://user-images.githubusercontent.com/30947306/63494446-41fff780-c4f0-11e9-8818-944e0aa8c845.png)<br>
速度上：单步方法SSD，YOLO最快。(VOC2007+voc2012数据集训练，mAP的计算方式是VOC2012)<br>
![fig11](https://user-images.githubusercontent.com/30947306/63494448-41fff780-c4f0-11e9-92ca-f7352571568b.png)<br>
mAP上：RetinaNet占优(COCO数据集)<br>
![fig12](https://user-images.githubusercontent.com/30947306/63494449-42988e00-c4f0-11e9-967f-5a827c7e41a9.png)<br>
物体大小：对于大物体，SSD即使使用一个较弱的特征抽取器也可以获取较好的精确度。但在小物体上SSD的表现结果非常不好。(COCO数据集)<br>
![fig13](https://user-images.githubusercontent.com/30947306/63494453-43312480-c4f0-11e9-92c9-c41e00931465.png)<br>

 

