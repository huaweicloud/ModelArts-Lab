{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载h5py 模块\n",
    "* 利用h5py.File()函数读取h5文件，文件结构类似dict\n",
    "* 利用keys()函数获取File的所有键值\n",
    "* 获取File相应键值的value——dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.files.File"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('E:/Alibaba German AI Challenge/round1_test_a_20181109.h5','r')\n",
    "\n",
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sen1', 'sen2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.dataset.Dataset"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1 = f['sen1']\n",
    "sen2 = f['sen2']\n",
    "type(sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4838, 32, 32, 8), (4838, 32, 32, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1.shape,sen2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39632896"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dataset类型的属性\n",
    "* shape\n",
    "* size\n",
    "* dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4838, 32, 32, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "s1 = np.array(sen1)\n",
    "s1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.files.File"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('E:/Alibaba German AI Challenge/origin_DATA/validation.h5','r')\n",
    "\n",
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'sen1', 'sen2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24119, 32, 32, 8), (24119, 32, 32, 10), (24119, 17))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "s1 = np.array(f['sen1'])\n",
    "s2 = np.array(f['sen2'])\n",
    "y = np.array(f['label'])\n",
    "s1.shape,s2.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24119, 18432)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(0,s1.shape[0]):\n",
    "    temp1 = s1[i].flatten()\n",
    "    temp2 = s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    x.append(temp)\n",
    "x = np.array(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24119, 18449)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.hstack((x,y))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the h5 file\n",
      "The shape of data is  (24119, 18449)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f792ea199851>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"step %d, train accuracy %g\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession              baseline                 accuracy = 0.702                \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/validation.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "print('Get the h5 file')\n",
    "\n",
    "s1 = np.array(f['sen1'])\n",
    "s2 = np.array(f['sen2'])\n",
    "y = np.array(f['label'])\n",
    "\n",
    "x = []\n",
    "for i in range(0,s1.shape[0]):\n",
    "    temp1 = s1[i].flatten()\n",
    "    temp2 = s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    x.append(temp)\n",
    "x = np.array(x)\n",
    "\n",
    "data = np.hstack((x,y))\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "for i in range(64000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sen1', 'sen2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4838, 18432)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4838, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = np.zeros((test.shape[0],17))\n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  2, 13, ...,  8, 13, 16], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4838,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4838), Dimension(17)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot=tf.one_hot(P,17)\n",
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'one_hot_1:0' shape=(4838, 17) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_one_hot = sess.run(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4838, 17)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    15   16  \n",
       "0  0.0  1.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_one_hot = Pred_one_hot.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
       "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
       "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
       "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4838, 17)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('first_64k_vali_as_train.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 挖掘该套策略的全部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (24119, 18449)\n",
      "step 0, train accuracy 0.0862, train loss 3501.060547\n",
      "step 200, train accuracy 0.4461, train loss 799.105713\n",
      "step 400, train accuracy 0.4806, train loss 627.051941\n",
      "step 600, train accuracy 0.4695, train loss 622.606873\n",
      "step 800, train accuracy 0.4913, train loss 527.536499\n",
      "step 1000, train accuracy 0.4936, train loss 395.613281\n",
      "step 1200, train accuracy 0.5007, train loss 447.039459\n",
      "step 1400, train accuracy 0.5109, train loss 371.680420\n",
      "step 1600, train accuracy 0.5092, train loss 365.894165\n",
      "step 1800, train accuracy 0.5167, train loss 327.782471\n",
      "step 2000, train accuracy 0.5172, train loss 310.625122\n",
      "step 2200, train accuracy 0.5187, train loss 343.159912\n",
      "step 2400, train accuracy 0.5199, train loss 317.177368\n",
      "step 2600, train accuracy 0.5196, train loss 298.385986\n",
      "step 2800, train accuracy 0.5222, train loss 307.308716\n",
      "step 3000, train accuracy 0.5177, train loss 290.503601\n",
      "step 3200, train accuracy 0.5262, train loss 303.264313\n",
      "step 3400, train accuracy 0.5288, train loss 227.586380\n",
      "step 3600, train accuracy 0.5204, train loss 270.798492\n",
      "step 3800, train accuracy 0.5299, train loss 217.417526\n",
      "step 4000, train accuracy 0.5317, train loss 235.790359\n",
      "step 4200, train accuracy 0.5207, train loss 226.035950\n",
      "step 4400, train accuracy 0.5219, train loss 204.773453\n",
      "step 4600, train accuracy 0.5245, train loss 200.878784\n",
      "step 4800, train accuracy 0.5245, train loss 184.222748\n",
      "step 5000, train accuracy 0.5268, train loss 191.572906\n",
      "step 5200, train accuracy 0.5323, train loss 175.063370\n",
      "step 5400, train accuracy 0.5327, train loss 221.116333\n",
      "step 5600, train accuracy 0.5321, train loss 172.873306\n",
      "step 5800, train accuracy 0.53, train loss 165.271393\n",
      "step 6000, train accuracy 0.5316, train loss 198.159790\n",
      "step 6200, train accuracy 0.5321, train loss 159.430389\n",
      "step 6400, train accuracy 0.529, train loss 124.171799\n",
      "step 6600, train accuracy 0.5306, train loss 127.805420\n",
      "step 6800, train accuracy 0.527, train loss 198.097198\n",
      "step 7000, train accuracy 0.5283, train loss 106.226723\n",
      "step 7200, train accuracy 0.5277, train loss 148.603180\n",
      "step 7400, train accuracy 0.5323, train loss 140.792709\n",
      "step 7600, train accuracy 0.5185, train loss 140.276901\n",
      "step 7800, train accuracy 0.5212, train loss 123.171547\n",
      "step 8000, train accuracy 0.5256, train loss 112.749374\n",
      "step 8200, train accuracy 0.529, train loss 109.996971\n",
      "step 8400, train accuracy 0.5257, train loss 106.260941\n",
      "step 8600, train accuracy 0.5312, train loss 91.963730\n",
      "step 8800, train accuracy 0.5339, train loss 110.179703\n",
      "step 9000, train accuracy 0.5317, train loss 91.508987\n",
      "step 9200, train accuracy 0.5406, train loss 95.721382\n",
      "step 9400, train accuracy 0.537, train loss 94.862167\n",
      "step 9600, train accuracy 0.5225, train loss 87.009529\n",
      "step 9800, train accuracy 0.5264, train loss 75.610199\n",
      "step 10000, train accuracy 0.5354, train loss 80.136482\n",
      "step 10200, train accuracy 0.5341, train loss 72.797127\n",
      "step 10400, train accuracy 0.5366, train loss 65.507492\n",
      "step 10600, train accuracy 0.5336, train loss 50.971867\n",
      "step 10800, train accuracy 0.5287, train loss 60.784271\n",
      "step 11000, train accuracy 0.5338, train loss 51.460274\n",
      "step 11200, train accuracy 0.5347, train loss 75.865540\n",
      "step 11400, train accuracy 0.5282, train loss 64.928368\n",
      "step 11600, train accuracy 0.5344, train loss 50.180599\n",
      "step 11800, train accuracy 0.534, train loss 56.809189\n",
      "step 12000, train accuracy 0.5225, train loss 41.142746\n",
      "step 12200, train accuracy 0.5249, train loss 39.652901\n",
      "step 12400, train accuracy 0.5301, train loss 35.942558\n",
      "step 12600, train accuracy 0.5349, train loss 55.452774\n",
      "step 12800, train accuracy 0.529, train loss 32.618370\n",
      "step 13000, train accuracy 0.5375, train loss 40.385403\n",
      "step 13200, train accuracy 0.5275, train loss 48.239578\n",
      "step 13400, train accuracy 0.5352, train loss 24.862421\n",
      "step 13600, train accuracy 0.5353, train loss 28.501842\n",
      "step 13800, train accuracy 0.5354, train loss 18.825657\n",
      "step 14000, train accuracy 0.5234, train loss 18.172403\n",
      "step 14200, train accuracy 0.5331, train loss 22.789442\n",
      "step 14400, train accuracy 0.5277, train loss 23.954348\n",
      "step 14600, train accuracy 0.529, train loss 37.169163\n",
      "step 14800, train accuracy 0.5435, train loss 32.932831\n",
      "step 15000, train accuracy 0.5167, train loss 18.884727\n",
      "step 15200, train accuracy 0.5255, train loss 17.409130\n",
      "step 15400, train accuracy 0.5287, train loss 32.244568\n",
      "step 15600, train accuracy 0.5287, train loss 13.260602\n",
      "step 15800, train accuracy 0.5403, train loss 12.203358\n",
      "step 16000, train accuracy 0.5354, train loss 10.713616\n",
      "step 16200, train accuracy 0.5311, train loss 10.433908\n",
      "step 16400, train accuracy 0.5195, train loss 11.055462\n",
      "step 16600, train accuracy 0.5277, train loss 8.829308\n",
      "step 16800, train accuracy 0.5267, train loss 40.047611\n",
      "step 17000, train accuracy 0.5266, train loss 5.443349\n",
      "step 17200, train accuracy 0.5291, train loss 25.834000\n",
      "step 17400, train accuracy 0.5325, train loss 8.994128\n",
      "step 17600, train accuracy 0.5232, train loss 5.358599\n",
      "step 17800, train accuracy 0.5314, train loss 23.465797\n",
      "step 18000, train accuracy 0.5306, train loss 7.691382\n",
      "step 18200, train accuracy 0.5298, train loss 5.016158\n",
      "step 18400, train accuracy 0.5301, train loss 5.386082\n",
      "step 18600, train accuracy 0.5231, train loss 24.474030\n",
      "step 18800, train accuracy 0.5353, train loss 21.288298\n",
      "step 19000, train accuracy 0.5353, train loss 5.166159\n",
      "step 19200, train accuracy 0.5269, train loss 20.167419\n",
      "step 19400, train accuracy 0.5304, train loss 2.392597\n",
      "step 19600, train accuracy 0.5338, train loss 21.501368\n",
      "step 19800, train accuracy 0.5388, train loss 20.055056\n",
      "step 20000, train accuracy 0.5286, train loss 19.844511\n",
      "step 20200, train accuracy 0.5351, train loss 5.360402\n",
      "step 20400, train accuracy 0.5294, train loss 2.361151\n",
      "step 20600, train accuracy 0.5357, train loss 2.364336\n",
      "step 20800, train accuracy 0.5278, train loss 3.070905\n",
      "step 21000, train accuracy 0.536, train loss 3.457203\n",
      "step 21200, train accuracy 0.5332, train loss 2.394508\n",
      "step 21400, train accuracy 0.5262, train loss 17.754168\n",
      "step 21600, train accuracy 0.5286, train loss 1.996065\n",
      "step 21800, train accuracy 0.5363, train loss 18.080193\n",
      "step 22000, train accuracy 0.5435, train loss 1.639857\n",
      "step 22200, train accuracy 0.5314, train loss 2.181937\n",
      "step 22400, train accuracy 0.5382, train loss 1.798491\n",
      "step 22600, train accuracy 0.5194, train loss 1.541744\n",
      "step 22800, train accuracy 0.529, train loss 1.827913\n",
      "step 23000, train accuracy 0.5275, train loss 1.782806\n",
      "step 23200, train accuracy 0.5322, train loss 2.008119\n",
      "step 23400, train accuracy 0.5263, train loss 1.869038\n",
      "step 23600, train accuracy 0.5283, train loss 17.500635\n",
      "step 23800, train accuracy 0.5316, train loss 33.234985\n",
      "step 24000, train accuracy 0.5396, train loss 1.394993\n",
      "step 24200, train accuracy 0.531, train loss 17.188120\n",
      "step 24400, train accuracy 0.5392, train loss 17.066181\n",
      "step 24600, train accuracy 0.5317, train loss 1.562879\n",
      "step 24800, train accuracy 0.5251, train loss 16.951746\n",
      "step 25000, train accuracy 0.5358, train loss 16.988338\n",
      "step 25200, train accuracy 0.5336, train loss 0.862144\n",
      "step 25400, train accuracy 0.5365, train loss 0.727670\n",
      "step 25600, train accuracy 0.5456, train loss 1.378552\n",
      "step 25800, train accuracy 0.5221, train loss 1.711792\n",
      "step 26000, train accuracy 0.5344, train loss 1.045460\n",
      "step 26200, train accuracy 0.5373, train loss 0.985956\n",
      "step 26400, train accuracy 0.5318, train loss 1.320104\n",
      "step 26600, train accuracy 0.5377, train loss 0.721354\n",
      "step 26800, train accuracy 0.5368, train loss 0.479354\n",
      "step 27000, train accuracy 0.5396, train loss 0.963211\n",
      "step 27200, train accuracy 0.5364, train loss 0.916299\n",
      "step 27400, train accuracy 0.5405, train loss 0.469299\n",
      "step 27600, train accuracy 0.5261, train loss 16.800262\n",
      "step 27800, train accuracy 0.5317, train loss 0.708570\n",
      "step 28000, train accuracy 0.5308, train loss 32.691940\n",
      "step 28200, train accuracy 0.5376, train loss 0.545526\n",
      "step 28400, train accuracy 0.5331, train loss 17.156610\n",
      "step 28600, train accuracy 0.5239, train loss 0.646387\n",
      "step 28800, train accuracy 0.5372, train loss 0.266282\n",
      "step 29000, train accuracy 0.5366, train loss 16.881886\n",
      "step 29200, train accuracy 0.5296, train loss 0.465662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29400, train accuracy 0.5348, train loss 0.323528\n",
      "step 29600, train accuracy 0.5333, train loss 0.361449\n",
      "step 29800, train accuracy 0.5247, train loss 0.311414\n",
      "step 30000, train accuracy 0.5379, train loss 0.442805\n",
      "step 30200, train accuracy 0.5312, train loss 0.410918\n",
      "step 30400, train accuracy 0.5373, train loss 0.450604\n",
      "step 30600, train accuracy 0.5227, train loss 0.862874\n",
      "step 30800, train accuracy 0.5293, train loss 16.504375\n",
      "step 31000, train accuracy 0.5274, train loss 16.497541\n",
      "step 31200, train accuracy 0.532, train loss 16.425076\n",
      "step 31400, train accuracy 0.5359, train loss 16.455519\n",
      "step 31600, train accuracy 0.5395, train loss 32.689018\n",
      "step 31800, train accuracy 0.5364, train loss 0.376679\n",
      "step 32000, train accuracy 0.5234, train loss 0.419510\n",
      "step 32200, train accuracy 0.5383, train loss 0.175797\n",
      "step 32400, train accuracy 0.5279, train loss 0.235509\n",
      "step 32600, train accuracy 0.5287, train loss 0.769063\n",
      "step 32800, train accuracy 0.5296, train loss 0.132920\n",
      "step 33000, train accuracy 0.5431, train loss 0.269671\n",
      "step 33200, train accuracy 0.5304, train loss 0.470904\n",
      "step 33400, train accuracy 0.5361, train loss 0.315477\n",
      "step 33600, train accuracy 0.5323, train loss 0.328735\n",
      "step 33800, train accuracy 0.5346, train loss 16.276106\n",
      "step 34000, train accuracy 0.529, train loss 0.139317\n",
      "step 34200, train accuracy 0.5428, train loss 0.202088\n",
      "step 34400, train accuracy 0.5382, train loss 0.214687\n",
      "step 34600, train accuracy 0.5354, train loss 0.197254\n",
      "step 34800, train accuracy 0.5363, train loss 0.185281\n",
      "step 35000, train accuracy 0.5355, train loss 0.335962\n",
      "step 35200, train accuracy 0.5416, train loss 0.196573\n",
      "step 35400, train accuracy 0.5343, train loss 0.606578\n",
      "step 35600, train accuracy 0.5436, train loss 0.242250\n",
      "step 35800, train accuracy 0.5316, train loss 0.151913\n",
      "step 36000, train accuracy 0.5344, train loss 0.169236\n",
      "step 36200, train accuracy 0.5385, train loss 16.249718\n",
      "step 36400, train accuracy 0.5402, train loss 16.267673\n",
      "step 36600, train accuracy 0.5367, train loss 0.360343\n",
      "step 36800, train accuracy 0.5334, train loss 0.188642\n",
      "step 37000, train accuracy 0.5416, train loss 16.209496\n",
      "step 37200, train accuracy 0.5267, train loss 0.122031\n",
      "step 37400, train accuracy 0.5307, train loss 0.192422\n",
      "step 37600, train accuracy 0.536, train loss 0.132596\n",
      "step 37800, train accuracy 0.537, train loss 0.148156\n",
      "step 38000, train accuracy 0.5332, train loss 16.222874\n",
      "step 38200, train accuracy 0.5411, train loss 0.166752\n",
      "step 38400, train accuracy 0.5414, train loss 0.115886\n",
      "step 38600, train accuracy 0.5366, train loss 0.127770\n",
      "step 38800, train accuracy 0.5389, train loss 16.239967\n",
      "step 39000, train accuracy 0.5205, train loss 0.117228\n",
      "step 39200, train accuracy 0.5366, train loss 0.088693\n",
      "step 39400, train accuracy 0.5311, train loss 0.109028\n",
      "step 39600, train accuracy 0.5298, train loss 0.448082\n",
      "step 39800, train accuracy 0.5416, train loss 0.085226\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/vali_data.npy'\n",
    "\n",
    "vali = np.load('E:/Alibaba German AI Challenge/data_process/sample_of_training_10k.npy')\n",
    "data = np.load(filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "Eval = []\n",
    "\n",
    "batch_size = 300\n",
    "for i in range(40000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "        Eval.append(train_accuracy)\n",
    "        print (\"step %d, train accuracy %g, train loss %f\" %(i, train_accuracy, loss_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAE/CAYAAAB8YAsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4SwiItLO/H+5jn0gvg8loQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2875e4539b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Eval)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Eval)\n",
    "plt.title('Eval')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.11      0.04      0.06       157\n",
      "          1       0.33      0.35      0.34       710\n",
      "          2       0.44      0.33      0.38       858\n",
      "          3       0.14      0.25      0.18       254\n",
      "          4       0.28      0.18      0.22       492\n",
      "          5       0.38      0.19      0.25       956\n",
      "          6       0.01      0.01      0.01        80\n",
      "          7       0.56      0.65      0.60      1092\n",
      "          8       0.20      0.33      0.25       389\n",
      "          9       0.21      0.12      0.16       323\n",
      "         10       0.70      0.90      0.79      1230\n",
      "         11       0.38      0.14      0.21       264\n",
      "         12       0.04      0.02      0.03       268\n",
      "         13       0.61      0.76      0.68      1186\n",
      "         14       0.19      0.08      0.11        65\n",
      "         15       0.44      0.55      0.49       251\n",
      "         16       0.94      0.97      0.96      1425\n",
      "\n",
      "avg / total       0.51      0.53      0.51     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = y_conv.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "train_val_y = np.argmax(vali[:,-17:],axis = 1)\n",
    "pred_y = np.argmax(pred, axis = 1)\n",
    "print (classification_report(train_val_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('forth_40k_batch_vali_Adam.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (24119, 18449)\n",
      "step 0, train accuracy 0.086, train loss 10549.526367\n",
      "step 100, train accuracy 0.485, train loss 3021.301758\n",
      "step 200, train accuracy 0.51, train loss 2443.161133\n",
      "step 300, train accuracy 0.532, train loss 1890.379150\n",
      "step 400, train accuracy 0.559, train loss 1780.113770\n",
      "step 500, train accuracy 0.557, train loss 1665.154541\n",
      "step 600, train accuracy 0.586, train loss 1497.526367\n",
      "step 700, train accuracy 0.614, train loss 1369.020996\n",
      "step 800, train accuracy 0.599, train loss 1304.535645\n",
      "step 900, train accuracy 0.613, train loss 1302.531250\n",
      "step 1000, train accuracy 0.654, train loss 1196.070557\n",
      "step 1100, train accuracy 0.661, train loss 1213.286011\n",
      "step 1200, train accuracy 0.683, train loss 1136.839600\n",
      "step 1300, train accuracy 0.68, train loss 1048.243896\n",
      "step 1400, train accuracy 0.698, train loss 1019.251892\n",
      "step 1500, train accuracy 0.677, train loss 1031.617920\n",
      "step 1600, train accuracy 0.715, train loss 920.973267\n",
      "step 1700, train accuracy 0.686, train loss 1006.383301\n",
      "step 1800, train accuracy 0.685, train loss 1018.709778\n",
      "step 1900, train accuracy 0.705, train loss 896.948303\n",
      "step 2000, train accuracy 0.735, train loss 924.078613\n",
      "step 2100, train accuracy 0.769, train loss 816.070190\n",
      "step 2200, train accuracy 0.768, train loss 799.420532\n",
      "step 2300, train accuracy 0.746, train loss 775.331787\n",
      "step 2400, train accuracy 0.769, train loss 776.388428\n",
      "step 2500, train accuracy 0.78, train loss 756.205933\n",
      "step 2600, train accuracy 0.79, train loss 721.189087\n",
      "step 2700, train accuracy 0.775, train loss 695.189209\n",
      "step 2800, train accuracy 0.799, train loss 719.804810\n",
      "step 2900, train accuracy 0.799, train loss 685.580383\n",
      "step 3000, train accuracy 0.814, train loss 615.439087\n",
      "step 3100, train accuracy 0.832, train loss 574.369019\n",
      "step 3200, train accuracy 0.816, train loss 588.131348\n",
      "step 3300, train accuracy 0.833, train loss 638.464233\n",
      "step 3400, train accuracy 0.835, train loss 553.778076\n",
      "step 3500, train accuracy 0.821, train loss 573.179688\n",
      "step 3600, train accuracy 0.855, train loss 487.541351\n",
      "step 3700, train accuracy 0.841, train loss 553.093750\n",
      "step 3800, train accuracy 0.846, train loss 494.666046\n",
      "step 3900, train accuracy 0.875, train loss 440.400818\n",
      "step 4000, train accuracy 0.865, train loss 437.459534\n",
      "step 4100, train accuracy 0.857, train loss 478.810059\n",
      "step 4200, train accuracy 0.873, train loss 408.566589\n",
      "step 4300, train accuracy 0.874, train loss 419.052582\n",
      "step 4400, train accuracy 0.891, train loss 377.765564\n",
      "step 4500, train accuracy 0.89, train loss 410.936401\n",
      "step 4600, train accuracy 0.889, train loss 393.196533\n",
      "step 4700, train accuracy 0.908, train loss 339.452759\n",
      "step 4800, train accuracy 0.904, train loss 373.273499\n",
      "step 4900, train accuracy 0.899, train loss 321.486389\n",
      "step 5000, train accuracy 0.913, train loss 306.311768\n",
      "step 5100, train accuracy 0.938, train loss 273.795532\n",
      "step 5200, train accuracy 0.932, train loss 261.967590\n",
      "step 5300, train accuracy 0.923, train loss 266.855469\n",
      "step 5400, train accuracy 0.937, train loss 238.778534\n",
      "step 5500, train accuracy 0.923, train loss 265.600708\n",
      "step 5600, train accuracy 0.951, train loss 226.953949\n",
      "step 5700, train accuracy 0.941, train loss 249.160431\n",
      "step 5800, train accuracy 0.946, train loss 215.878830\n",
      "step 5900, train accuracy 0.94, train loss 257.293457\n",
      "step 6000, train accuracy 0.95, train loss 252.912048\n",
      "step 6100, train accuracy 0.958, train loss 202.372986\n",
      "step 6200, train accuracy 0.964, train loss 177.334473\n",
      "step 6300, train accuracy 0.964, train loss 158.086975\n",
      "step 6400, train accuracy 0.967, train loss 179.947021\n",
      "step 6500, train accuracy 0.953, train loss 162.297012\n",
      "step 6600, train accuracy 0.977, train loss 166.109894\n",
      "step 6700, train accuracy 0.98, train loss 159.041473\n",
      "step 6800, train accuracy 0.982, train loss 136.565491\n",
      "step 6900, train accuracy 0.968, train loss 205.061493\n",
      "step 7000, train accuracy 0.978, train loss 117.683792\n",
      "step 7100, train accuracy 0.982, train loss 112.446381\n",
      "step 7200, train accuracy 0.985, train loss 100.861473\n",
      "step 7300, train accuracy 0.987, train loss 104.313957\n",
      "step 7400, train accuracy 0.988, train loss 101.061707\n",
      "step 7500, train accuracy 0.992, train loss 91.327087\n",
      "step 7600, train accuracy 0.991, train loss 88.861084\n",
      "step 7700, train accuracy 0.99, train loss 89.160477\n",
      "step 7800, train accuracy 0.99, train loss 84.204407\n",
      "step 7900, train accuracy 0.992, train loss 64.442932\n",
      "step 8000, train accuracy 0.986, train loss 121.414948\n",
      "step 8100, train accuracy 0.998, train loss 64.984985\n",
      "step 8200, train accuracy 0.996, train loss 57.075768\n",
      "step 8300, train accuracy 0.997, train loss 51.368309\n",
      "step 8400, train accuracy 0.994, train loss 66.212280\n",
      "step 8500, train accuracy 0.996, train loss 69.521210\n",
      "step 8600, train accuracy 0.996, train loss 59.466068\n",
      "step 8700, train accuracy 0.996, train loss 43.531979\n",
      "step 8800, train accuracy 0.997, train loss 53.813118\n",
      "step 8900, train accuracy 0.999, train loss 37.345432\n",
      "step 9000, train accuracy 0.996, train loss 52.797287\n",
      "step 9100, train accuracy 0.998, train loss 30.884176\n",
      "step 9200, train accuracy 0.995, train loss 51.253876\n",
      "step 9300, train accuracy 0.997, train loss 45.961800\n",
      "step 9400, train accuracy 0.996, train loss 62.833691\n",
      "step 9500, train accuracy 0.999, train loss 29.905762\n",
      "step 9600, train accuracy 0.997, train loss 74.284592\n",
      "step 9700, train accuracy 1, train loss 22.936134\n",
      "step 9800, train accuracy 0.997, train loss 54.036182\n",
      "step 9900, train accuracy 0.999, train loss 24.603813\n",
      "step 10000, train accuracy 0.999, train loss 19.257931\n",
      "step 10100, train accuracy 0.999, train loss 24.361551\n",
      "step 10200, train accuracy 0.998, train loss 39.591461\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fc15086b981d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mloss_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fc15086b981d>\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(data, batch_size)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/vali_data.npy'\n",
    "\n",
    "vali = np.load('E:/Alibaba German AI Challenge/data_process/sample_of_training_10k.npy')\n",
    "data = np.load(filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "Eval = []\n",
    "\n",
    "batch_size = 1000\n",
    "for i in range(12000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g, train loss %f\" %(i, train_accuracy, loss_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
