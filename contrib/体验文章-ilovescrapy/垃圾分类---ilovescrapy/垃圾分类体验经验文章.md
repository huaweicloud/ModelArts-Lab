# huaweicloud_garbage_classify
赛事链接：https://developer.huaweicloud.com/competition/competitions/1000007620/introduction
1. Baseline开始
最开始入手是采用官方提供的baseline,按部就班的把数据代码文件上传,在modelarts进行训练，然后部署上线并发布，结果准确率只有0.65左右，仔细查验官方baseline代码，发现在多个文件中将loss写成了二分类loss，objective = 'binary_crossentropy'导致keras评估验证集合准确率错误，无法给出正确的评估。立时修改，终于准确率评估正常了,至此baseline code成功跑通。
## nets
包含vgg16，resnet50，senet50
将数据集按9：1分成训练集和测试集，并按照官方提供的基本代码跑了一次（基本参数：ResNet50,batch_size=32,learning_rate=1e-4,epochs=10）
采用senet50但并没有太大提高，将模型采用熟悉的VGG16并采用Dropout防止过拟合，最后训练进度提升到70%以上。
将数据集进行预训练：原始的预训练采用的思路是冻结了特征提取，并对最后加的两层Dense layer分类器进行训练，实际上并没有对特征提取进行调节。基于这个思路萌发了改造原始的代码训练最后5层的特征抽取和分类器训练，实际效果上可以发现有所提高，但是提高的不大，大概有1-2个点的提升但是目前的数据只有1.5W左右，训练整个模型数据量偏小，同时观察存在类别不均衡的情况，于是乎再进一步魔改做个数据增强，平衡一下类别，通过ImageDataGenerator将数据量增强到了70000多张图片，同时也缓解了类别不均衡问题。 于是乎在上述数据上再次训练，这次由于是采用全部打开的结构，训练时间非常长，所以架构上采用了ResNet50进行训练，差不多用了2个小时多在1* P100上训练，然后可喜的是模型获得了极大的提高准确率达到了0.89以上。

## metrics
包含各种softmax的改进，时间原因无法把前三个用于分类；NormFace可以应对类别数据不平衡，不过目前效果和softmax差不多。
softmax loss倾向于学习到一个radial分布的特征，其原因在于特征的scale越大就会使得softmax的loss越小，figure.3里面是softmax之前的fc有bias的情况下会使得有些类别在角度上没有区分性但是通过bias可以区分，在这种情况下如果对feature做normalize，会使得中间的那个小类别的feature变成一个单位球形并与其他的feature重叠在一起，所以在feature normalize的时候是不能加bias的。
做了归一化之后，softmax的优化就变成了直接优化cosine距离了：
但是这里有遇到一个问题是这样训练并不收敛，其原因在于normalize之后softmax loss的输入处于一个[-1,1]的分布，其最小值被抑制、是有下限的，并且对于一个类别的概率来说，即使样本被完美分类，即对应类别的输出为1，其他的为-1，那么这个概率Py还是一个比较小的值，而softmax loss的梯度为1-Py，这使得容易的样本梯度也很大。相比于原来的softmax loss，其输入的scale可以很大使得概率Py是个接近于1的数使得难易样本的梯度差别比较明显。所以解决办法也就显而易见了，就是在normalize之后加个scale，让这个差距再拉大，所以最终normalize之后的softmax loss如下，其中w和f都是归一化的。

## others
基于baseline的修改，加了一些tricks，比如label smooth，center crop等等，待你们发现
对了，测试的地方增加了random crop，然后集成，能提升1%
label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。

#### Tricks
（1） data-augmentation.简单粗暴，比如将图像放大，利用 image pyramid多尺度检测，最后将检测结果融合.缺点是操作复杂，计算量大，实际情况中不实用;
（2） 特征融合方法：FPN这些，多尺度feature map预测，feature stride可以从更小的开始;
（3）合适的训练方法：CVPR2018的SNIP以及SNIPER;
（4）设置更小更稠密的anchor，设计anchor match strategy等，参考S3FD;
（5）利用GAN将小物体放大再检测，CVPR2018有这样的论文;
（6）利用context信息，简历object和context的联系，比如relation network;
（7）有密集遮挡，如何把location 和Classification 做的更好，参考IoU loss, repulsion loss等.
（8）卷积神经网络设计时尽量度采用步长为1，尽可能保留多的目标特征。
