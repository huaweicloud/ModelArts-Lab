{
	"cells": [{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### <span style=\"color:red\">本案例创建环境须创建10G以上的EVS配置</span>\n",
				"\n",
   				"| 参数 | 说明 |\n",
  				"| - - - - - | - - - - - |\n",
    				"| 计费方式 | 按需计费  |\n",
   				"| 名称 | Notebook实例名称，如 nlp_bert_ner |\n",
    				"| 工作环境 | Python3 |\n",
    				"| 资源池 | 选择\"公共资源池\"即可 |\n",
    				"| 类型 | GPU |\n",
    				"| 规格 | GPU:1*p100 CPU:8核64GiB |\n",
    				"| 存储配置 | 选择EVS，磁盘规格10GB |\n",
    				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 命名实体识别\n",
				"\n",
				"命名实体识别（Named EntitiesRecognition，NER）是NLP里的一项很基础的任务，就是指从文本中识别出命名性指称项，为信息抽取、信息检索、机器翻译、问答系统等任务做铺垫。其目的是识别语料中人名、地名、组织机构名等命名实体。在特定的领域中，会相应地定义领域内的各种实体类型。\n",
				"\n",
				"命名实体是命名实体识别的研究主体，一般包括三大类（实体类、时间类和数字类）和七小类（人名、地名、机构名、时间、日期、货币和百分比）命名实体。其中人名、地名、机构名相对于其他实体识别较复杂，无法用模式匹配的方式获得较好的识别效果，是近年来研究的主要部分。\n",
				"\n",
				"汉语作为象形文字，相比于英文等拼音文字来说，针对中文的NER任务来说往往要更有挑战性，下面列举几点：\n",
				"\n",
				"1.  中文文本里不像英文那样有空格作为词语的界限标志，而且“词”在中文里本来就是一个很模糊的概念，中文也不具备英文中的字母大小写等形态指示。\n",
				"\n",
				"2. 中文的用字灵活多变，有些词语在脱离上下文语境的情况下无法判断是否是命名实体，而且就算是命名实体，当其处在不同的上下文语境下也可能是不同的实体类型。\n",
				"\n",
				"3. 命名实体存在嵌套现象，如“北京大学第三医院”这一组织机构名中还嵌套着同样可以作为组织机构名的“北京大学”，而且这种现象在组织机构名中尤其严重。\n",
				"\n",
				"4. 中文里广泛存在简化表达现象，如“北医三院”、“国科大”，乃至简化表达构成的命名实体，如“国科大桥”。\n",
				"\n",
				"常用的命名实体标注方法有BIO标注、BIOES标注、BMES等。在本案例中选择使用BIO标注：将每个元素标注为“B-X”、“I-X”或者“O”。\n",
				"\n",
				"1. “B-X”（Begin）表示此元素所在的片段属于X类型并且此元素在此片段的开头；\n",
				"2. “I-X”（Inside）表示此元素所在的片段属于X类型并且此元素在此片段的中间位置；\n",
				"3. “O”（Outside）表示不属于任何类型。\n",
				"\n",
				"即B-PER、I-PER代表人名首字、人名非首字，B-LOC、I-LOC代表地名首字、地名非首字，B-ORG、I-ORG代表组织机构名首字、组织机构名非首字，示例如下：\n",
				"![BIO](./img/BIO.png)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## BERT简介\n",
				"BERT（Bidirectional Encoder Representations from Transformers）是一种预训练NLP模型，由Google在2018年10月发布的论文[《BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding》](https://arxiv.org/pdf/1810.04805.pdf)中提出。该模型刷新了自然语言处理任务的11项记录。\n",
				"\n",
				"BERT的通过联合调节所有层中的双向Transformer来训练预训练深度双向表示，只需要一个额外的输出层来对预训练BERT进行微调就可以满足各种任务，没有必要针对特定任务对模型进行修改，其先进性基于两点：其一，是使用Masked Langauge Model（MLM）和Next Sentense Prediction（NSP）的新预训练任务，两种方法分别捕捉词语和句子级别的representation；其二，是大量数据和计算能力满足BERT的训练强度，BERT训练数据采用了英文的开源语料BooksCropus 以及英文维基百科数据，一共有33亿个词,同时BERT模型的标准版本有1亿的参数量，而BERT的large版本有3亿多参数量,其团队训练一个预训练模型需要在64块TPU芯片上训练4天完成，而一块TPU的速度约是目前主流GPU的7-8倍。Google团队开源了多个预训练模型，以供多种下游任务需求使用。开源的预训练模型如下：\n",
				"\n",
				"- [BERT-Base, Uncased](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip): 12-layer, 768-hidden, 12-heads, 110M parameters\n",
				"- [BERT-Large, Uncased](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip): 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
				"- [BERT-Base, Cased](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip): 12-layer, 768-hidden, 12-heads , 110M parameters\n",
				"- [BERT-Large, Cased](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip): 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
				"- [BERT-Base, Multilingual Cased (New, recommended)](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip): 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n",
				"- [BERT-Base, Multilingual Uncased (Orig, not recommended)](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip)(Not recommended, use Multilingual Cased instead): 102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n",
				"- [BERT-Base, Chinese](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip): Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters\n",
				"\n",
				"前4个是英文模型，Multilingual 是多语言模型，最后一个是中文模型（只有字级别的）。其中 Uncased 是字母全部转换成小写，而Cased是保留了大小写。\n",
				"这里layer是layers层数（即Transformer blocks个数），hidden是hidden vector size，heads是self-attention的heads。\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## BERT预训练的两个重要步骤\n",
				"\n",
				"#### Masked语言模型\n",
				"\n",
				"为了训练深度双向语言表示向量，作者用了一个非常直接的方式，遮住句子里某些单词，让编码器预测这个单词是什么。\n",
				"\n",
				"训练方法为：\n",
				"\n",
				"1）80%的单词用***[MASK]*** token来代替\n",
				"\n",
				"my dog is ***hairy*** → my dog is ***[MASK]***\n",
				"\n",
				"2）10%单词用任意的词来进行代替\n",
				"\n",
				"my dog is ***hairy*** → my dog is ***apple***\n",
				"\n",
				"3）10%单词不变\n",
				"\n",
				"my dog is ***hairy*** → my dog is ***hairy***\n",
				"\n",
				"作者在论文中提到这样做的好处是，编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量。另外作者表示，每个batchsize只有15%的词被遮盖的原因，是性能开销。双向编码器比单项编码器训练要慢。\n",
				"\n",
				"#### 预测下一个句子：Next Sentence Prediction（NSP）\n",
				"\n",
				"预训练一个二分类的模型，来学习句子之间的关系。预测下一个句子的方法对学习句子之间关系很有帮助。\n",
				"\n",
				"训练方法：正样本和负样本比例是1：1，50%的句子是正样本，随机选择50%的句子作为负样本。\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 数据集\n",
				"\n",
				"本案例使用《人民日报1998年中文标注语料库》\n",
				"\n",
				"以1998年人民日报语料为对象，由北京大学计算语言学研究所和富士通研究开发中心有限公司共同制作的标注语料库。该语料库对600多万字节的中文文章进行了分词及词性标注，其被作为原始数据应用于大量的研究和论文中。\n",
				"\n",
				"数据集格式如下图，每行的第一个是字，第二个是它的标签，字与标签之间使用空格分隔，两句话之间空一行。\n",
				"![data](./img/数据集示例.png)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 预训练模型\n",
				"\n",
				"本案例使用中文**BERT-Base,Chinese**预训练模型。从链接[BERT-Base, Chinese](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)下载解压后得到如下预训练模型文件\n",
				"\n",
				"- bert_config.json\n",
				"- bert_model.ckpt.data-00000-of-00001\n",
				"- bert_model.ckpt.index\n",
				"- bert_model.ckpt.meta\n",
				"- vocab.txt\n",
				"\n",
				"其中\n",
				"* 一个config file(bert_config.json) ：存储预训练模型超参数\n",
				"* 三个tensorflow checkpoint (bert_model.ckpt) ：包含预训练模型checkpoint\n",
				"* 一个vocab文件(vocab.txt)：将WordPiece映射成word id  \n",
				"\n",
				"从obs上下载所需数据集、文件及模型如下："
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"start download NER .tar.gz. Waiting......30 sec\n",
					"Successfully download file modelarts-labs/end2end/NLP/NER .tar.gz from OBS to local ./NER.tar.gz\n",
					"total 750272\n",
					"drwsrwsrwx  7 root   users      4096 May 26 05:07 .\n",
					"drwsrwsr-x 20 jovyan users      4096 May 26 04:34 ..\n",
					"drwsrws---  3 root   users      4096 May 26 04:44 img\n",
					"drwsrws---  2 root   users      4096 May 26 05:07 .ipynb_checkpoints\n",
					"drwsrws---  2 root   users     16384 May 24 01:42 lost+found\n",
					"drwsrws---  8 root   users      4096 May 25 14:49 NER\n",
					"-rw-rw----  1 root   users 384063639 May 25 15:26 NER1.tar.gz\n",
					"-rw-r-----  1 jovyan users    101022 May 26 04:54 NER-BERT.ipynb\n",
					"-rw-r-----  1 jovyan users 384063613 May 26 05:11 NER.tar.gz\n",
					"drwsrws---  4 root   users      4096 May 26 04:44 .Trash-1000\n",
					"-rw-rw----  1 root   users       809 May 26 03:01 Untitled.ipynb\n"
				]
			}],
			"source": [
				"from modelarts.session import Session\n",
				"session = Session()\n",
				"print(\"start download NER .tar.gz. Waiting......30 sec\")\n",
				"session.download_data(bucket_path=\"modelarts-labs/end2end/NLP/NER .tar.gz\", path=\"./NER.tar.gz\")\n",
				"# 以上步骤在ModelArts Notebook中运行，只有在下载完毕后才有输出\n",
				"\n",
				"#查看目录\n",
				"!ls -la    "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"解压从obs下载的压缩包，解压后删除压缩包。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"tar: ./NER/chinese_L-12_H-768_A-12: Cannot utime: Operation not permitted\n",
					"tar: ./NER/chinese_L-12_H-768_A-12: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER/bert/__pycache__: Cannot utime: Operation not permitted\n",
					"tar: ./NER/bert/__pycache__: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER/bert: Cannot utime: Operation not permitted\n",
					"tar: ./NER/bert: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER/data: Cannot utime: Operation not permitted\n",
					"tar: ./NER/data: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER/output/.ipynb_checkpoints: Cannot utime: Operation not permitted\n",
					"tar: ./NER/output/.ipynb_checkpoints: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER/output: Cannot utime: Operation not permitted\n",
					"tar: ./NER/output: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER/.ipynb_checkpoints: Cannot utime: Operation not permitted\n",
					"tar: ./NER/.ipynb_checkpoints: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER/src/__pycache__: Cannot utime: Operation not permitted\n",
					"tar: ./NER/src/__pycache__: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER/src: Cannot utime: Operation not permitted\n",
					"tar: ./NER/src: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: ./NER: Cannot utime: Operation not permitted\n",
					"tar: ./NER: Cannot change mode to rwsr-s---: Operation not permitted\n",
					"tar: Exiting with failure status due to previous errors\n",
					"total 375208\n",
					"drwsrwsrwx  7 root   users      4096 May 26 05:11 .\n",
					"drwsrwsr-x 20 jovyan users      4096 May 26 04:34 ..\n",
					"drwsrws---  3 root   users      4096 May 26 04:44 img\n",
					"drwsrws---  2 root   users      4096 May 26 05:07 .ipynb_checkpoints\n",
					"drwsrws---  2 root   users     16384 May 24 01:42 lost+found\n",
					"drwsrws---  8 root   users      4096 May 25 14:49 NER\n",
					"-rw-rw----  1 root   users 384063639 May 25 15:26 NER1.tar.gz\n",
					"-rw-r-----  1 jovyan users    101022 May 26 04:54 NER-BERT.ipynb\n",
					"drwsrws---  4 root   users      4096 May 26 04:44 .Trash-1000\n",
					"-rw-rw----  1 root   users       809 May 26 03:01 Untitled.ipynb\n"
				]
			}],
			"source": [
				"# 使用tar命令解压资源包\n",
				"!tar xf ./NER.tar.gz\n",
				"\n",
				"# 使用rm命令删除压缩包\n",
				"!rm ./NER.tar.gz\n",
				"\n",
				"#查看目录，与上对比检查是否增加NER文件夹\n",
				"!ls -la    "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"解压后文件夹结构如下：\n",
				"\n",
				"```bash\n",
				"\n",
				"NER\n",
				"├── bert\n",
				"│       ├── modeling.py\n",
				"│       ├── optimization.py\n",
				"│       └── tokenization.py\n",
				"│\n",
				"├── chinese_L-12_H-768_A-12\n",
				"│       ├──bert_config.json\n",
				"│       ├──bert_model.ckpt.data-00000-of-00001\n",
				"│       ├──bert_model.ckpt.index\n",
				"│       ├──bert_model.ckpt.meta\n",
				"│       └──vocab.txt\n",
				"├── data\n",
				"│       ├── train.txt  \n",
				"│       ├── dev.txt\n",
				"│       └── test.txt\n",
				"├── src\n",
				"│       ├── conlleval.pl  \n",
				"│       ├── conlleval.py\n",
				"│       ├── models.py\n",
				"│       └── terminal_predict.py\n",
				"├── img\n",
				"└── output\n",
				"\n",
				"```"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 引入运行需要的包\n",
				"\n",
				"本案例所需条件：\n",
				"\n",
				"tensorflow >= 1.11.0   \n",
				"\n",
				"tensorflow-gpu  >= 1.11.0 \n",
				"\n",
				"因此需要首先升级tensorflow和tensorflow-gpu"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"start update tensorflow Waiting......15 sec\n",
					"Collecting tensorflow==1.11.0\n",
					"  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ce/d5/38cd4543401708e64c9ee6afa664b936860f4630dd93a49ab863f9998cd2/tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (63.0MB)\n",
					"\u001b[K    100% |████████████████████████████████| 63.0MB 94.1MB/s eta 0:00:01   2% |▉                               | 1.6MB 80.3MB/s eta 0:00:01 96.1MB/s eta 0:00:01██▍                          | 10.6MB 91.6MB/s eta 0:00:01             | 18.6MB 87.5MB/s eta 0:00:01██▉                    | 23.3MB 93.6MB/s eta 0:00:01           | 27.0MB 90.5MB/s eta 0:00:01              | 30.9MB 82.8MB/s eta 0:00:01████████▉              | 35.0MB 77.6MB/s eta 0:00:01�            | 39.3MB 90.6MB/s eta 0:00:01:00:01�████████████        | 47.4MB 95.0MB/s eta 0:00:01   | 51.4MB 89.1MB/s eta 0:00:01��█████████████████████████▎   | 55.7MB 81.7MB/s eta 0:00:01�██▎ | 59.5MB 86.8MB/s eta 0:00:01\n",
					"\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow==1.11.0)\n",
					"Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow==1.11.0)\n",
					"Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow==1.11.0)\n",
					"Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow==1.11.0)\n",
					"Collecting keras-applications>=1.0.5 (from tensorflow==1.11.0)\n",
					"  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
					"\u001b[K    100% |████████████████████████████████| 61kB 64.2MB/s ta 0:00:01\n",
					"\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/jovyan/modelarts-sdk (from tensorflow==1.11.0)\n",
					"Collecting protobuf>=3.6.0 (from tensorflow==1.11.0)\n",
					"  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5a/aa/a858df367b464f5e9452e1c538aa47754d467023850c00b000287750fa77/protobuf-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
					"\u001b[K    100% |████████████████████████████████| 1.2MB 26.8MB/s ta 0:00:01\n",
					"\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow==1.11.0)\n",
					"Collecting keras-preprocessing>=1.0.3 (from tensorflow==1.11.0)\n",
					"  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
					"\u001b[K    100% |████████████████████████████████| 61kB 29.4MB/s ta 0:00:01\n",
					"\u001b[?25hRequirement already satisfied: setuptools<=39.1.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow==1.11.0)\n",
					"Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow==1.11.0)\n",
					"Requirement already satisfied: astor>=0.6.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow==1.11.0)\n",
					"Collecting tensorboard<1.12.0,>=1.11.0 (from tensorflow==1.11.0)\n",
					"  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl (3.0MB)\n",
					"\u001b[K    100% |████████████████████████████████| 3.0MB 31.0MB/s ta 0:00:01█████████████████   | 2.7MB 79.9MB/s eta 0:00:01\n",
					"\u001b[?25hRequirement already satisfied: h5py in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from keras-applications>=1.0.5->tensorflow==1.11.0)\n",
					"Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0)\n",
					"Requirement already satisfied: werkzeug>=0.11.10 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0)\n",
					"Installing collected packages: keras-applications, protobuf, keras-preprocessing, tensorboard, tensorflow\n",
					"  Found existing installation: Keras-Applications 1.0.2\n",
					"    Uninstalling Keras-Applications-1.0.2:\n",
					"      Successfully uninstalled Keras-Applications-1.0.2\n",
					"  Found existing installation: protobuf 3.5.2\n",
					"    Uninstalling protobuf-3.5.2:\n",
					"      Successfully uninstalled protobuf-3.5.2\n",
					"  Found existing installation: Keras-Preprocessing 1.0.1\n",
					"    Uninstalling Keras-Preprocessing-1.0.1:\n",
					"      Successfully uninstalled Keras-Preprocessing-1.0.1\n",
					"  Found existing installation: tensorboard 1.8.0\n",
					"    Uninstalling tensorboard-1.8.0:\n",
					"      Successfully uninstalled tensorboard-1.8.0\n",
					"Successfully installed keras-applications-1.0.7 keras-preprocessing-1.0.9 protobuf-3.7.1 tensorboard-1.11.0 tensorflow-1.11.0\n",
					"下载完毕后tensorflow需要自动配置，等待15秒再执行后面的内容\n"
				]
			}],
			"source": [
				"print(\"start update tensorflow Waiting......15 sec\")\n",
				"!pip install tensorflow==1.11.0\n",
				"print(\"下载完毕后tensorflow需要自动配置，等待15秒再执行后面的内容\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"start update tensorflow-gpu Waiting.....15 sec\n",
					"Collecting tensorflow-gpu==1.11.0\n",
					"  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/25/52/01438b81806765936eee690709edc2a975472c4e9d8d465a01840869c691/tensorflow_gpu-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (258.8MB)\n",
					"\u001b[K    100% |████████████████████████████████| 258.8MB 34.0MB/s ta 0:00:011  1% |▍                               | 3.5MB 87.5MB/s eta 0:00:03[K    3% |█                               | 8.0MB 90.7MB/s eta 0:00:03:00:03| 18.4MB 90.7MB/s eta 0:00:03                   | 23.1MB 96.7MB/s eta 0:00:03                     | 27.5MB 72.8MB/s eta 0:00:04��█                            | 32.2MB 104.2MB/s eta 0:00:03                   | 37.0MB 87.5MB/s eta 0:00:03██▏                          | 41.9MB 97.5MB/s eta 0:00:03             | 46.6MB 98.0MB/s eta 0:00:035MB 84.5MB/s eta 0:00:036MB 80.8MB/s eta 0:00:03█████▍                        | 60.1MB 72.7MB/s eta 0:00:03                     | 64.8MB 85.1MB/s eta 0:00:03.4MB 96.7MB/s eta 0:00:025MB 67.3MB/s eta 0:00:03             | 77.7MB 90.6MB/s eta 0:00:03MB/s eta 0:00:02MB/s eta 0:00:02███▎                    | 91.2MB 98.4MB/s eta 0:00:02██▉                    | 95.5MB 86.1MB/s eta 0:00:02291.0MB/s eta 0:00:02███▍                  | 108.6MB 95.4MB/s eta 0:00:02�██                  | 112.8MB 96.3MB/s eta 0:00:02    45% |██████████████▌                 | 117.2MB 95.9MB/s eta 0:00:02   47% |███████████████                 | 121.7MB 94.3MB/s eta 0:00:02   | 125.5MB 93.1MB/s eta 0:00:02     | 130.1MB 99.4MB/s eta 0:00:02   | 134.2MB 88.3MB/s eta 0:00:02[K    53% |█████████████████▏              | 138.7MB 98.3MB/s eta 0:00:02    | 150.7MB 92.1MB/s eta 0:00:02�███████████             | 154.3MB 67.8MB/s eta 0:00:02�████████▋            | 158.4MB 95.3MB/s eta 0:00:02.4MB 82.8MB/s eta 0:00:02█████████████████▌           | 166.1MB 79.0MB/s eta 0:00:02 |█████████████████████           | 170.2MB 87.7MB/s eta 0:00:02MB 86.2MB/s eta 0:00:01██          | 177.7MB 97.8MB/s eta 0:00:01 70% |██████████████████████▍         | 181.3MB 91.0MB/s eta 0:00:01��███         | 185.2MB 86.7MB/s eta 0:00:01�████▍        | 189.4MB 94.0MB/s eta 0:00:01�█████████████████▉        | 192.6MB 104.2MB/s eta 0:00:01  | 206.4MB 92.9MB/s eta 0:00:01  | 210.5MB 97.2MB/s eta 0:00:01��████████▋     | 214.9MB 92.3MB/s eta 0:00:01�█████████     | 218.8MB 91.9MB/s eta 0:00:01█████▍    | 221.9MB 91.1MB/s eta 0:00:01 88% |████████████████████████████▎   | 228.9MB 96.6MB/s eta 0:00:01    89% |████████████████████████████▊   | 232.3MB 87.0MB/s eta 0:00:01.1MB 94.4MB/s eta 0:00:01�███████▎ | 245.2MB 97.5MB/s eta 0:00:01███████▊ | 248.2MB 95.5MB/s eta 0:00:01████████ | 251.5MB 93.8MB/s eta 0:00:01�██████████████████| 258.4MB 86.7MB/s eta 0:00:01\n",
					"\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: keras-applications>=1.0.5 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: six>=1.10.0 in /home/jovyan/modelarts-sdk (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: keras-preprocessing>=1.0.3 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: setuptools<=39.1.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: gast>=0.2.0 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: h5py in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from keras-applications>=1.0.5->tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: werkzeug>=0.11.10 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0)\n",
					"Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python36_tf/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0)\n",
					"Installing collected packages: tensorflow-gpu\n",
					"  Found existing installation: tensorflow-gpu 1.8.0\n",
					"    Uninstalling tensorflow-gpu-1.8.0:\n",
					"      Successfully uninstalled tensorflow-gpu-1.8.0\n",
					"Successfully installed tensorflow-gpu-1.11.0\n",
					"下载完毕后tensorflow-gpu需要自动配置，等待15秒再执行后面的内容\n"
				]
			}],
			"source": [
				"print(\"start update tensorflow-gpu Waiting.....15 sec\")\n",
				"!pip install tensorflow-gpu==1.11.0\n",
				"print(\"下载完毕后tensorflow-gpu需要自动配置，等待15秒再执行后面的内容\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"打印tensorflow版本，确认正确更新 等待10秒\n",
					"tensorflow-version： 1.11.0\n"
				]
			}],
			"source": [
				"print(\"打印tensorflow版本，确认正确更新 等待10秒\")\n",
				"import tensorflow as tf\n",
				"print(\"tensorflow-version：\", tf.__version__)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"成功引入本案例需要的主要lib\n"
				]
			}],
			"source": [
				"#引入本案例需要的主要lib\n",
				"import os\n",
				"import json\n",
				"import numpy as np\n",
				"import tensorflow as tf\n",
				"import codecs\n",
				"import pickle\n",
				"import collections\n",
				"from NER.bert import modeling, optimization, tokenization\n",
				"print(\"成功引入本案例需要的主要lib\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 定义数据、模型等路径和执行的任务。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"设置所需要的路径参数&全局参数\n",
					"数据集路径为： ./NER/data\n",
					"输出路径为： ./NER/output\n",
					"中文字典路径为： ./NER/chinese_L-12_H-768_A-12/vocab.txt\n",
					"预训练模型参数路径为： ./NER/chinese_L-12_H-768_A-12/bert_config.json\n",
					"预训练模型checkpoint路径为： ./NER/chinese_L-12_H-768_A-12/bert_model.ckpt\n",
					"预先设定的最大序列长度为： 128\n",
					"批尺寸为： 64\n",
					"训练轮数： 3.0\n"
				]
			}],
			"source": [
				"print(\"设置所需要的路径参数&全局参数\")\n",
				"data_dir = \"./NER/data\"    #数据集路径\n",
				"output_dir = \"./NER/output\"    #输出路径\n",
				"vocab_file = \"./NER/chinese_L-12_H-768_A-12/vocab.txt\"    #中文字典路径\n",
				"data_config_path = \"./NER/chinese_L-12_H-768_A-12/bert_config.json\"    #预训练模型参数路径\n",
				"init_checkpoint = \"./NER/chinese_L-12_H-768_A-12/bert_model.ckpt\"    #预训练模型checkpoint路径\n",
				"max_seq_length = 128    #预先设定的最大序列长度\n",
				"batch_size = 64    #批尺寸\n",
				"num_train_epochs = 3.0    #训练轮数\n",
				"print(\"数据集路径为：\",data_dir)\n",
				"print(\"输出路径为：\",output_dir)\n",
				"print(\"中文字典路径为：\",vocab_file)\n",
				"print(\"预训练模型参数路径为：\",data_config_path)\n",
				"print(\"预训练模型checkpoint路径为：\",init_checkpoint)\n",
				"print(\"预先设定的最大序列长度为：\",max_seq_length)\n",
				"print(\"批尺寸为：\",batch_size)\n",
				"print(\"训练轮数：\",num_train_epochs)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"***最佳实践：***\n",
				"\n",
				"***batch_size = 64    #批尺寸，本案例中为了加快训练速度只设置为64，如果需要提高训练精度可适当增大。***\n",
				"\n",
				"***num_train_epochs = 3.0 #训练轮数，本案例中为了加快训练速度只设置为3，如果需要提高训练精度可适当增大。***"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"首先，定义一个NER处理部分的字典为processor，打印labels"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'X', '[CLS]', '[SEP]']\n"
				]
			}],
			"source": [
				"#将 TensorFlow 日志信息输出到屏幕\n",
				"tf.logging.set_verbosity(tf.logging.INFO)\n",
				"from NER.src.models import InputFeatures, InputExample, DataProcessor, NerProcessor\n",
				"\n",
				"#构造NER processor类，用来获取训练集、验证集、测试集，读取BIO标注数据\n",
				"processors = {\"ner\": NerProcessor }\n",
				"processor = processors[\"ner\"](output_dir)\n",
				"\n",
				"#输出processor的labels 确认构造processor成功\n",
				"label_list = processor.get_labels()\n",
				"print(\"labels:\", label_list)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"以上labels分别表示：\n",
				"- O：非标注实体\n",
				"- B-PER：人名首字\n",
				"- I-PER：人名非首字\n",
				"- B-ORG：组织首字\n",
				"- I-ORG：组织名非首字\n",
				"- B-LOC：地名首字\n",
				"- I-LOC：地名非首字\n",
				"- X：未知\n",
				"- [CLS]：句首\n",
				"- [SEP]：句尾"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"从预训练模型的json文件中加载参数，打印预训练参数，读取中文字典。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"显示配置信息:\n",
					"attention_probs_dropout_prob:0.1\n",
					"directionality:bidi\n",
					"hidden_act:gelu\n",
					"hidden_dropout_prob:0.1\n",
					"hidden_size:768\n",
					"initializer_range:0.02\n",
					"intermediate_size:3072\n",
					"max_position_embeddings:512\n",
					"num_attention_heads:12\n",
					"num_hidden_layers:12\n",
					"pooler_fc_size:768\n",
					"pooler_num_attention_heads:12\n",
					"pooler_num_fc_layers:3\n",
					"pooler_size_per_head:128\n",
					"pooler_type:first_token_transform\n",
					"type_vocab_size:2\n",
					"vocab_size:21128\n",
					"num_train_steps:978\n",
					"num_warmup_steps:97\n",
					"num_train_size:20864\n",
					"\n",
					"加载参数完毕！\n"
				]
			}],
			"source": [
				"data_config = json.load(codecs.open(data_config_path))\n",
				"train_examples = processor.get_train_examples(data_dir)    #获取训练数据\n",
				"num_train_steps = int(len(train_examples) / batch_size * num_train_epochs)    #训练步骤\n",
				"num_warmup_steps = int(num_train_steps * 0.1)    #预热训练步骤\n",
				"data_config['num_train_steps'] = num_train_steps\n",
				"data_config['num_warmup_steps'] = num_warmup_steps\n",
				"data_config['num_train_size'] = len(train_examples)\n",
				"\n",
				"print(\"显示配置信息:\")\n",
				"for key,value in data_config.items():\n",
				"    print('{key}:{value}'.format(key = key, value = value))\n",
				"\n",
				"#读取bert 官方提供的预训练模型参数配置(对应文件： ~/work/NER/chinese_L-12_H-768_A-12/bert_config.json)\n",
				"bert_config = modeling.BertConfig.from_json_file(data_config_path)\n",
				"#读取bert 预训练模型中文字典(对应文件： ~/work/NER/chinese_L-12_H-768_A-12/vocab.txt)\n",
				"tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
				"#tf.estimator运行参数\n",
				"run_config = tf.estimator.RunConfig(\n",
				"    model_dir=output_dir,\n",
				"    save_summary_steps=500,\n",
				"    save_checkpoints_steps=500,\n",
				"    session_config=tf.ConfigProto(\n",
				"        log_device_placement=False,\n",
				"        inter_op_parallelism_threads=0,\n",
				"        intra_op_parallelism_threads=0,\n",
				"        allow_soft_placement=True\n",
				"    )\n",
				")\n",
				"\n",
				"print(\"\\n加载参数完毕！\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"BERT的输入部分是个线性序列，句子通过分隔符分割，句前和句后分别增加两个标识符号[CLS]和[SEP]。\n",
				"\n",
				"![BERT输入](./img/bert输入.png)\n",
				"\n",
				"- **token embeddings**：词向量，将各个词转换成固定维度的向量。在BERT中，每个词会被转换成768维的向量表示。输入文本在送入token embeddings 层之前要先进行tokenization处理。在目前的BERT模型中，文章作者还将英文词汇作进一步切割，划分为更细粒度的语义单位（WordPiece），例如：将playing分割为play和##ing。对于中文，目前作者尚未对输入文本进行分词，而是直接将单字作为构成文本的基本单位。\n",
				"\n",
				"- **position embeddings**：位置向量，表示位置信息，由于词顺序是很重要的特征，出现在文本不同位置的字/词所携带的语义信息存在差异（比如：“我爱你”和“你爱我”），因此，BERT模型对不同位置的字/词分别附加一个不同的向量以作区分。\n",
				"\n",
				"- **segment embeddings**：文本向量，用来区别两种句子，只有两种向量表示。前一个向量是把0赋给第一个句子中的各个token, 后一个向量是把1赋给第二个句子中的各个token。如果输入仅仅只有一个句子，那么它的segment embedding就是全0。\n",
				"\n",
				"最后，BERT模型将字向量、文本向量和位置向量的加和作为模型输入。\n",
				"\n",
				"BERT能够处理最长512个token的输入序列。论文作者通过让BERT在各个位置上学习一个向量表示来讲序列顺序的信息编码进来。这意味着Position Embeddings layer 实际上就是一个大小为 (512, 768) 的lookup表，表的第一行是代表第一个序列的第一个位置，第二行代表序列的第二个位置，以此类推。因此，如果有这样两个句子“Hello world” 和“Hi there”, “Hello” 和“Hi”会由完全相同的position embeddings，因为他们都是句子的第一个词。同理，“world” 和“there”也会有相同的position embedding。\n",
				"\n",
				"在此，我们将训练集字符转换成的向量特征储存在tf_record文件中。tfrecord格式是tensorflow官方推荐的数据格式，把数据、标签进行统一的存储。tfrecord内部使用了“Protocol Buffer”二进制数据编码方案，它只占用一个内存块，只需要一次性加载一个二进制文件的方式即可，简单，快速，尤其对大型训练数据很友好。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {},
			"outputs": [{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:tensorflow:Writing example 0 of 20864\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: train-0\n",
						"INFO:tensorflow:tokens: 全 国 人 民 代 表 大 会 澳 门 特 别 行 政 区 筹 备 委 员 会 第 一 次 全 体 会 议 今 天 上 午 在 北 京 人 民 大 会 堂 开 幕 ， 国 务 院 副 总 理 、 筹 委 会 主 任 委 员 钱 其 琛 在 致 开 幕 词 中 指 出 ， 筹 建 澳 门 特 别 行 政 区 的 工 作 已 经 启 动 ， 筹 委 会 面 临 的 工 作 是 大 量 的 、 紧 迫 的 ， 筹 委 们 任 重 道 远 ， 希 望 大 家 齐 心 协 力 为 澳 门 的 平 稳 过 渡\n",
						"INFO:tensorflow:input_ids: 101 1059 1744 782 3696 807 6134 1920 833 4078 7305 4294 1166 6121 3124 1277 5040 1906 1999 1447 833 5018 671 3613 1059 860 833 6379 791 1921 677 1286 1762 1266 776 782 3696 1920 833 1828 2458 2391 8024 1744 1218 7368 1199 2600 4415 510 5040 1999 833 712 818 1999 1447 7178 1071 4422 1762 5636 2458 2391 6404 704 2900 1139 8024 5040 2456 4078 7305 4294 1166 6121 3124 1277 4638 2339 868 2347 5307 1423 1220 8024 5040 1999 833 7481 707 4638 2339 868 3221 1920 7030 4638 510 5165 6833 4638 8024 5040 1999 812 818 7028 6887 6823 8024 2361 3307 1920 2157 7970 2552 1291 1213 711 4078 7305 4638 2398 4937 6814 3941 102\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 6 7 6 7 7 7 7 1 1 1 4 5 5 1 1 1 1 4 5 5 1 1 1 1 2 3 3 1 1 1 1 1 1 1 1 1 1 1 6 7 7 7 7 7 7 1 1 1 1 1 1 1 1 4 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 7 1 1 1 1 1 10\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: train-1\n",
						"INFO:tensorflow:tokens: 全 国 人 大 常 委 会 副 委 员 长 周 光 召 、 信 息 产 业 部 部 长 吴 基 传 、 英 特 尔 公 司 董 事 长 安 德 鲁 · 葛 鲁 夫 等 应 邀 出 席 了 这 次 活 动 。\n",
						"INFO:tensorflow:input_ids: 101 1059 1744 782 1920 2382 1999 833 1199 1999 1447 7270 1453 1045 1374 510 928 2622 772 689 6956 6956 7270 1426 1825 837 510 5739 4294 2209 1062 1385 5869 752 7270 2128 2548 7826 185 5867 7826 1923 5023 2418 6913 1139 2375 749 6821 3613 3833 1220 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 4 5 5 5 5 5 5 1 1 1 1 2 3 3 1 4 5 5 5 5 1 1 2 3 3 1 4 5 5 5 5 1 1 1 2 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: train-2\n",
						"INFO:tensorflow:tokens: 乡 党 委 书 记 韩 二 秃 介 绍 说 ， 西 柏 坡 乡 是 个 移 民 乡 ， 修 岗 南 水 库 把 各 村 的 好 地 都 占 了 ， 剩 下 的 就 是 荒 山 。\n",
						"INFO:tensorflow:input_ids: 101 740 1054 1999 741 6381 7506 753 4901 792 5305 6432 8024 6205 3377 1786 740 3221 702 4919 3696 740 8024 934 2266 1298 3717 2417 2828 1392 3333 4638 1962 1765 6963 1304 749 8024 1197 678 4638 2218 3221 5774 2255 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 4 5 5 1 1 2 3 3 1 1 1 1 6 7 7 7 1 1 1 1 1 1 1 6 7 7 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: train-3\n",
						"INFO:tensorflow:tokens: 泰 国 总 理 川 · 立 派 最 近 在 讲 话 中 表 示 ， 泰 国 政 府 将 根 据 努 军 委 员 会 的 金 融 风 暴 调 查 总 结 报 告 和 专 案 审 查 委 员 会 的 审 查 ， 坚 决 、 果 断 处 理 造 成 国 家 巨 大 经 济 损 失 的 有 关 人 物 。\n",
						"INFO:tensorflow:input_ids: 101 3805 1744 2600 4415 2335 185 4989 3836 3297 6818 1762 6382 6413 704 6134 4850 8024 3805 1744 3124 2424 2199 3418 2945 1222 1092 1999 1447 833 4638 7032 6084 7599 3274 6444 3389 2600 5310 2845 1440 1469 683 3428 2144 3389 1999 1447 833 4638 2144 3389 8024 1780 1104 510 3362 3171 1905 4415 6863 2768 1744 2157 2342 1920 5307 3845 2938 1927 4638 3300 1068 782 4289 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 6 7 1 1 2 3 3 3 1 1 1 1 1 1 1 1 1 6 7 1 1 1 1 1 4 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 4 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: train-4\n",
						"INFO:tensorflow:tokens: 本 报 巴 黎 6 月 8 日 电 记 者 陈 昭 、 王 芳 报 道 ： 在 今 天 下 午 结 束 的 国 际 足 联 第 五 十 一 届 大 会 上 ， 布 拉 特 当 选 为 新 的 国 际 足 联 主 席 。\n",
						"INFO:tensorflow:input_ids: 101 3315 2845 2349 7944 127 3299 129 3189 4510 6381 5442 7357 3220 510 4374 5710 2845 6887 8038 1762 791 1921 678 1286 5310 3338 4638 1744 7354 6639 5468 5018 758 1282 671 2237 1920 833 677 8024 2357 2861 4294 2496 6848 711 3173 4638 1744 7354 6639 5468 712 2375 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 1 6 7 1 1 1 1 1 1 1 2 3 1 2 3 1 1 1 1 1 1 1 1 1 1 1 4 5 5 5 1 1 1 1 1 1 1 1 1 2 3 3 1 1 1 1 1 4 5 5 5 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:Writing example 5000 of 20864\n",
						"INFO:tensorflow:Writing example 10000 of 20864\n",
						"INFO:tensorflow:Writing example 15000 of 20864\n",
						"INFO:tensorflow:Writing example 20000 of 20864\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"数据集读取完成！\n"
					]
				}
			],
			"source": [
				"#convert_single_example函数用来把一句话的每个字转换成BERT的输入，即词向量、文本向量、位置向量，\n",
				"#以及每个字对应的实体标签。同时在句首和句尾分别加上[CLS]和[SEP]标志。\n",
				"def convert_single_example(ex_index, example, label_list, max_seq_length, \n",
				"                           tokenizer, output_dir, mode):\n",
				"    #将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n",
				"    label_map = {}\n",
				"    # 1表示从1开始对标签进行index化\n",
				"    for (i, label) in enumerate(label_list, 1):\n",
				"        label_map[label] = i\n",
				"    # 保存label->index 的map\n",
				"    if not os.path.exists(os.path.join(output_dir, 'label2id.pkl')):\n",
				"        with codecs.open(os.path.join(output_dir, 'label2id.pkl'), 'wb') as w:\n",
				"            pickle.dump(label_map, w)\n",
				"\n",
				"    textlist = example.text.split(' ')\n",
				"    labellist = example.label.split(' ')\n",
				"    tokens = []\n",
				"    labels = []\n",
				"    for i, word in enumerate(textlist):\n",
				"        # 中文是分字,但是对于一些不在BERT的vocab.txt中得字符会被进行WordPiece处理\n",
				"        #（例如中文的引号），可以将所有的分字操作替换为list(input)\n",
				"        token = tokenizer.tokenize(word)\n",
				"        tokens.extend(token)\n",
				"        label_1 = labellist[i]\n",
				"        for m in range(len(token)):\n",
				"            if m == 0:\n",
				"                labels.append(label_1)\n",
				"            else:  # 一般不会出现else\n",
				"                labels.append(\"X\")\n",
				"    # 序列截断\n",
				"    if len(tokens) >= max_seq_length - 1:\n",
				"        tokens = tokens[0:(max_seq_length - 2)]\n",
				"        # -2 是因为序列需要加一个句首和句尾标志\n",
				"        labels = labels[0:(max_seq_length - 2)]\n",
				"    ntokens = []\n",
				"    segment_ids = []\n",
				"    label_ids = []\n",
				"    ntokens.append(\"[CLS]\")  # 句子开始设置CLS 标志\n",
				"    segment_ids.append(0)\n",
				"    label_ids.append(label_map[\"[CLS]\"])  \n",
				"    for i, token in enumerate(tokens):\n",
				"        ntokens.append(token)\n",
				"        segment_ids.append(0)\n",
				"        label_ids.append(label_map[labels[i]])\n",
				"    ntokens.append(\"[SEP]\")  # 句尾添加[SEP] 标志\n",
				"    segment_ids.append(0)\n",
				"    label_ids.append(label_map[\"[SEP]\"])\n",
				"    input_ids = tokenizer.convert_tokens_to_ids(ntokens)  # 将序列中的字(ntokens)转化为ID形式\n",
				"    input_mask = [1] * len(input_ids)\n",
				"\n",
				"    while len(input_ids) < max_seq_length:\n",
				"        input_ids.append(0)\n",
				"        input_mask.append(0)\n",
				"        segment_ids.append(0)\n",
				"        label_ids.append(0)\n",
				"        ntokens.append(\"**NULL**\")\n",
				"\n",
				"    assert len(input_ids) == max_seq_length\n",
				"    assert len(input_mask) == max_seq_length\n",
				"    assert len(segment_ids) == max_seq_length\n",
				"    assert len(label_ids) == max_seq_length\n",
				"\n",
				"    # 打印前5个数据\n",
				"    if ex_index < 5:\n",
				"        tf.logging.info(\"*** Example ***\")\n",
				"        tf.logging.info(\"guid: %s\" % (example.guid))    #每个句子的独立id\n",
				"        tf.logging.info(\"tokens: %s\" % \" \".join([tokenization.printable_text(x) for x in tokens]))    #每个字作为一个token\n",
				"        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))    #字向量token embeddings\n",
				"        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))    #位置向量position embeddings\n",
				"        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))    #文本向量segment embeddings\n",
				"        tf.logging.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))    #标签labels\n",
				"\n",
				"    # 结构化为一个类\n",
				"    feature = InputFeatures(\n",
				"        input_ids=input_ids,\n",
				"        input_mask=input_mask,\n",
				"        segment_ids=segment_ids,\n",
				"        label_ids=label_ids,\n",
				"    )\n",
				"   \n",
				"    return feature\n",
				"\n",
				"#filed_based_convert_examples_to_features函数使用convert_single_example函数把所有句子example转化成作为BERT输入的features，\n",
				"#并保存为tf_record格式\n",
				"def filed_based_convert_examples_to_features(\n",
				"        examples, label_list, max_seq_length, tokenizer, output_file, mode=None):\n",
				"    #将数据转化为TF_Record 结构，作为模型数据输入\n",
				"    writer = tf.python_io.TFRecordWriter(output_file)\n",
				"    # 遍历数据集\n",
				"    for (ex_index, example) in enumerate(examples):\n",
				"        if ex_index % 5000 == 0:\n",
				"            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
				"        # 对于每一个训练样本,\n",
				"        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode)\n",
				"\n",
				"        def create_int_feature(values):\n",
				"            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
				"            return f\n",
				"\n",
				"        features = collections.OrderedDict()\n",
				"        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
				"        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
				"        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
				"        features[\"label_ids\"] = create_int_feature(feature.label_ids)\n",
				"        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
				"        writer.write(tf_example.SerializeToString())\n",
				"    print(\"数据集读取完成！\")\n",
				"\n",
				"#在output_dir中保存训练集的tf_record\n",
				"train_file = os.path.join(output_dir, \"train.tf_record\")\n",
				"\n",
				"#将训练集中字符转化为features作为训练的输入\n",
				"filed_based_convert_examples_to_features(\n",
				"            train_examples, label_list, max_seq_length, tokenizer, output_file=train_file)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"以上输出了五个样本句子，分别标出了token embeddings，segment embeddings，position embeddings和label，预先设定的最大序列长度max_seq_length为128 ，少于128字符的用“0”补齐。其中labels的1-10分别代表：'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'X', '[CLS]', '[SEP]'。\n",
				"\n",
				"\n",
				"以第五个示例（guid = train-4）为例，分别标识为：\n",
				"\n",
				"![tokens](./img/tokens.png)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## BiLSTM+CRF层\n",
				"\n",
				"LSTM的全称是Long Short-Term Memory，它是RNN（Recurrent Neural Network，循环神经网络）的一种。LSTM区别于RNN的地方，主要就在于它在算法中加入了一个判断信息有用与否的结构，该结构中被放置了三扇门，分别叫做输入门、遗忘门和输出门。一个信息进入LSTM的网络当中，可以根据规则来判断是否有用。只有符合算法认证的信息才会留下，不符的信息则通过遗忘门被遗忘。LSTM由于其设计的特点，非常适合用于对时序数据的建模，如文本数据。BiLSTM是Bi-directional Long Short-Term Memory的缩写，是由前向LSTM与后向LSTM组合而成。这两个都连接着一个输出层。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。\n",
				"\n",
				"我们可以单独使用BiLSTM命名实体识别模型，如图。\n",
				"\n",
				"![BiLSTM](./img/LSTM.png)\n",
				"\n",
				"因为每个单词的BiLSTM输出是标签分数。我们可以选择每个单词得分最高的标签。例如，对于w0，“B-Person”得分最高（1.5），因此我们可以选择“B-Person”作为其最佳预测标签。同样，我们可以为w1选择“I-Person”，w2选择”O“，w3选择“B-Organization”，w4选择\"O\"。\n",
				" \n",
				"虽然由此可以得到句子中每个单元的正确标签，但是我们不能保证标签每次都是预测正确的。例如，下图中的例子，标签序列是“I-Organization I-Person” and “B-Organization I-Person”，很显然这是错误的。\n",
				"\n",
				"![BiLSTM2](./img/LSTM2.png)\n",
				" \n",
				"而CRF层能从训练数据中获得约束性的规则。CRF（Conditional Random Fields，条件随机场）是给定一组输入随机变量条件下，另外一组输出随机变量的条件概率分布模型，其特点是假设输出变量构成马尔可夫随机场。CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合法的。这些约束可以是：\n",
				"\n",
				"- 句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”\n",
				"- 标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合法的序列, 但是“B-Person I-Organization” 是非法标签序列\n",
				"- 标签序列“O I-label” is 非法的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。\n",
				"\n",
				"有了这些约束，标签序列预测中非法序列出现的概率将会大大降低。\n",
				"\n",
				"![CRF](./img/CRF.png)\n",
				"\n",
				"上图说明了BiLSTM层的输出是每个标签的分数。例如，对于w0，BiLSTM节点的输出为1.5（B-Person），0.9（I-Person），0.1（B-Organization），0.08（I-Organization）和0.05（O）。这些分数将是CRF层的输入。然后，BiLSTM块预测的所有分数被馈送到CRF层。在CRF层中，将选择具有最高预测分数的标签序列作为最佳答案。\n",
				"\n",
				"下面引入BiLSTM+CRF层，具体结构可从src文件夹下的models.py中的BLSTM_CRF类中找到。把BiLSTM+CRF作为下游模型，结合BERT建立NER模型。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"创建模型model_fn\n"
				]
			}],
			"source": [
				"#定义模型参数\n",
				"learning_rate = 5e-5    #学习率（关系模型收敛速率）\n",
				"dropout_rate = 1.0    #随机失活率（减少过拟合）\n",
				"lstm_size=1    \n",
				"cell='lstm'\n",
				"num_layers=1\n",
				"\n",
				"#从models里引用BiLSTM-CRF层\n",
				"from NER.src.models import BLSTM_CRF\n",
				"from tensorflow.contrib.layers.python.layers import initializers\n",
				"\n",
				"def create_model(bert_config, is_training, input_ids, input_mask,\n",
				"                 segment_ids, labels, num_labels, use_one_hot_embeddings,\n",
				"                 dropout_rate=dropout_rate, lstm_size=1, cell='lstm', num_layers=1):\n",
				"    # 使用数据加载BertModel,获取对应的字embedding\n",
				"    model = modeling.BertModel(\n",
				"        config=bert_config,\n",
				"        is_training=is_training,\n",
				"        input_ids=input_ids,\n",
				"        input_mask=input_mask,\n",
				"        token_type_ids=segment_ids,\n",
				"        use_one_hot_embeddings=use_one_hot_embeddings\n",
				"    )\n",
				"    # 获取对应的embedding 输入数据[batch_size, seq_length, embedding_size]\n",
				"    embedding = model.get_sequence_output()\n",
				"    max_seq_length = embedding.shape[1].value\n",
				"    # 算序列真实长度\n",
				"    used = tf.sign(tf.abs(input_ids))\n",
				"    lengths = tf.reduce_sum(used, reduction_indices=1)  # [batch_size] 大小的向量，包含了当前batch中的序列长度\n",
				"    # 加载BLSTM-CRF模型对象\n",
				"    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=1, cell_type='lstm', num_layers=1,\n",
				"                          dropout_rate=dropout_rate, initializers=initializers, num_labels=num_labels,\n",
				"                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n",
				"    rst = blstm_crf.add_blstm_crf_layer(crf_only=True)\n",
				"    return rst\n",
				"\n",
				"def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
				"                     num_train_steps, num_warmup_steps,use_one_hot_embeddings=False):\n",
				"    #构建模型\n",
				"    def model_fn(features, labels, mode, params):\n",
				"        tf.logging.info(\"*** Features ***\")\n",
				"        for name in sorted(features.keys()):\n",
				"            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
				"        input_ids = features[\"input_ids\"]\n",
				"        input_mask = features[\"input_mask\"]\n",
				"        segment_ids = features[\"segment_ids\"]\n",
				"        label_ids = features[\"label_ids\"]\n",
				"\n",
				"        print('shape of input_ids', input_ids.shape)\n",
				"        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
				"\n",
				"        # 使用参数构建模型,input_idx 就是输入的样本idx表示，label_ids 就是标签的idx表示\n",
				"        total_loss, logits, trans, pred_ids = create_model(\n",
				"            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
				"            num_labels, False, dropout_rate, lstm_size, cell, num_layers)\n",
				"\n",
				"        tvars = tf.trainable_variables()\n",
				"        # 加载BERT模型\n",
				"        if init_checkpoint:\n",
				"            (assignment_map, initialized_variable_names) = \\\n",
				"                 modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
				"            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
				"        \n",
				"        output_spec = None\n",
				"        if mode == tf.estimator.ModeKeys.TRAIN:\n",
				"            train_op = optimization.create_optimizer(\n",
				"                 total_loss, learning_rate, num_train_steps, num_warmup_steps, False)\n",
				"            hook_dict = {}\n",
				"            hook_dict['loss'] = total_loss\n",
				"            hook_dict['global_steps'] = tf.train.get_or_create_global_step()\n",
				"            logging_hook = tf.train.LoggingTensorHook(\n",
				"                hook_dict, every_n_iter=100)\n",
				"\n",
				"            output_spec = tf.estimator.EstimatorSpec(\n",
				"                mode=mode,\n",
				"                loss=total_loss,\n",
				"                train_op=train_op,\n",
				"                training_hooks=[logging_hook])\n",
				"\n",
				"        elif mode == tf.estimator.ModeKeys.EVAL:\n",
				"            def metric_fn(label_ids, pred_ids):\n",
				"                # 首先对结果进行维特比解码\n",
				"                # crf 解码\n",
				"                return {\n",
				"                    \"eval_loss\": tf.metrics.mean_squared_error(labels=label_ids, predictions=pred_ids),   }\n",
				"            \n",
				"            eval_metrics = metric_fn(label_ids, pred_ids)\n",
				"            output_spec = tf.estimator.EstimatorSpec(\n",
				"                mode=mode,\n",
				"                loss=total_loss,\n",
				"                eval_metric_ops=eval_metrics\n",
				"            )\n",
				"        else:\n",
				"            output_spec = tf.estimator.EstimatorSpec(\n",
				"                mode=mode,\n",
				"                predictions=pred_ids\n",
				"            )\n",
				"        return output_spec\n",
				"\n",
				"    return model_fn\n",
				"\n",
				"print(\"创建模型model_fn\")\n",
				"\n",
				"model_fn = model_fn_builder(\n",
				"        bert_config=bert_config,\n",
				"        num_labels=len(label_list) + 1,\n",
				"        init_checkpoint=init_checkpoint,\n",
				"        learning_rate=learning_rate,\n",
				"        num_train_steps=num_train_steps,\n",
				"        num_warmup_steps=num_warmup_steps,\n",
				"        use_one_hot_embeddings=False)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 在训练集上进行训练"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"metadata": {},
			"outputs": [{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:tensorflow:***** Running training *****\n",
						"INFO:tensorflow:  Num examples = 20864\n",
						"INFO:tensorflow:  Batch size = 64\n",
						"INFO:tensorflow:  Num steps = 978\n",
						"INFO:tensorflow:Using config: {'_model_dir': './NER/output', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
						", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3663340470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
						"INFO:tensorflow:Calling model_fn.\n",
						"INFO:tensorflow:*** Features ***\n",
						"INFO:tensorflow:  name = input_ids, shape = (32, 128)\n",
						"INFO:tensorflow:  name = input_mask, shape = (32, 128)\n",
						"INFO:tensorflow:  name = label_ids, shape = (32, 128)\n",
						"INFO:tensorflow:  name = segment_ids, shape = (32, 128)\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"进行训练，约20min\n",
						"shape of input_ids (32, 128)\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:tensorflow:Done calling model_fn.\n",
						"INFO:tensorflow:Create CheckpointSaverHook.\n",
						"INFO:tensorflow:Graph was finalized.\n",
						"INFO:tensorflow:Running local_init_op.\n",
						"INFO:tensorflow:Done running local_init_op.\n",
						"INFO:tensorflow:Saving checkpoints for 0 into ./NER/output/model.ckpt.\n",
						"INFO:tensorflow:loss = 106.046326, step = 0\n",
						"INFO:tensorflow:global_steps = 0, loss = 106.046326\n",
						"INFO:tensorflow:global_step/sec: 1.46726\n",
						"INFO:tensorflow:loss = 46.631207, step = 100 (68.156 sec)\n",
						"INFO:tensorflow:global_steps = 100, loss = 46.631207 (68.155 sec)\n",
						"INFO:tensorflow:global_step/sec: 1.81712\n",
						"INFO:tensorflow:loss = 37.328934, step = 200 (55.033 sec)\n",
						"INFO:tensorflow:global_steps = 200, loss = 37.328934 (55.033 sec)\n",
						"INFO:tensorflow:global_step/sec: 1.82583\n",
						"INFO:tensorflow:loss = 37.050262, step = 300 (54.769 sec)\n",
						"INFO:tensorflow:global_steps = 300, loss = 37.050262 (54.769 sec)\n",
						"INFO:tensorflow:global_step/sec: 1.82732\n",
						"INFO:tensorflow:loss = 37.160378, step = 400 (54.725 sec)\n",
						"INFO:tensorflow:global_steps = 400, loss = 37.160378 (54.725 sec)\n",
						"INFO:tensorflow:Saving checkpoints for 500 into ./NER/output/model.ckpt.\n",
						"INFO:tensorflow:global_step/sec: 1.55118\n",
						"INFO:tensorflow:loss = 38.998734, step = 500 (64.466 sec)\n",
						"INFO:tensorflow:global_steps = 500, loss = 38.998734 (64.466 sec)\n",
						"INFO:tensorflow:global_step/sec: 1.82595\n",
						"INFO:tensorflow:loss = 40.138336, step = 600 (54.767 sec)\n",
						"INFO:tensorflow:global_steps = 600, loss = 40.138336 (54.767 sec)\n",
						"INFO:tensorflow:global_step/sec: 1.82669\n",
						"INFO:tensorflow:loss = 37.701122, step = 700 (54.744 sec)\n",
						"INFO:tensorflow:global_steps = 700, loss = 37.701122 (54.744 sec)\n",
						"INFO:tensorflow:global_step/sec: 1.82625\n",
						"INFO:tensorflow:loss = 38.045074, step = 800 (54.757 sec)\n",
						"INFO:tensorflow:global_steps = 800, loss = 38.045074 (54.757 sec)\n",
						"INFO:tensorflow:global_step/sec: 1.83092\n",
						"INFO:tensorflow:loss = 31.680422, step = 900 (54.617 sec)\n",
						"INFO:tensorflow:global_steps = 900, loss = 31.680422 (54.617 sec)\n",
						"INFO:tensorflow:Saving checkpoints for 978 into ./NER/output/model.ckpt.\n",
						"INFO:tensorflow:Loss for final step: 38.36219.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"训练结束\n"
					]
				}
			],
			"source": [
				"def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
				"    name_to_features = {\n",
				"        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
				"        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
				"        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
				"        \"label_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
				"    }\n",
				"\n",
				"    def _decode_record(record, name_to_features):\n",
				"        example = tf.parse_single_example(record, name_to_features)\n",
				"        for name in list(example.keys()):\n",
				"            t = example[name]\n",
				"            if t.dtype == tf.int64:\n",
				"                t = tf.to_int32(t)\n",
				"            example[name] = t\n",
				"        return example\n",
				"\n",
				"    def input_fn(params):\n",
				"        params[\"batch_size\"] = 32\n",
				"        batch_size = params[\"batch_size\"]\n",
				"        d = tf.data.TFRecordDataset(input_file)\n",
				"        if is_training:\n",
				"            d = d.repeat()\n",
				"            d = d.shuffle(buffer_size=300)\n",
				"        d = d.apply(tf.contrib.data.map_and_batch(\n",
				"            lambda record: _decode_record(record, name_to_features),\n",
				"            batch_size=batch_size,\n",
				"            drop_remainder=drop_remainder\n",
				"        ))\n",
				"        return d\n",
				"\n",
				"    return input_fn\n",
				"\n",
				"#训练输入\n",
				"train_input_fn = file_based_input_fn_builder(\n",
				"            input_file=train_file,\n",
				"            seq_length=max_seq_length,\n",
				"            is_training=True,\n",
				"            drop_remainder=True)\n",
				"\n",
				"num_train_size = len(train_examples)\n",
				"\n",
				"print(\"进行训练，约20min\")\n",
				"tf.logging.info(\"***** Running training *****\")\n",
				"tf.logging.info(\"  Num examples = %d\", num_train_size)\n",
				"tf.logging.info(\"  Batch size = %d\", batch_size)\n",
				"tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
				"\n",
				"#模型预测estimator\n",
				"estimator = tf.estimator.Estimator(\n",
				"        model_fn=model_fn,\n",
				"        config=run_config,\n",
				"        params={\n",
				"        'batch_size':batch_size\n",
				"    })\n",
				"\n",
				"estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
				"print(\"训练结束\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 在验证集上验证模型"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {},
			"outputs": [{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:tensorflow:Writing example 0 of 4631\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: dev-0\n",
						"INFO:tensorflow:tokens: 1 9 9 0 年 4 月 通 过 的 《 中 华 人 民 共 和 国 香 港 特 别 行 政 区 基 本 法 》 明 确 规 定 ， 香 港 特 区 的 终 审 权 属 于 特 区 终 审 法 院 。\n",
						"INFO:tensorflow:input_ids: 101 122 130 130 121 2399 125 3299 6858 6814 4638 517 704 1290 782 3696 1066 1469 1744 7676 3949 4294 1166 6121 3124 1277 1825 3315 3791 518 3209 4802 6226 2137 8024 7676 3949 4294 1277 4638 5303 2144 3326 2247 754 4294 1277 5303 2144 3791 7368 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 1 1 1 1 1 1 1 1 1 1 6 7 7 7 7 7 7 6 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 6 7 7 7 1 1 1 1 1 1 4 5 5 5 5 5 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: dev-1\n",
						"INFO:tensorflow:tokens: 新 华 社 北 京 3 月 1 1 日 电 最 高 人 民 法 院 院 长 任 建 新 今 天 在 八 届 全 国 人 大 五 次 会 议 第 五 次 全 体 会 议 作 报 告 时 说 ， 严 肃 执 法 是 社 会 主 义 法 制 建 设 的 重 要 内 容 ， 是 党 和 国 家 对 司 法 活 动 的 根 本 要 求 。\n",
						"INFO:tensorflow:input_ids: 101 3173 1290 4852 1266 776 124 3299 122 122 3189 4510 3297 7770 782 3696 3791 7368 7368 7270 818 2456 3173 791 1921 1762 1061 2237 1059 1744 782 1920 758 3613 833 6379 5018 758 3613 1059 860 833 6379 868 2845 1440 3198 6432 8024 698 5484 2809 3791 3221 4852 833 712 721 3791 1169 2456 6392 4638 7028 6206 1079 2159 8024 3221 1054 1469 1744 2157 2190 1385 3791 3833 1220 4638 3418 3315 6206 3724 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 4 5 5 6 7 1 1 1 1 1 1 4 5 5 5 5 5 1 1 2 3 3 1 1 1 4 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: dev-2\n",
						"INFO:tensorflow:tokens: 他 的 收 藏 品 超 过 了 上 海 的 收 藏 名 家 严 汉 祥 和 扬 州 的 季 之 光 （ 严 收 藏 有 火 花 1 3 0 多 万 枚 ， 季 则 有 1 5 0 万 枚 ） ， 成 为 [UNK] 中 国 火 花 收 藏 第 一 人 [UNK] 。\n",
						"INFO:tensorflow:input_ids: 101 800 4638 3119 5966 1501 6631 6814 749 677 3862 4638 3119 5966 1399 2157 698 3727 4872 1469 2813 2336 4638 2108 722 1045 8020 698 3119 5966 3300 4125 5709 122 124 121 1914 674 3361 8024 2108 1156 3300 122 126 121 674 3361 8021 8024 2768 711 100 704 1744 4125 5709 3119 5966 5018 671 782 100 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 1 1 1 1 1 1 1 6 7 1 1 1 1 1 2 3 3 1 6 7 1 2 3 3 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 6 7 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: dev-3\n",
						"INFO:tensorflow:tokens: 另 外 ， 科 威 特 首 相 萨 阿 德 等 高 级 官 员 在 科 威 特 会 见 以 对 外 友 协 会 长 齐 怀 远 为 首 的 中 国 代 表 团 时 ， 对 邓 小 平 逝 世 表 示 了 深 切 的 哀 悼 。\n",
						"INFO:tensorflow:input_ids: 101 1369 1912 8024 4906 2014 4294 7674 4685 5855 7350 2548 5023 7770 5277 2135 1447 1762 4906 2014 4294 833 6224 809 2190 1912 1351 1291 833 7270 7970 2577 6823 711 7674 4638 704 1744 807 6134 1730 3198 8024 2190 6924 2207 2398 6860 686 6134 4850 749 3918 1147 4638 1500 2656 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 1 1 6 7 7 1 1 2 3 3 1 1 1 1 1 1 6 7 7 1 1 1 4 5 5 5 1 1 2 3 3 1 1 1 4 5 5 5 5 1 1 1 2 3 3 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: dev-4\n",
						"INFO:tensorflow:tokens: 本 报 北 京 1 月 2 1 日 讯 记 者 苏 宁 报 道 ： 中 共 中 央 书 记 处 书 记 、 中 央 政 法 委 书 记 任 建 新 今 天 在 中 国 法 学 会 第 四 次 会 员 代 表 大 会 上 发 表 讲 话 时 指 出 ： 社 会 主 义 民 主 与 法 制 建 设 是 一 项 宏 大 的 系 统 工 程 ， 涉 及 到 政 治 、 经 济 、 文 化 、\n",
						"INFO:tensorflow:input_ids: 101 3315 2845 1266 776 122 3299 123 122 3189 6380 6381 5442 5722 2123 2845 6887 8038 704 1066 704 1925 741 6381 1905 741 6381 510 704 1925 3124 3791 1999 741 6381 818 2456 3173 791 1921 1762 704 1744 3791 2110 833 5018 1724 3613 833 1447 807 6134 1920 833 677 1355 6134 6382 6413 3198 2900 1139 8038 4852 833 712 721 3696 712 680 3791 1169 2456 6392 3221 671 7555 2131 1920 4638 5143 5320 2339 4923 8024 3868 1350 1168 3124 3780 510 5307 3845 510 3152 1265 510 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 1 6 7 1 1 1 1 1 1 1 1 2 3 1 1 1 4 5 5 5 5 5 5 1 1 1 4 5 5 5 5 1 1 2 3 3 1 1 1 4 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:***** Running evaluation *****\n",
						"INFO:tensorflow:  Num examples = 4631\n",
						"INFO:tensorflow:  Batch size = 64\n",
						"INFO:tensorflow:Calling model_fn.\n",
						"INFO:tensorflow:*** Features ***\n",
						"INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
						"INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
						"INFO:tensorflow:  name = label_ids, shape = (?, 128)\n",
						"INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"数据集读取完成！\n",
						"shape of input_ids (?, 128)\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:tensorflow:Done calling model_fn.\n",
						"INFO:tensorflow:Starting evaluation at 2019-05-26-05:23:35\n",
						"INFO:tensorflow:Graph was finalized.\n",
						"INFO:tensorflow:Restoring parameters from ./NER/output/model.ckpt-978\n",
						"INFO:tensorflow:Running local_init_op.\n",
						"INFO:tensorflow:Done running local_init_op.\n",
						"INFO:tensorflow:Finished evaluation at 2019-05-26-05:24:16\n",
						"INFO:tensorflow:Saving dict for global step 978: eval_loss = 0.051539555, global_step = 978, loss = 30.230492\n",
						"INFO:tensorflow:Saving 'checkpoint_path' summary for global step 978: ./NER/output/model.ckpt-978\n",
						"INFO:tensorflow:***** Eval results *****\n",
						"INFO:tensorflow:  eval_loss = 0.051539555\n",
						"INFO:tensorflow:  global_step = 978\n",
						"INFO:tensorflow:  loss = 30.230492\n"
					]
				}
			],
			"source": [
				"eval_examples = processor.get_dev_examples(data_dir)\n",
				"eval_file = os.path.join(output_dir, \"eval.tf_record\")\n",
				"filed_based_convert_examples_to_features(\n",
				"                eval_examples, label_list, max_seq_length, tokenizer, eval_file)\n",
				"data_config['eval.tf_record_path'] = eval_file\n",
				"data_config['num_eval_size'] = len(eval_examples)\n",
				"num_eval_size = data_config.get('num_eval_size', 0)\n",
				"\n",
				"# 打印验证集数据信息\n",
				"tf.logging.info(\"***** Running evaluation *****\")\n",
				"tf.logging.info(\"  Num examples = %d\", num_eval_size)\n",
				"tf.logging.info(\"  Batch size = %d\", batch_size)\n",
				"\n",
				"eval_steps = None\n",
				"eval_drop_remainder = False\n",
				"eval_input_fn = file_based_input_fn_builder(\n",
				"            input_file=eval_file,\n",
				"            seq_length=max_seq_length,\n",
				"            is_training=False,\n",
				"            drop_remainder=eval_drop_remainder)\n",
				"\n",
				"result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
				"output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
				"with codecs.open(output_eval_file, \"w\", encoding='utf-8') as writer:\n",
				"    tf.logging.info(\"***** Eval results *****\")\n",
				"    for key in sorted(result.keys()):\n",
				"        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
				"        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
				"\n",
				"# 保存数据的配置文件，避免在以后的训练过程中多次读取训练以及测试数据集，消耗时间\n",
				"if not os.path.exists(data_config_path):\n",
				"    with codecs.open(data_config_path, 'a', encoding='utf-8') as fd:\n",
				"        json.dump(data_config, fd)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 在测试集上进行测试"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"metadata": {},
			"outputs": [{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:tensorflow:Writing example 0 of 68\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: test-0\n",
						"INFO:tensorflow:tokens: 美 国 的 华 莱 士 ， 我 和 他 谈 笑 风 生 。\n",
						"INFO:tensorflow:input_ids: 101 5401 1744 4638 1290 5812 1894 8024 2769 1469 800 6448 5010 7599 4495 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 6 7 1 2 2 2 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: test-1\n",
						"INFO:tensorflow:tokens: 在 香 港 回 归 前 的 最 后 阶 段 ， 中 共 中 央 举 办 《 [UNK] 一 国 两 制 [UNK] 与 香 港 基 本 法 》 讲 座 ， 中 央 领 导 同 志 认 真 听 讲 ， 虚 心 学 习 ， 很 有 意 义 。\n",
						"INFO:tensorflow:input_ids: 101 1762 7676 3949 1726 2495 1184 4638 3297 1400 7348 3667 8024 704 1066 704 1925 715 1215 517 100 671 1744 697 1169 100 680 7676 3949 1825 3315 3791 518 6382 2429 8024 704 1925 7566 2193 1398 2562 6371 4696 1420 6382 8024 5994 2552 2110 739 8024 2523 3300 2692 721 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 6 7 1 1 1 1 1 1 1 1 1 4 5 5 5 1 1 1 1 1 1 1 1 1 1 6 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: test-2\n",
						"INFO:tensorflow:tokens: 这 表 明 ， 以 江 泽 民 同 志 为 核 心 的 党 中 央 坚 定 不 移 地 贯 彻 邓 小 平 同 志 [UNK] 一 国 两 制 [UNK] 的 伟 大 构 想 ， 不 折 不 扣 地 执 行 基 本 法 。\n",
						"INFO:tensorflow:input_ids: 101 6821 6134 3209 8024 809 3736 3813 3696 1398 2562 711 3417 2552 4638 1054 704 1925 1780 2137 679 4919 1765 6581 2515 6924 2207 2398 1398 2562 100 671 1744 697 1169 100 4638 836 1920 3354 2682 8024 679 2835 679 2807 1765 2809 6121 1825 3315 3791 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 1 1 1 1 2 3 3 1 1 1 1 1 1 4 5 5 1 1 1 1 1 1 1 2 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: test-3\n",
						"INFO:tensorflow:tokens: [UNK] 一 国 两 制 [UNK] 是 邓 小 平 同 志 的 一 个 伟 大 构 想 ， 《 中 华 人 民 共 和 国 香 港 特 别 行 政 区 基 本 法 》 是 贯 彻 落 实 [UNK] 一 国 两 制 [UNK] 伟 大 构 想 的 一 部 全 国 性 法 律 ， 是 一 部 有 鲜 明 中 国 特 色 的 法 律 。\n",
						"INFO:tensorflow:input_ids: 101 100 671 1744 697 1169 100 3221 6924 2207 2398 1398 2562 4638 671 702 836 1920 3354 2682 8024 517 704 1290 782 3696 1066 1469 1744 7676 3949 4294 1166 6121 3124 1277 1825 3315 3791 518 3221 6581 2515 5862 2141 100 671 1744 697 1169 100 836 1920 3354 2682 4638 671 6956 1059 1744 2595 3791 2526 8024 3221 671 6956 3300 7831 3209 704 1744 4294 5682 4638 3791 2526 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 1 1 1 1 1 1 2 3 3 1 1 1 1 1 1 1 1 1 1 1 6 7 7 7 7 7 7 6 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 7 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:*** Example ***\n",
						"INFO:tensorflow:guid: test-4\n",
						"INFO:tensorflow:tokens: 看 包 公 断 案 的 戏 ， 看 他 威 风 凛 凛 坐 公 堂 拍 桌 子 动 刑 具 ， 多 少 还 有 一 点 担 心 ， 总 怕 靠 这 一 套 办 法 弄 出 错 案 来 ， 放 过 了 真 正 的 坏 人 ；\n",
						"INFO:tensorflow:input_ids: 101 4692 1259 1062 3171 3428 4638 2767 8024 4692 800 2014 7599 1123 1123 1777 1062 1828 2864 3430 2094 1220 1152 1072 8024 1914 2208 6820 3300 671 4157 2857 2552 8024 2600 2586 7479 6821 671 1947 1215 3791 2462 1139 7231 3428 3341 8024 3123 6814 749 4696 3633 4638 1776 782 8039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:label_ids: 9 1 2 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
						"INFO:tensorflow:***** Running prediction*****\n",
						"INFO:tensorflow:  Num examples = 68\n",
						"INFO:tensorflow:  Batch size = 64\n",
						"INFO:tensorflow:Calling model_fn.\n",
						"INFO:tensorflow:*** Features ***\n",
						"INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
						"INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
						"INFO:tensorflow:  name = label_ids, shape = (?, 128)\n",
						"INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"数据集读取完成！\n",
						"shape of input_ids (?, 128)\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:tensorflow:Done calling model_fn.\n",
						"INFO:tensorflow:Starting evaluation at 2019-05-26-05:24:21\n",
						"INFO:tensorflow:Graph was finalized.\n",
						"INFO:tensorflow:Restoring parameters from ./NER/output/model.ckpt-978\n",
						"INFO:tensorflow:Running local_init_op.\n",
						"INFO:tensorflow:Done running local_init_op.\n",
						"INFO:tensorflow:Finished evaluation at 2019-05-26-05:24:23\n",
						"INFO:tensorflow:Saving dict for global step 978: eval_loss = 0.009765625, global_step = 978, loss = 29.149973\n",
						"INFO:tensorflow:Saving 'checkpoint_path' summary for global step 978: ./NER/output/model.ckpt-978\n",
						"INFO:tensorflow:***** Predict results *****\n",
						"INFO:tensorflow:  eval_loss = 0.009765625\n",
						"INFO:tensorflow:  global_step = 978\n",
						"INFO:tensorflow:  loss = 29.149973\n",
						"INFO:tensorflow:Calling model_fn.\n",
						"INFO:tensorflow:*** Features ***\n",
						"INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
						"INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
						"INFO:tensorflow:  name = label_ids, shape = (?, 128)\n",
						"INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"shape of input_ids (?, 128)\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:tensorflow:Done calling model_fn.\n",
						"INFO:tensorflow:Graph was finalized.\n",
						"INFO:tensorflow:Restoring parameters from ./NER/output/model.ckpt-978\n",
						"INFO:tensorflow:Running local_init_op.\n",
						"INFO:tensorflow:Done running local_init_op.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"processed 2270 tokens with 78 phrases; found: 75 phrases; correct: 68.\n",
						"\n",
						"accuracy:  99.52%; precision:  90.67%; recall:  87.18%; FB1:  88.89\n",
						"\n",
						"              LOC: precision: 100.00%; recall:  97.78%; FB1:  98.88  44\n",
						"\n",
						"              ORG: precision:  77.78%; recall:  87.50%; FB1:  82.35  9\n",
						"\n",
						"              PER: precision:  77.27%; recall:  68.00%; FB1:  72.34  22\n",
						"\n"
					]
				}
			],
			"source": [
				"token_path = os.path.join(output_dir, \"token_test.txt\")\n",
				"if os.path.exists(token_path):\n",
				"    os.remove(token_path)\n",
				"\n",
				"with codecs.open(os.path.join(output_dir, 'label2id.pkl'), 'rb') as rf:\n",
				"    label2id = pickle.load(rf)\n",
				"    id2label = {value: key for key, value in label2id.items()}\n",
				"\n",
				"predict_examples = processor.get_test_examples(data_dir)\n",
				"predict_file = os.path.join(output_dir, \"predict.tf_record\")\n",
				"filed_based_convert_examples_to_features(predict_examples, label_list,\n",
				"                                                 max_seq_length, tokenizer,\n",
				"                                                 predict_file, mode=\"test\")\n",
				"\n",
				"tf.logging.info(\"***** Running prediction*****\")\n",
				"tf.logging.info(\"  Num examples = %d\", len(predict_examples))\n",
				"tf.logging.info(\"  Batch size = %d\", batch_size)\n",
				"    \n",
				"predict_drop_remainder = False\n",
				"predict_input_fn = file_based_input_fn_builder(\n",
				"            input_file=predict_file,\n",
				"            seq_length=max_seq_length,\n",
				"            is_training=False,\n",
				"            drop_remainder=predict_drop_remainder)\n",
				"\n",
				"predicted_result = estimator.evaluate(input_fn=predict_input_fn)\n",
				"output_eval_file = os.path.join(output_dir, \"predicted_results.txt\")\n",
				"with codecs.open(output_eval_file, \"w\", encoding='utf-8') as writer:\n",
				"    tf.logging.info(\"***** Predict results *****\")\n",
				"    for key in sorted(predicted_result.keys()):\n",
				"        tf.logging.info(\"  %s = %s\", key, str(predicted_result[key]))\n",
				"        writer.write(\"%s = %s\\n\" % (key, str(predicted_result[key])))\n",
				"\n",
				"result = estimator.predict(input_fn=predict_input_fn)\n",
				"output_predict_file = os.path.join(output_dir, \"label_test.txt\")\n",
				"\n",
				"def result_to_pair(writer):\n",
				"    for predict_line, prediction in zip(predict_examples, result):\n",
				"        idx = 0\n",
				"        line = ''\n",
				"        line_token = str(predict_line.text).split(' ')\n",
				"        label_token = str(predict_line.label).split(' ')\n",
				"        if len(line_token) != len(label_token):\n",
				"            tf.logging.info(predict_line.text)\n",
				"            tf.logging.info(predict_line.label)\n",
				"        for id in prediction:\n",
				"            if id == 0:\n",
				"                continue\n",
				"            curr_labels = id2label[id]\n",
				"            if curr_labels in ['[CLS]', '[SEP]']:\n",
				"                continue\n",
				"            try:\n",
				"                line += line_token[idx] + ' ' + label_token[idx] + ' ' + curr_labels + '\\n'\n",
				"            except Exception as e:\n",
				"                tf.logging.info(e)\n",
				"                tf.logging.info(predict_line.text)\n",
				"                tf.logging.info(predict_line.label)\n",
				"                line = ''\n",
				"                break\n",
				"            idx += 1\n",
				"        writer.write(line + '\\n')\n",
				"            \n",
				"from NER.src.conlleval import return_report\n",
				"\n",
				"with codecs.open(output_predict_file, 'w', encoding='utf-8') as writer:\n",
				"    result_to_pair(writer)\n",
				"eval_result = return_report(output_predict_file)\n",
				"for line in eval_result:\n",
				"    print(line)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 在线命名实体识别\n",
				"\n",
				"由以上训练得到模型进行在线测试，可以任意输入句子，进行命名实体识别。\n",
				"\n",
				"### <span style=\"color:red\">若下述程序未执行成功，则表示训练完成后，GPU显存还在占用，需要restart kernel，然后执行下面的%run命令。</span>\n",
				"\n",
				"释放资源具体流程为：菜单 > Kernel > Restart  \n",
				"\n",
				"![释放资源](./img/释放资源.png)\n",
				"\n",
				"以保证后续步骤资源充足\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"checkpoint path:./NER/output/checkpoint\n",
					"going to restore checkpoint\n",
					"INFO:tensorflow:Restoring parameters from ./NER/output/model.ckpt-978\n",
					"{1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'X', 9: '[CLS]', 10: '[SEP]'}\n",
					"输入句子:\n",
					"“双打最佳搭档”许昕将与陈梦联手出战混双比赛。即将于5月28日至6月2日在深圳进行的国际乒联世界巡回赛中国公开赛上，他们将作为6号种子与头号种子中国香港组合黄镇廷/杜凯琹、2号种子斯洛伐克组合皮斯特耶/巴拉佐娃、8号种子日本组合张本智和/石川佳纯等展开对决。\n",
					"[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-ORG', 'B-LOC', 'I-LOC', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-ORG', 'I-LOC', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O']]\n",
					"LOC, 深圳, 中国, 香港, 斯洛伐克, 日本\n",
					"PER, 许昕, 陈梦, 黄镇廷, 杜凯[UNK], 皮斯特耶, 巴拉佐娃, 张本智和, 石川佳纯\n",
					"ORG, 国际乒联, 中国\n",
					"time used: 0.927599 sec\n",
					"输入句子:\n",
					"1976年5月17日，王力宏出生于美国纽约罗切斯特。奶奶许留芬是清华大学经济系的高材生，舅公许倬云是芝加哥大学博士。\n",
					"[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O']]\n",
					"LOC, 美国, 纽约, 罗切斯特\n",
					"PER, 王力宏, 许留芬, 许倬云\n",
					"ORG, 清华大学经济系, 芝加哥大学\n",
					"time used: 0.060211 sec\n",
					"输入句子:\n",
					"由清华大学主办的“2019清华五道口全球金融论坛”于5月25日至26日举行，本次论坛主题为“金融供给侧改革与开放”。中国银行保险监督管理委员会主席郭树清、中国人民银行副行长陈雨露、中国证券监督管理委员会副主席阎庆民等重量级嘉宾出席论坛。\n",
					"[['O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-ORG', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
					"LOC, 五道口\n",
					"PER, 郭树清, 陈雨露, 阎庆民\n",
					"ORG, 清华大学, 清华, 中国银行保险监督管理委员会, 中国人民银行, 中国证券监督管理委员会\n",
					"time used: 0.11861 sec\n",
					"输入句子:\n"
				]
			}],
			"source": [
				"%run NER/src/terminal_predict.py"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### <span style=\"color:red\">注意，现阶段ModelArts创建环境5G以下的EVS配置免费，超过请在实践项目结束后停止并删除该开发环境，以免产生多余费用。</span>\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.6.4"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
