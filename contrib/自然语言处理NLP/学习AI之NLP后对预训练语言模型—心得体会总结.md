一、学习NLP背景介绍：

   从2019年4月份开始跟着华为云ModelArts实战营同学们一起进行了6期关于图像深度学习的学习，初步了解了关于图像标注、图像分类、物体检测，图像都目标物体检测等，基本了解了卷积神经网络(CNN)原理及相关常用模型，如：VGG16、MaxNet等。之后从9月份开始在华为云AI专家的带领指引下，对AI深度学习的另外一个重要领域：自然语言处理（NLP）的学习，到目前为止学习了：命名实体识别、文本分类、文本相似度分析、问答系统、人脸检测。在这一个多月对NLP的处理流程，常用模型及原理进行了初步了解及理解，到目前还只是部分理解，不能全部吃透，感觉比前期图像领域的深度学习理论知识复杂及难理解很多，主要是体现在图像领域有很多图像架构暂时原理及公司推导；而NLP这方面比较少，为了在这1个多月对NLP的理解及后期帮助复习巩固，以下对NLP领域的相关只是根据自己的理解及网上相关知识做整理和归纳。

二、自然语言处理NLP数学基础：
    1、基础概率简介：
        概率论是统计自然语言处理的理论基础，有些知识我们在中学或者是大学已经学过了，在这里就不在做过多的赘述。比如，概率空间( Probability spaces )、条件概率和独立性(Conditional probability and independence)、随机变量、期望和方差。
        在这里我只是介绍一下条件概率，因为这个概率模型以后会很多次提到，并且用途也很广。比如最大熵问题，HMM模型，语言模型中求字符串的概率。用的比较多的是多参数的链式法则。 在机器翻译中这个模型是语言模型。
        例如，S表示特定排列的词串w1,w2,w3,w4,w5...wn。如果要求出S在文中出现的概率，即用P(S)表示S的概率，利用条件概率这个S的概率等于S中各个词的概率乘积，于是P(S)可以展开为：P(S)=P(w1*w2*w3...wn)=P(w1)P(w2|w1)P(w3|w1w2)...P(wn|w1w2w3...wn-1)，其中P(w1)表示词w1在文中出现的频率，P(w2|w1)表示已知第一个词的前提下，第二个词出现的频率，以后的一次类推。
        在实际应用中，如果词串太长后面的计算量太大，用 N 元语法模型或者是 马尔可夫假设 ，即任意一个词 wi 出现的概率只有前一个 wi-1 词决定 , 这样前面的公式就变得简单多了，P(S)=P(w1)P(w2|w1)P(w3|w2)...P(wn|wn-1)  ，这里用的是二元语法模型，也可以用三元，四元模型，不过随着数量的增加计算的复杂度也相应的增加。接下来的问题是，如果计算 P(wi|wi-1) ， 只要数一数这对词（ wi-1,wi)  在统计的文本中出现了多少次，以及  wi-1  本身在同样的文本中前后相邻出现了多少次 ，然后用两个数一除就可以了 ,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1) 。
    2、贝叶斯定理 (Bayes’theorem)：
        简单地说 Bayes 是个逆概率问题， 比如，我们要可以把求P(A/B) ，的问题转换为求 P(B/A) 的问题。 当正向概率很复杂时，这个公式就显示出他的作用，以后我们会经常用到的。他由条件概率的定义引出。
        Bayes定理：P(B|A)=P(AB)/P(A) = P(A|B)P(B)/P(A)
        这样在给定A的条件下，求P(B|A)，我们可以忽略A的只，因为他在所有的情况下都是常数。所以，argmax(P(A|B)P(B)/P(A)) = argmax(P(A|B)P(B))，这个公式在机器翻译中用的很广泛，比如用在 翻译模型 。
    3、 Bayes 贝叶斯网络：
        我们在以前提到马尔科夫链(MarkovChain),它描述了这样的一个 状态 序列，每个状态值取决于其前面的有限的状态。 这种模型，对很多实际问题来讲是一种很粗略的简化。在现实生活中，很多事物相互的关系并不能用一条链来串起来。它们之间的关系可能是交叉的、错综复杂的。 不能用一条链来描述。 如下图：
            0_1288611898NT2I.gif
        我们可以把上述的 有向图看成一个网络， 它就是贝叶斯网络。其中每个圆圈表示一个状态。状态之间的连线表示它们的因果关系。比如从心血管疾病出发到吸烟的弧线表示心血管疾病可能和吸烟有关。当然，这些关系可以有一个量化的可信度(belief)，用一个概率描述。我们可以通过这样一张网络估计出一个人的心血管疾病的可能性。在网络中每个节点概率的计算，可以用贝叶斯公式来进行，贝叶斯网络因此而得名。由于网络的每个弧有一个可信度，贝叶斯网络也被称作信念网络(belief networks)。和马尔可夫链类似，贝叶斯网络中的每个状态值取决于前面有限个状态。不同的是，贝叶斯网络比马尔可夫链灵活，它不受马尔可夫链的链状结构的约束，因此可以更准确地描述事件之间的相关性。可以讲，马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。
    4、全概率公式：
         我们在处理NLP中，我们对一个句子的处理实际是获得这个文本序列的联合概率，如：假定 S是一个有意义的句子，它由一串排序确定的词组成，即
S=w1,w2,…,wn。这里每个 wk表示一个词语，n是句子包含词语的数量，即句子的长度。显而易见，把 S 放在从古至今所有的语境里去检查概率是不可行的，因此我们需要做一个模型来估算。我们把 P(S) 展开，得到（2）中的条件概率形式。 
这里写图片描述 
        其中 P(wk∣w1,w2,…,wi),(i=k−1)表示 wk接在序列 w1,w2,…,wi  之后的条件概率。不难看出，wk 的概率，取决于它本身在整个语言中出现的概率，以及它前面 i=k−1 个词顺序出现的概率。
        学过概率论的读者应该知道，计算三元的条件概率（即计算类似 P(w3∣w1,w2)已经很困难了，更别说计算整个句子最后一个词的条件概率。因此，我们需要对模型进行简化。
数学专业的读者可能会发现，这样的条件概率，实际上是一个随机过程的概率。在随机过程中，为了简化分析，对此类过程有一个马尔科夫假设（Markov Hypothesis），即假设 wk 
wk的概率只和 wi,(i=k−1)有关。即假设 
这里写图片描述 
这样一来，我们就有 
这里写图片描述

以上P(S)即为全概率公式


三、自然语言处理NLP步骤：
    1667471-37315f7baaee75f4.jpg
四、自然语言处理NLP主要研究方向：
    4.1、信息抽取（命名实体识别）：从给定文本中抽取重要的信息，比如时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。
    4.2、文本生成：机器像人一样使用自然语言进行表达和写作。依据输入的不同，文本生成技术主要包括数据到文本生成和文本到文本生成。数据到文本生成是指将包含键值对的数据转化为自然语言文本；文本到文本生成对输入文本进行转化和处理从而产生新的文本。
    4.3、问答系统：对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。
    4.4、对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。
    4.5、文本挖掘：包括文本聚类、分类、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。
    4.6、语音识别和生成：语音识别是将输入计算机的语音符号识别转换成书面语表示。语音生成又称文语转换、语音合成，它是指将书面文本自动转换成对应的语音表征。
    4.7、信息过滤：通过计算机系统自动识别和过滤符合特定条件的文档信息。通常指网络有害信息的自动识别和过滤，主要用于信息安全和防护，网络内容管理等。
    4.8、舆情分析：是指收集和处理海量信息，自动化地对网络舆情进行分析，以实现及时应对网络舆情的目的。
    4.9、信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。
    4.10、机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。
五、自然语言处理NLP语言模型预训练方法（ELMo、GPT和BERT）：
    5.1、 引言
    在介绍论文之前，我将先简单介绍一些相关背景知识。首先是语言模型（Language Model），语言模型简单来说就是一串词序列的概率分布。具体来说，语言模型的作用是为一个长度为m的文本确定一个概率分布P，表示这段文本存在的可能性。在实践中，如果文本的长度较长，P(wi | w1, w2, . . . , wi−1)的估算会非常困难。因此，研究者们提出使用一个简化模型：n元模型（n-gram model）。在 n 元模型中估算条件概率时，只需要对当前词的前n个词进行计算。在n元模型中，传统的方法一般采用频率计数的比例来估算n元条件概率。当n较大时，机会存在数据稀疏问题，导致估算结果不准确。因此，一般在百万词级别的语料中，一般也就用到三元模型。

   为了缓解n元模型估算概率时遇到的数据稀疏问题，研究者们提出了神经网络语言模型。代表性工作是Bengio等人在2003年提出的神经网络语言模型，该语言模型使用了一个三层前馈神经网络来进行建模。其中有趣的发现了第一层参数，用做词表示不仅低维紧密，而且能够蕴涵语义，也就为现在大家都用的词向量（例如word2vec）打下了基础。其实，语言模型就是根据上下文去预测下一个词是什么，这不需要人工标注语料，所以语言模型能够从无限制的大规模单语语料中，学习到丰富的语义知识。
   接下来在简单介绍一下预训练的思想。我们知道目前神经网络在进行训练的时候基本都是基于后向传播（BP）算法，通过对网络模型参数进行随机初始化，然后通过BP算法利用例如SGD这样的优化算法去优化模型参数。那么预训练的思想就是，该模型的参数不再是随机初始化，而是先有一个任务进行训练得到一套模型参数，然后用这套参数对模型进行初始化，再进行训练。其实早期的使用自编码器栈式搭建深度神经网络就是这个思想。还有词向量也可以看成是第一层word embedding进行了预训练，此外在基于神经网络的迁移学习中也大量用到了这个思想。

接下来，我们就具体看一下这几篇用语言模型进行预训练的工作。
5.2、 ELMo
5.2.1 引言
《Deep Contextualized Word Representations》这篇论文来自华盛顿大学的工作，最后是发表在今年的NAACL会议上，并获得了最佳论文。其实这个工作的前身来自同一团队在ACL2017发表的《Semi-supervised sequence tagging with bidirectional language models》 [4]，只是在这篇论文里，他们把模型更加通用化了。首先我们来看看他们工作的动机，他们认为一个预训练的词表示应该能够包含丰富的句法和语义信息，并且能够对多义词进行建模。而传统的词向量（例如word2vec）是上下文无关的。例如下面"apple"的例子，这两个"apple"根据上下文意思是不同的，但是在word2vec中，只有apple一个词向量，无法对一词多义进行建模。



所以他们利用语言模型来获得一个上下文相关的预训练表示，称为ELMo，并在6个NLP任务上获得了提升。



5.2.2 方法
在EMLo中，他们使用的是一个双向的LSTM语言模型，由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然。



在预训练好这个语言模型之后，ELMo就是根据下面的公式来用作词表示，其实就是把这个双向语言模型的每一中间层进行一个求和。最简单的也可以使用最高层的表示来作为ELMo。



然后在进行有监督的NLP任务时，可以将ELMo直接当做特征拼接到具体任务模型的词向量输入或者是模型的最高层表示上。总结一下，不像传统的词向量，每一个词只对应一个词向量，ELMo利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（对于不同上下文的同一个词的表示是不一样的），再当成特征加入到具体的NLP有监督模型里。

5.2.3 实验
这里我们简单看一下主要的实验，具体实验还需阅读论文。首先是整个模型效果的实验。他们在6个NLP任务上进行了实验，首先根据目前每个任务搭建了不同的模型作为baseline，然后加入ELMo，可以看到加入ELMo后6个任务都有所提升，平均大约能够提升2个多百分点，并且最后的结果都超过了之前的先进结果（SOTA）。



在下面的分析实验中，我们可以看到使用所有层的效果要比只使用最后一层作为ELMo的效果要好。在输入还是输出上面加EMLo效果好的问题上，并没有定论，不同的任务可能效果不一样。



5.3、 Open AI GPT
5.3.1 引言
我们来看看第二篇论文《Improving Language Understanding by Generative Pre-Training》，这是OpenAI 团队前一段时间放出来的预印版论文。他们的目标是学习一个通用的表示，能够在大量任务上进行应用。这篇论文的亮点主要在于，他们利用了Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构。然后在进行具体任务有监督微调时使用了语言模型作为附属任务训练目标。最后再12个NLP任务上进行了实验，9个任务获得了SOTA。



5.3.2 方法
首先我们来看一下他们无监督预训练时的语言模型。他们仍然使用的是标准的语言模型目标函数，即通过前k个词预测当前词，但是在语言模型网络上他们使用了google团队在《Attention is all your need》论文中提出的Transformer解码器作为语言模型。Transformer模型主要是利用自注意力（self-attention）机制的模型，这里我就不多进行介绍，大家可以看论文或者参考我之前的博客（https://www.cnblogs.com/robert-dlut/p/8638283.html）。



然后再具体NLP任务有监督微调时，与ELMo当成特征的做法不同，OpenAI GPT不需要再重新对任务构建新的模型结构，而是直接在transformer这个语言模型上的最后一层接上softmax作为任务输出层，然后再对这整个模型进行微调。他们额外发现，如果使用语言模型作为辅助任务，能够提升有监督模型的泛化能力，并且能够加速收敛。



由于不同NLP任务的输入有所不同，在transformer模型的输入上针对不同NLP任务也有所不同。具体如下图，对于分类任务直接讲文本输入即可；对于文本蕴涵任务，需要将前提和假设用一个Delim分割向量拼接后进行输入；对于文本相似度任务，在两个方向上都使用Delim拼接后，进行输入；对于像问答多选择的任务，就是将每个答案和上下文进行拼接进行输入。



5.3.3 实验
下面我简单的列举了一下不同NLP任务上的实验结果。

语言推理任务：



问答和常识推理任务：



语义相似度和分类任务：



可以看到在多项任务上，OpenAI GPT的效果要比ELMo的效果更好。从下面的消除实验来看，在去掉预训练部分后，所有任务都大幅下降，平均下降了14.8%，说明预训练很有效；在大数据集上使用语言模型作为附加任务的效果更好，小数据集不然；利用LSTM代替Transformer后，结果平均下降了5.6%，也体现了Transformer的性能。



5.4.、BERT
5.4.1引言
上周Google放出了他们的语言模型预训练方法，瞬时受到了各界广泛关注，不少媒体公众号也进行了相应报道，那我们来看看这篇论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》。这篇论文把预训练语言表示方法分为了基于特征的方法（代表ELMo）和基于微调的方法（代表OpenAI GPT）。而目前这两种方法在预训练时都是使用单向的语言模型来学习语言表示。



这篇论文中，作者们证明了使用双向的预训练效果更好。其实这篇论文方法的整体框架和GPT类似，是进一步的发展。具体的，他们BERT是使用Transformer的编码器来作为语言模型，在语言模型预训练的时候，提出了两个新的目标任务（即遮挡语言模型MLM和预测下一个句子的任务），最后在11个NLP任务上取得了SOTA。



5.4.2方法
在语言模型上，BERT使用的是Transformer编码器，并且设计了一个小一点Base结构和一个更大的Large网络结构。



对比一下三种语言模型结构，BERT使用的是Transformer编码器，由于self-attention机制，所以模型上下层直接全部互相连接的。而OpenAI GPT使用的是Transformer解码器，它是一个需要从左到右的受限制的Transformer，而ELMo使用的是双向LSTM，虽然是双向的，但是也只是在两个单向的LSTM的最高层进行简单的拼接。所以作者们任务只有BERT是真正在模型所有层中是双向的。



而在模型的输入方面，BERT做了更多的细节，如下图。他们使用了WordPiece embedding作为词向量，并加入了位置向量和句子切分向量。并在每一个文本输入前加入了一个CLS向量，后面会有这个向量作为具体的分类向量。



在语言模型预训练上，他们不在使用标准的从左到右预测下一个词作为目标任务，而是提出了两个新的任务。第一个任务他们称为MLM，即在输入的词序列中，随机的挡上15%的词，然后任务就是去预测挡上的这些词，可以看到相比传统的语言模型预测目标函数，MLM可以从任何方向去预测这些挡上的词，而不仅仅是单向的。但是这样做会带来两个缺点：1）预训练用[MASK]提出挡住的词后，在微调阶段是没有[MASK]这个词的，所以会出现不匹配；2）预测15%的词而不是预测整个句子，使得预训练的收敛更慢。但是对于第二点，作者们觉得虽然是慢了，但是效果提升比较明显可以弥补。



对于第一点他们采用了下面的技巧来缓解，即不是总是用[MASK]去替换挡住的词，在10%的时间用一个随机词取替换，10%的时间就用这个词本身。



而对于传统语言模型，并没有对句子之间的关系进行考虑。为了让模型能够学习到句子之间的关系，作者们提出了第二个目标任务就是预测下一个句子。其实就是一个二元分类问题，50%的时间，输入一个句子和下一个句子的拼接，分类标签是正例，而另50%是输入一个句子和非下一个随机句子的拼接，标签为负例。最后整个预训练的目标函数就是这两个任务的取和求似然。



在微调阶段，不同任务的模型如下图，只是在输入层和输出层有所区别，然后整个模型所有参数进行微调。



5.4.3 实验
下面我们列出一下不同NLP上BERT的效果。

GLUE结果：



QA结果：



实体识别结果：



SWAG结果：



可以看到在这些所有NLP任务上，BERT都取得了SOTA，而且相比EMLo和GPT的效果提升还是比较大的。

在预训练实验分析上，可以看到本文提出的两个目标任务的作用还是很有效的，特别是在MLM这个目标任务上。



作者也做了模型规模的实验，大规模的模型效果更好，即使在小数据集上。



此外，作者也做了像ELMo当成特征加入的实验，从下图可以看到，当成特征加入最好效果能达到96.1%和微调的96.4%差不多，说明BERT对于基于特征和基于微调这两种方法都是有效的。



 

5.5. 总结
   最后进行简单的总结，和传统的词向量相比，使用语言模型预训练其实可以看成是一个句子级别的上下文的词表示，它可以充分利用大规模的单语语料，并且可以对一词多义进行建模。而且从后面两篇论文可以看到，通过大规模语料预训练后，使用统一的模型或者是当成特征直接加到一些简单模型上，对各种NLP任务都能取得不错的效果，说明很大程度上缓解了具体任务对模型结构的依赖。在目前很多评测上也都取得了SOTA。ELMo也提供了官网供大家使用。但是这些方法在空间和时间复杂度上都比较高，特别是BERT，在论文中他们训练base版本需要在16个TGPU上，large版本需要在64个TPU上训练4天，对于一般条件，一个GPU训练的话，得用上1年。还有就是可以看出这些方法里面都存在很多工程细节，一些细节做得不好的话，效果也会大大折扣。







六、预训练语言模型-BERT原理详解：

        在华为云ModelArts实战营中对NLP的学习案例中均采用了Goole公司的BERT双向语言预训练模型，并且在：命名实体识别、文本分类、文本相似度分析、问答系统、人脸检测等案例中，模型均有突出的表现，结合第五大点中对自然语言预训练模型ElMo、GPT、BERT中性能比较，得出BERT模型在多方面均表现了突出的能力，因此本节重点针对BERT模型进行全访问的剖析，其它模型，感兴趣的可以自己去阅读。以下为对Bert模型几个方面做阐述：

    6.1、Bert模型的主要特点：

               第一：是这个模型非常的深，12层，并不宽(wide），中间层只有1024，而之前的Transformer模型中间层有2048。这似乎又印证了计算机图像处理的一个观点——深而窄 比 浅而宽 的模型更

               第二：MLM（Masked Language Model），同时利用左侧和右侧的词语，这个在ELMo上已经出现了，绝对不是原创。其次，对于Mask（遮挡）在语言模型上的应用，已经被Ziang Xie提出了（我很有幸的也参与到了这篇论文中）：[1703.02573] Data Noising as Smoothing in Neural Network Language Models。这也是篇巨星云集的论文：Sida Wang，Jiwei Li（香侬科技的创始人兼CEO兼史上发文最多的NLP学者），Andrew Ng，Dan Jurafsky都是Coauthor。但很可惜的是他们没有关注到这篇论文。用这篇论文的方法去做Masking，相信BRET的能力说不定还会有提升。

    6.2、BERT 的五个关键词 Pre-training、Deep、Bidirectional、Transformer、Language Understanding ：           

        《A Neural Probabilistic Language Model》这篇论文讲的 Language Model，严格讲是语言生成模型（Language Generative Model），预测语句中下一个将会出现的词汇。语言生成模型能不能直接移用到其它 NLP 问题上去？

         譬如，淘宝上有很多用户评论，能否把每一条用户转换成评分？-2、-1、0、1、2，其中 -2 是极差，+2 是极好。假如有这样一条用户评语，“买了一件鹿晗同款衬衫，没想到，穿在自己身上，不像小鲜肉，倒像是厨师”，请问这条评语，等同于 -2，还是其它？

         语言生成模型，能不能很好地解决上述问题？进一步问，有没有 “通用的” 语言模型，能够理解语言的语义，适用于各种 NLP 问题？BERT 这篇论文的题目很直白，《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》，一眼看去，就能猜得到这篇文章会讲哪些内容。

         这个题目有五个关键词，分别是 Pre-training、Deep、Bidirectional、Transformers、和 Language Understanding。其中 pre-training 的意思是，作者认为，确实存在通用的语言模型，先用文章预训练通用模型，然后再根据具体应用，用 supervised 训练数据，精加工（fine tuning）模型，使之适用于具体应用。为了区别于针对语言生成的 Language Model，作者给通用的语言模型，取了一个名字，叫语言表征模型 Language Representation Model。

webp

       能实现语言表征目标的模型，可能会有很多种，具体用哪一种呢？作者提议，用 Deep Bidirectional Transformers 模型。假如给一个句子 “能实现语言表征[mask]的模型”，遮盖住其中“目标”一词。从前往后预测[mask]，也就是用“能/实现/语言/表征”，来预测[mask]；或者，从后往前预测[mask]，也就是用“模型/的”，来预测[mask]，称之为单向预测 unidirectional。单向预测，不能完整地理解整个语句的语义。于是研究者们尝试双向预测。把从前往后，与从后往前的两个预测，拼接在一起 [mask1/mask2]，这就是双向预测 bi-directional。细节参阅《Neural Machine Translation by Jointly Learning to Align and Translate》。

      BERT 的作者认为，bi-directional 仍然不能完整地理解整个语句的语义，更好的办法是用上下文全向来预测[mask]，也就是用 “能/实现/语言/表征/../的/模型”，来预测[mask]。BERT 作者把上下文全向的预测方法，称之为 deep bi-directional。如何来实现上下文全向预测呢？BERT 的作者建议使用 Transformer 模型。这个模型由《Attention Is All You Need》一文发明。

      这个模型的核心是聚焦机制，对于一个语句，可以同时启用多个聚焦点，而不必局限于从前往后的，或者从后往前的，序列串行处理。不仅要正确地选择模型的结构，而且还要正确地训练模型的参数，这样才能保障模型能够准确地理解语句的语义。BERT 用了两个步骤，试图去正确地训练模型的参数。第一个步骤是把一篇文章中，15% 的词汇遮盖，让模型根据上下文全向地预测被遮盖的词。假如有 1 万篇文章，每篇文章平均有 100 个词汇，随机遮盖 15% 的词汇，模型的任务是正确地预测这 15 万个被遮盖的词汇。通过全向预测被遮盖住的词汇，来初步训练 Transformer 模型的参数。

      然后，用第二个步骤继续训练模型的参数。譬如从上述 1 万篇文章中，挑选 20 万对语句，总共 40 万条语句。挑选语句对的时候，其中 210 万对语句，是连续的两条上下文语句，另外 210 万对语句，不是连续的语句。然后让 Transformer 模型来识别这 20 万对语句，哪些是连续的，哪些不连续。

     这两步训练合在一起，称为预训练 pre-training。训练结束后的 Transformer 模型，包括它的参数，是作者期待的通用的语言表征模型。

6.3、Bert模型架构：       

    BERT的模型架构是基于Vaswani et al. (2017) 中描述的原始实现multi-layer bidirectional Transformer编码器，并在tensor2tensor库中发布。由于Transformer的使用最近变得无处不在，论文中的实现与原始实现完全相同，因此这里将省略对模型结构的详细描述。

在这项工作中，论文将层数（即Transformer blocks）表示为L，将隐藏大小表示为H，将self-attention heads的数量表示为A。在所有情况下，将feed-forward/filter 的大小设置为 4H，即H = 768时为3072，H = 1024时为4096。论文主要报告了两种模型大小的结果：

webp

为了进行比较，论文选择了BERT LARGE ，它与OpenAI GPT具有相同的模型大小。然而，重要的是，BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。研究团队注意到，在文献中，双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，因为它可以用于文本生成。BERT，OpenAI GPT和ELMo之间的比较如图1所示。

图1：预训练模型架构的差异。BERT使用双向Transformer。OpenAI GPT使用从左到右的Transformer。ELMo使用经过独立训练的从左到右和从右到左LSTM的串联来生成下游任务的特征。三个模型中，只有BERT表示在所有层中  共同依赖于左右上下文。

输入表示（input representation）

论文的输入表示（input representation）能够在一个token序列中明确地表示单个文本句子或一对文本句子（例如， [Question, Answer]）。对于给定token，其输入表示通过对相应的token、segment和position embeddings进行求和来构造。图2是输入表示的直观表示：

图2：BERT输入表示。输入嵌入是token embeddings, segmentation embeddings 和position embeddings 的总和。

webp

具体如下：

（1）使用WordPiece嵌入（Wu et al., 2016）和30,000个token的词汇表。用##表示分词。
（2）使用学习的positional embeddings，支持的序列长度最多为512个token。
每个序列的第一个token始终是特殊分类嵌入（[CLS]）。对应于该token的最终隐藏状态（即，Transformer的输出）被用作分类任务的聚合序列表示。对于非分类任务，将忽略此向量。
（3）句子对被打包成一个序列。以两种方式区分句子。首先，用特殊标记（[SEP]）将它们分开。其次，添加一个learned sentence A嵌入到第一个句子的每个token中，一个sentence B嵌入到第二个句子的每个token中。
（4）对于单个句子输入，只使用 sentence A嵌入。

    6.4、预训练任务（关键创新）：

        与Peters et al. (2018) 和 Radford et al. (2018)不同，论文不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，使用两个新的无监督预测任务对BERT进行预训练。

任务1: Masked LM

        从直觉上看，研究团队有理由相信，深度双向模型比left-to-right 模型或left-to-right and right-to-left模型的浅层连接更强大。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件作用将允许每个单词在多层上下文中间接地“see itself”。

        为了训练一个深度双向表示（deep bidirectional representation），研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)，尽管在文献中它经常被称为Cloze任务(Taylor, 1953)。

        在这个例子中，与masked token对应的最终隐藏向量被输入到词汇表上的输出softmax中，就像在标准LM中一样。在团队所有实验中，随机地屏蔽了每个序列中15%的WordPiece token。与去噪的自动编码器（Vincent et al.， 2008）相反，只预测masked words而不是重建整个输入。

        虽然这确实能让团队获得双向预训练模型，但这种方法有两个缺点。首先，预训练和finetuning之间不匹配，因为在finetuning期间从未看到[MASK]token。为了解决这个问题，团队并不总是用实际的[MASK]token替换被“masked”的词汇。相反，训练数据生成器随机选择15％的token。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：

数据生成器将执行以下操作，而不是始终用[MASK]替换所选单词：

80％的时间：用[MASK]标记替换单词，例如，my dog is hairy → my dog is [MASK]
10％的时间：用一个随机的单词替换该单词，例如，my dog is hairy → my dog is apple
10％的时间：保持单词不变，例如，my dog is hairy → my dog is hairy. 这样做的目的是将表示偏向于实际观察到的单词。

 Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示。此外，因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。

使用MLM的第二个缺点是每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。

任务2：下一句预测

许多重要的下游任务，如问答（QA）和自然语言推理（NLI）都是基于理解两个句子之间的关系，这并没有通过语言建模直接获得。

在为了训练一个理解句子的模型关系，预先训练一个二进制化的下一句测任务，这一任务可以从任何单语语料库中生成。具体地说，当选择句子A和B作为预训练样本时，B有50％的可能是A的下一个句子，也有50％的可能是来自语料库的随机句子。例如：

Input = [CLS] the man went to [MASK] store [SEP]

he bought a gallon [MASK] milk [SEP]

Label = IsNext

Input = [CLS] the man [MASK] to the store [SEP]

penguin [MASK] are flight ##less birds [SEP]

Label = NotNext

团队完全随机地选择了NotNext语句，最终的预训练模型在此任务上实现了97％-98％的准确率

    6.5、BERT模型的影响：        

        BERT是一个语言表征模型（language representation model），通过超大数据、巨大模型、和极大的计算开销训练而成，在11个自然语言处理的任务中取得了最优（state-of-the-art, SOTA）结果。或许你已经猜到了此模型出自何方，没错，它产自谷歌。估计不少人会调侃这种规模的实验已经基本让一般的实验室和研究员望尘莫及了，但它确实给我们提供了很多宝贵的经验：

    6.5.1、深度学习就是表征学习 （Deep learning is representation learning）

        "We show that pre-trained representations eliminate the needs of many heavily engineered task-specific architectures". 在11项BERT刷出新境界的任务中，大多只在预训练表征（pre-trained representation）微调（fine-tuning）的基础上加一个线性层作为输出（linear output layer）。在序列标注的任务里（e.g. NER），甚至连序列输出的依赖关系都先不管（i.e. non-autoregressive and no CRF），照样秒杀之前的SOTA，可见其表征学习能力之强大。

    6.5.2、规模很重要（Scale matters）

        "One of our core claims is that the deep bidirectionality of BERT, which is enabled by masked LM pre-training, is the single most important improvement of BERT compared to previous work". 这种遮挡（mask）在语言模型上的应用对很多人来说已经不新鲜了，但确是BERT的作者在如此超大规模的数据+模型+算力的基础上验证了其强大的表征学习能力。这样的模型，甚至可以延伸到很多其他的模型，可能之前都被不同的实验室提出和试验过，只是由于规模的局限没能充分挖掘这些模型的潜力，而遗憾地让它们被淹没在了滚滚的paper洪流之中。

    6.5.3、预训练价值很大（Pre-training is important）

        "We believe that this is the first work to demonstrate that scaling to extreme model sizes also leads to large improvements on very small-scale tasks, provided that the model has been sufficiently pre-trained". 预训练已经被广泛应用在各个领域了（e.g. ImageNet for CV, Word2Vec in NLP），多是通过大模型大数据，这样的大模型给小规模任务能带来的提升有几何，作者也给出了自己的答案。BERT模型的预训练是用Transformer做的，但我想换做LSTM或者GRU的话应该不会有太大性能上的差别，当然训练计算时的并行能力就另当别论了。

    6.6、BERT模型的观点：

        6.6.1、high-performance的原因其实还是归结于两点，除了模型的改进，更重要的是用了超大的数据集（BooksCorpus 800M + English Wikipedia 2.5G单词）和超大的算力（对应于超大模型）在相关的任务上做预训练，实现了在目标任务上表现的单调增长。

        6.6.2、这个模型的双向和Elmo不一样，大部分人对他这个双向在novelty上的contribution 的大小有误解，我觉得这个细节可能是他比Elmo显著提升的原因。Elmo是拼一个左到右和一个右到左，他这个是训练中直接开一个窗口，用了个有顺序的cbow。

        6.6.3、可复现性差：有钱才能为所欲为（Reddit对跑一次BERT的价格讨论）

For TPU pods:
 
4 TPUs * ~$2/h (preemptible) * 24 h/day * 4 days = $768 (base model)
 
16 TPUs = ~$3k (large model)
 
 
 
For TPU:
 
16 tpus * $8/hr * 24 h/day * 4 days = 12k
 
64 tpus * $8/hr * 24 h/day * 4 days = 50k
七、BERT模型源代码细节记录：

        7.1、预训练输入参数说明（摘自...）：

        输入包含七个部分,分别为:input_ids,input_mask,segment_ids,masked_lm_positions,mask_lm_ids,masked_lm_weights,next_sentence_labels.

            input_ids:表示tokens的ids

            input_mask:表示哪些是input,哪些是padding.len(input_ids)个1,后面继续补0.对于mask的词,主要占了全部vocabulary的15%左右,在代码中对于每个词80%replace with [mask],10% keep original,10% replace with random word.超过了mask的词数,则终止.

            segment_ids:第一个句子到[SEP]为0,后面为1.主要是对输入进行区分,判断输入的两个句子.

            masked_lm_positions:表示句子中mask的token的position.

            mask_lm_ids:表示句子中mask的token的id.

            masked_lm_weights:表示句子中mask的token的权重.

            next_sentence_labels:表示两个句子是不是相连的.

        对于每个句子,sentence->embedding->postprocess embedding: 将输入[batch_size, seq_len]->[batch_size, seq_len, embedding_size]

       7.2、bert模型详解 源码解析

        bert模型源代码详细讲解，查看代码详细解析，请点击...,此处有非常详细的对Bert模型各模块及参数，代码的详细解释说明，请大家查阅。
