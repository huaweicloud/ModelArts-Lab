
00任务的名称是：训练轮数与callbacks，其中callbacks，即回调函数。
训练轮次好理解就是：
callbacks回调函数怎么理解呢？kears官网是这样解释：
回调函数是一组在训练的特定阶段被调用的函数集，你可以使用回调函数来观察训练过程中网络内部的状态和统计信息。
通过传递回调函数列表到模型的.fit()中，即可在给定的训练阶段调用该函数集中的函数。
可知在接下来的操作中要深入了解如图的各项内容，代码的位置：

![image](https://user-images.githubusercontent.com/46392381/60750598-3195cb00-9fdd-11e9-94aa-aed45bbff466.png)

直白的讲，就是设置模型的训练次数、等参数的。

接着任务介绍里面提到本次任务我们还会进行
（ModelCheckpoint），（EarlyStopping），（ReduceLROnPlateau）
这三项的相关实践。简单百度了下，对实践内容可以更好的理解。

1.ModelCheckpoint翻译过来是模型检查点，模型好理解，检查点是什么，为什么要有检查点？
Keras文档为检查点提供了一个很好的解释:
模型的体系结构，允许你重新创建模型、模型的权重
训练配置(损失、优化器、epochs和其他元信息)
优化器的状态，允许在你离开的地方恢复训练
同样，一个检查点包含了保存当前实验状态所需的信息，以便你可以从这一点恢复训练。
代码中位置：

![image](https://user-images.githubusercontent.com/46392381/60750809-86871080-9fe0-11e9-92de-c61c4345956b.png)

直白的讲，一种保存你实验状态的方法，这样你就可以从你离开的地方开始继续学习。

2.EarlyStopping翻译过来早停，一百度就是早停法。顾名思义就是让你提前停止训练，那为啥要提前呢？查询相关资料了解到。
在进行模型的训练的时候，设置的epoch太少的时候，网络会发生欠拟合也就是学习不充分。
所以，我们往往会把epoch设置很大直到发生过拟合，这时候通过刚刚说的ModelCheckpoint来获得所有的训练结果来分析，
找到即将发生过拟合的点，然后操作EarlyStopping来早停，从而获得最佳模型参数。
如图：可以看到12~13次后就要开始过拟合

![QQ截图20190704225110](https://user-images.githubusercontent.com/46392381/60750974-02358d00-9fe2-11e9-8912-7d10d3dc7256.png)

准确值断崖式下降，这时候我们就需要使用EarlyStopping来解决了。
直白的说，当监测数据不再变好，我们就停止训练。相关代码如下：

![image](https://user-images.githubusercontent.com/46392381/60751115-2ba2e880-9fe3-11e9-9825-31ad606c0893.png)

3.ReduceLROnPlateau
这个称为学习率衰减，为什么要设置这个参数呢？找了些相关资料进行了解：
a.为了防止学习率过大，在收敛到全局最优点的时候会来回摆荡，所以要让学习率随着训练轮数不断按指数级下降，收敛梯度下降的学习步长。
b.网络的评价指标不在提升的时候，可以通过降低网络的学习率来提高网络性能。
c.加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减。
d.就想我们问路一样，别人告诉我们直走五分钟，有的人走的快，有的人走的慢，所以如果走的快的话，当再次问路的时候，就会发现走多了，而折回来，这就是我们训练过程中的loss曲线震荡严重的原因之一. 所以学习率要设置在合理的大小。
(这段来自https://www.cnblogs.com/gongxijun/p/8039262.html，感谢Gxjun的直白说明)
总结下：就是开始阶段学习率可以快，到了后面得降速，因为高速率下来回震荡也大。只能这样感觉的理解啦。
相关代码如下：

![image](https://user-images.githubusercontent.com/46392381/60751406-9144a400-9fe6-11e9-9b9c-2058238cc3dd.png)

————————————————————————————————
ok，大概理解本次任务的理论了，接下再把相关的代码过一遍。


![image](https://user-images.githubusercontent.com/46392381/60752031-97408200-9ff2-11e9-831a-08e65143ad93.png)


![image](https://user-images.githubusercontent.com/46392381/60752049-cf47c500-9ff2-11e9-9cb1-6a5c72ff25ac.png)

为什么要转化为浮点型：
浮点数的运算速度比整数要快。特别是显卡运算都是使用的浮点数，图形计算用到的很多公式，其中间结果都是有小数的，
你如果用整数计算，这些小数都被略掉了，在积累很多步骤以后，最后的结果误差可能会很大。为了提高渲染物体位置精度，
从GPU诞生时刻起就专注于大规模使用浮点计算单元。
x_train /= 255：对数据进行归一化到0-1 因为图像数据最大是255。
数据归一化：把数据变成(0,1)或者(1,1)之间的小数。主要是为了数据处理方便提出来的,把数据映射到0～1范围之内处理,更加便捷快速。
to_categorical：简单来说,to_categorical就是将类别向量转换为二进制。
n_classes：为标签类别总数。

**构建模型**

![image](https://user-images.githubusercontent.com/46392381/60752117-0074c500-9ff4-11e9-9af6-1fc9e4238aa4.png)
这一部分内容比较深，主要了解下如何调用。

![image](https://user-images.githubusercontent.com/46392381/60752140-3ade6200-9ff4-11e9-9800-419b2e7f686e.png)

model.summary():打印出模型概况，可以大概了解调用模型里面的神经网络结构。
**模型训练**

![image](https://user-images.githubusercontent.com/46392381/60752155-7842ef80-9ff4-11e9-8d46-34c12e020332.png)

model.compile(loss='目标函数', optimizer=optimizer, metrics=['accuracy'])

loss:目标函数,也叫损失函数,是网络中的性能函数.
这次的损失函数为binary_crossentropy。
binary_cross_entropy是二分类的交叉熵，实际是多分类softmax_cross_entropy的一种特殊情况，当多分类中，
类别只有两类时，即0或者1，即为二分类，二分类也是一个逻辑回归问题，也可以套用逻辑回归的损失函数。
所以这次是训练猫狗识别，选用了该损失函数来运行。

optimizer：也叫优化器它的作用：一句话来说，用来更新和计算影响模型训练和模型输出的网络参数，
使其逼近或达到最优值，从而最小化(或最大化)损失函数E(x)这种算法使用各参数的梯度值来最小化或最大化损失函数E(x)。
最常用的一阶优化算法是梯度下降。
目前有：SGD、RMSprop（本次使用）、Adagrad、Adadelta、Adam、Adamax、Nadam、TFOptimizer。
相关算法详细（https://blog.csdn.net/bvl10101111/article/details/72616378）

![image](https://user-images.githubusercontent.com/46392381/60752207-6ada3500-9ff5-11e9-9af2-2817d81108c1.png)

metrics：衡量指标，常见指标Accuracy、Precision、Recall针对多分类的计算方法。我们这次使用准确率（accuracy），准确率即分类正确的样本数占总样本数的比例。

---------------------------------------------------------------------------------------------------
最重要的部分来啦

![image](https://user-images.githubusercontent.com/46392381/60752332-b6d9a980-9ff6-11e9-8b05-147af6fc1e5e.png)

我们先把代码过下，然后再一次了解下（ModelCheckpoint, EarlyStopping, ReduceLROnPlateau）这三指标的意义。不过在此之前先了解
运行的数据集分为：训练集(train set)，验证集(validation set)和测试集(test set)。
train_loss是训练集上的损失值,train_acc是训练集准确率。
val 代表 validation
val_loss是验证集上的损失值,val_acc是验证集上的准确率。
test_loss是测试集上的损失值,test_acc是测试集上的准确率。
先来早停
**早停EarlyStopping**
`es = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=5, verbose=1, mode='auto')`
monitor：中文意思监视器。监控你需要的值，如”val_acc“。

min_delta:增大或减小的阈值，只有大于这个部分才算作improvement。这个值的大小取决于monitor，也反映了你的容忍程度。

patience：当early stop被激活（如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。
又表示为能够容忍多少个epoch内都没有improvement。这个设置其实是在抖动和真正的准确率下降之间做tradeoff。

verbose：信息展示模式，当verbose = 0 为不在标准输出流输出日志信息verbose = 1 为输出进度条记录。
例如 0或1(checkpoint的保存信息，类似Epoch 00001: saving model to ...)。

mode：‘auto’，‘min’，‘max’之一，在save_best_only=True时决定性能最佳模型的评判准则。
例如，当监测值为val_acc时，模式应为max，当监测值为val_loss时，模式应为min。在auto模式下，评价准则由被监测值的名字自动推断。

小结：min_delta和patience都和“避免模型停止在抖动过程中”有关系，所以调节的时候需要互相协调。
通常情况下，min_delta降低，那么patience可以适当减少；min_delta增加，那么patience需要适当延长；反之亦然。

**保存最佳模型ModelCheckpoint**

`cp = ModelCheckpoint(filepath="./model/ckp_vgg16_dog_and_cat.h5", monitor="val_acc", verbose=1, save_best_only=True, mode="auto", period=1)`
filepath：字符串，保存模型的路径。
monitor：中文意思监视器。监控你需要的值，如”val_acc“。
verbose：同上。
save_best_only：当设置为True时，监测值有改进时才会保存当前的模型
（ the latest best model according to the quantity monitored will not be overwritten）。
mode：同上。
period：CheckPoint之间的间隔的epoch数。
小结下：运行后发现patience个epoch中看不到模型性能提升，则可以减少学习率。接下来就是学习率了！！！

**学习率衰减ReduceLROnPlateau**
`lr = ReduceLROnPlateau(monitor="val_acc", factor=0.1, patience=3, verbose=1, mode="auto", min_lr=0)`
还是先走一遍代码
monitor：同上。
factor：学习率每次降低多少，new_lr = old_lr * factor。
patience：容忍网路的性能不提升的次数，高于这个次数就降低学习率。
verbose：同上。
mode：同上。
min_lr学习率的下限。

更多参数：
`lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10,
 verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)`

mode ：(str) ，可选择‘min’或者‘max’，min表示当监控量停止下降的时候，学习率将减小，max表示当监控量停止上升的时候，
学习率将减小。默认值为‘min’。
threshold：（float） - 测量新最佳值的阈值，仅关注重大变化。 默认值：1e-4。
cooldown： 减少lr后恢复正常操作之前要等待的时期数。 默认值：0。
eps ，适用于lr的最小衰减。 如果新旧lr之间的差异小于eps，则忽略更新。 默认值：1e-8。

小结下：主要是在训练后期进行收敛的操作，先通过多次数的训练找到过拟合点，再进行学习率衰减。目前，跑完是这样理解了。接下来开始实操

---------------------------------------------------------------------------------------------------------
![image](https://user-images.githubusercontent.com/46392381/60752795-6402f080-9ffc-11e9-955c-0f2860242214.png)

`history = model.fit(x=x_train, 
                  y=y_train, 
                  batch_size=16, 
                  epochs=5, 
                  verbose=1, 
                  callbacks=callbacks, 
                  validation_split=0.25, 
                  shuffle=True, 
                  initial_epoch=0, 
                 )`
先过下代码
x：输入数据。如果模型只有一个输入，那么x的类型是numpy
array，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array
y：标签，numpy array

batch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。
理解为批处理参数，它的极限值为训练集样本总数。
在深度学习中所涉及到的数据都是比较多的，一般都采用小批量数据处理原则。但是，批次越小，梯度的估值就越不准确。
![image](https://user-images.githubusercontent.com/46392381/60752826-f1464500-9ffc-11e9-918e-2eea063052ba.png)
Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况。
Batch Size从小到大的变化对网络影响 
1、没有Batch Size，梯度准确，只适用于小样本数据库 
2、Batch Size=1，梯度变来变去，非常不准确，网络很难收敛。 
3、Batch Size增大，梯度变准确， 
4、Batch Size增大，梯度已经非常准确，再增加Batch Size也没有用
所以在硬件性能有保障的前提下，可以逐步提升直到梯度准确没有太大变化。


epochs：整数，训练终止时的epoch值，训练将在达到该epoch值时停止，当没有设置initial_epoch时，它就是训练的总轮数，
否则训练的总轮数为epochs - inital_epoch
verbose：同上。
callbacks：list，其中的元素是keras.callbacks.Callback的对象。这个list中的回调函数将会在训练过程中的适当时机被调用
，参考回调函数。本次案例为[es,cp,lr]

validation_split：0~1之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，
并在每个epoch结束后测试的模型的指标，如损失函数、精确度等。注意，validation_split的划分在shuffle之前，
因此如果你的数据本身是有序的，需要先手工打乱再指定validation_split，否则可能会出现验证集样本不均匀。

shuffle：布尔值或字符串，一般为布尔值，表示是否在训练过程中随机打乱输入样本的顺序。若为字符串“batch”，
则是用来处理HDF5数据的特殊情况，它将在batch内部将数据打乱。

initial_epoch: 从该参数指定的epoch开始训练，在继续之前的训练时有用。

fit函数返回一个History的对象，其History.history属性记录了损失函数和其他指标的数值随epoch变化的情况，
如果有验证集的话，也包含了验证集的这些指标变化情况

小结下：batch_size、epochs的调整对训练结果有明显的区别，同时要根据输出的日志查看到相应的提升变化进行调整。
------------------------------------------------------------------------------------------------


