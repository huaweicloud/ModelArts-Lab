{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352366"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/training.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "\n",
    "s1 = f['sen1']\n",
    "s2 = f['sen2']\n",
    "y = f['label']\n",
    "\n",
    "\n",
    "len(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 利用Adam算法优化CNN的demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the h5 file\n",
      "The shape of data is  (24119, 18449)\n",
      "step 0, train accuracy 0.13\n",
      "step 0, train loss 2227.583252\n",
      "step 200, train accuracy 0.52\n",
      "step 200, train loss 566.776917\n",
      "step 400, train accuracy 0.52\n",
      "step 400, train loss 457.642822\n",
      "step 600, train accuracy 0.585\n",
      "step 600, train loss 313.471619\n",
      "step 800, train accuracy 0.505\n",
      "step 800, train loss 326.307465\n",
      "step 1000, train accuracy 0.57\n",
      "step 1000, train loss 329.557800\n",
      "step 1200, train accuracy 0.6\n",
      "step 1200, train loss 292.324890\n",
      "step 1400, train accuracy 0.56\n",
      "step 1400, train loss 285.143494\n",
      "step 1600, train accuracy 0.61\n",
      "step 1600, train loss 234.953766\n",
      "step 1800, train accuracy 0.595\n",
      "step 1800, train loss 270.195343\n",
      "step 2000, train accuracy 0.64\n",
      "step 2000, train loss 286.467865\n",
      "step 2200, train accuracy 0.645\n",
      "step 2200, train loss 226.883713\n",
      "step 2400, train accuracy 0.63\n",
      "step 2400, train loss 265.833801\n",
      "step 2600, train accuracy 0.72\n",
      "step 2600, train loss 252.993439\n",
      "step 2800, train accuracy 0.69\n",
      "step 2800, train loss 194.748169\n",
      "step 3000, train accuracy 0.735\n",
      "step 3000, train loss 196.917816\n",
      "step 3200, train accuracy 0.74\n",
      "step 3200, train loss 197.452957\n",
      "step 3400, train accuracy 0.735\n",
      "step 3400, train loss 161.954132\n",
      "step 3600, train accuracy 0.735\n",
      "step 3600, train loss 192.121277\n",
      "step 3800, train accuracy 0.73\n",
      "step 3800, train loss 173.849182\n",
      "step 4000, train accuracy 0.715\n",
      "step 4000, train loss 166.942200\n",
      "step 4200, train accuracy 0.76\n",
      "step 4200, train loss 196.328262\n",
      "step 4400, train accuracy 0.79\n",
      "step 4400, train loss 125.378304\n",
      "step 4600, train accuracy 0.825\n",
      "step 4600, train loss 124.819923\n",
      "step 4800, train accuracy 0.78\n",
      "step 4800, train loss 174.134247\n",
      "step 5000, train accuracy 0.805\n",
      "step 5000, train loss 130.072205\n",
      "step 5200, train accuracy 0.83\n",
      "step 5200, train loss 126.195404\n",
      "step 5400, train accuracy 0.805\n",
      "step 5400, train loss 140.400986\n",
      "step 5600, train accuracy 0.78\n",
      "step 5600, train loss 141.315964\n",
      "step 5800, train accuracy 0.85\n",
      "step 5800, train loss 111.255424\n",
      "step 6000, train accuracy 0.835\n",
      "step 6000, train loss 118.379578\n",
      "step 6200, train accuracy 0.81\n",
      "step 6200, train loss 147.612320\n",
      "step 6400, train accuracy 0.85\n",
      "step 6400, train loss 113.098145\n",
      "step 6600, train accuracy 0.82\n",
      "step 6600, train loss 130.739319\n",
      "step 6800, train accuracy 0.835\n",
      "step 6800, train loss 103.583351\n",
      "step 7000, train accuracy 0.845\n",
      "step 7000, train loss 115.811813\n",
      "step 7200, train accuracy 0.85\n",
      "step 7200, train loss 106.656372\n",
      "step 7400, train accuracy 0.835\n",
      "step 7400, train loss 153.533417\n",
      "step 7600, train accuracy 0.845\n",
      "step 7600, train loss 92.523415\n",
      "step 7800, train accuracy 0.85\n",
      "step 7800, train loss 103.503738\n",
      "step 8000, train accuracy 0.9\n",
      "step 8000, train loss 80.453629\n",
      "step 8200, train accuracy 0.895\n",
      "step 8200, train loss 72.551720\n",
      "step 8400, train accuracy 0.9\n",
      "step 8400, train loss 81.987152\n",
      "step 8600, train accuracy 0.885\n",
      "step 8600, train loss 83.815979\n",
      "step 8800, train accuracy 0.91\n",
      "step 8800, train loss 74.126190\n",
      "step 9000, train accuracy 0.89\n",
      "step 9000, train loss 81.973282\n",
      "step 9200, train accuracy 0.92\n",
      "step 9200, train loss 66.200623\n",
      "step 9400, train accuracy 0.91\n",
      "step 9400, train loss 65.700615\n",
      "step 9600, train accuracy 0.925\n",
      "step 9600, train loss 58.008343\n",
      "step 9800, train accuracy 0.895\n",
      "step 9800, train loss 59.875912\n",
      "step 10000, train accuracy 0.92\n",
      "step 10000, train loss 73.653687\n",
      "step 10200, train accuracy 0.91\n",
      "step 10200, train loss 66.290039\n",
      "step 10400, train accuracy 0.915\n",
      "step 10400, train loss 71.936813\n",
      "step 10600, train accuracy 0.885\n",
      "step 10600, train loss 71.550003\n",
      "step 10800, train accuracy 0.92\n",
      "step 10800, train loss 57.737221\n",
      "step 11000, train accuracy 0.9\n",
      "step 11000, train loss 67.421051\n",
      "step 11200, train accuracy 0.93\n",
      "step 11200, train loss 43.895424\n",
      "step 11400, train accuracy 0.94\n",
      "step 11400, train loss 46.180389\n",
      "step 11600, train accuracy 0.935\n",
      "step 11600, train loss 57.503139\n",
      "step 11800, train accuracy 0.96\n",
      "step 11800, train loss 49.116089\n",
      "step 12000, train accuracy 0.94\n",
      "step 12000, train loss 46.606701\n",
      "step 12200, train accuracy 0.935\n",
      "step 12200, train loss 43.627991\n",
      "step 12400, train accuracy 0.93\n",
      "step 12400, train loss 51.439102\n",
      "step 12600, train accuracy 0.955\n",
      "step 12600, train loss 38.022369\n",
      "step 12800, train accuracy 0.96\n",
      "step 12800, train loss 28.641804\n",
      "step 13000, train accuracy 0.95\n",
      "step 13000, train loss 30.766560\n",
      "step 13200, train accuracy 0.955\n",
      "step 13200, train loss 46.473660\n",
      "step 13400, train accuracy 0.97\n",
      "step 13400, train loss 24.439915\n",
      "step 13600, train accuracy 0.965\n",
      "step 13600, train loss 41.067402\n",
      "step 13800, train accuracy 0.975\n",
      "step 13800, train loss 23.538319\n",
      "step 14000, train accuracy 0.965\n",
      "step 14000, train loss 51.547684\n",
      "step 14200, train accuracy 0.97\n",
      "step 14200, train loss 30.345348\n",
      "step 14400, train accuracy 0.985\n",
      "step 14400, train loss 20.635719\n",
      "step 14600, train accuracy 0.975\n",
      "step 14600, train loss 36.440590\n",
      "step 14800, train accuracy 0.975\n",
      "step 14800, train loss 26.545742\n",
      "step 15000, train accuracy 0.995\n",
      "step 15000, train loss 13.887944\n",
      "step 15200, train accuracy 0.985\n",
      "step 15200, train loss 18.495258\n",
      "step 15400, train accuracy 0.995\n",
      "step 15400, train loss 31.025215\n",
      "step 15600, train accuracy 0.995\n",
      "step 15600, train loss 15.508052\n",
      "step 15800, train accuracy 0.99\n",
      "step 15800, train loss 14.614855\n",
      "step 16000, train accuracy 0.99\n",
      "step 16000, train loss 16.615267\n",
      "step 16200, train accuracy 1\n",
      "step 16200, train loss 8.236763\n",
      "step 16400, train accuracy 0.995\n",
      "step 16400, train loss 25.243799\n",
      "step 16600, train accuracy 0.995\n",
      "step 16600, train loss 11.107920\n",
      "step 16800, train accuracy 0.995\n",
      "step 16800, train loss 9.672273\n",
      "step 17000, train accuracy 0.99\n",
      "step 17000, train loss 11.707303\n",
      "step 17200, train accuracy 0.99\n",
      "step 17200, train loss 12.264877\n",
      "step 17400, train accuracy 1\n",
      "step 17400, train loss 8.011111\n",
      "step 17600, train accuracy 1\n",
      "step 17600, train loss 8.075419\n",
      "step 17800, train accuracy 0.995\n",
      "step 17800, train loss 22.157843\n",
      "step 18000, train accuracy 1\n",
      "step 18000, train loss 7.118594\n",
      "step 18200, train accuracy 0.995\n",
      "step 18200, train loss 21.714878\n",
      "step 18400, train accuracy 1\n",
      "step 18400, train loss 6.607351\n",
      "step 18600, train accuracy 1\n",
      "step 18600, train loss 3.884237\n",
      "step 18800, train accuracy 1\n",
      "step 18800, train loss 7.447076\n",
      "step 19000, train accuracy 0.995\n",
      "step 19000, train loss 6.891587\n",
      "step 19200, train accuracy 0.995\n",
      "step 19200, train loss 5.864908\n",
      "step 19400, train accuracy 1\n",
      "step 19400, train loss 5.564776\n",
      "step 19600, train accuracy 1\n",
      "step 19600, train loss 5.825256\n",
      "step 19800, train accuracy 1\n",
      "step 19800, train loss 4.114892\n",
      "step 20000, train accuracy 1\n",
      "step 20000, train loss 4.144970\n",
      "step 20200, train accuracy 1\n",
      "step 20200, train loss 5.516111\n",
      "step 20400, train accuracy 1\n",
      "step 20400, train loss 4.119995\n",
      "step 20600, train accuracy 0.995\n",
      "step 20600, train loss 20.327686\n",
      "step 20800, train accuracy 1\n",
      "step 20800, train loss 3.290332\n",
      "step 21000, train accuracy 1\n",
      "step 21000, train loss 3.728442\n",
      "step 21200, train accuracy 1\n",
      "step 21200, train loss 3.859458\n",
      "step 21400, train accuracy 1\n",
      "step 21400, train loss 2.556481\n",
      "step 21600, train accuracy 1\n",
      "step 21600, train loss 3.328175\n",
      "step 21800, train accuracy 1\n",
      "step 21800, train loss 2.886195\n",
      "step 22000, train accuracy 1\n",
      "step 22000, train loss 2.047382\n",
      "step 22200, train accuracy 0.995\n",
      "step 22200, train loss 18.649828\n",
      "step 22400, train accuracy 0.995\n",
      "step 22400, train loss 19.259907\n",
      "step 22600, train accuracy 1\n",
      "step 22600, train loss 2.745069\n",
      "step 22800, train accuracy 1\n",
      "step 22800, train loss 2.603540\n",
      "step 23000, train accuracy 0.99\n",
      "step 23000, train loss 34.383980\n",
      "step 23200, train accuracy 0.995\n",
      "step 23200, train loss 18.206684\n",
      "step 23400, train accuracy 1\n",
      "step 23400, train loss 1.648728\n",
      "step 23600, train accuracy 1\n",
      "step 23600, train loss 2.009357\n",
      "Final:step 23675, train loss 0.976542\n",
      "Final accuracy 1\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/validation.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "print('Get the h5 file')\n",
    "\n",
    "s1 = np.array(f['sen1'])\n",
    "s2 = np.array(f['sen2'])\n",
    "y = np.array(f['label'])\n",
    "\n",
    "x = []\n",
    "for i in range(0,s1.shape[0]):\n",
    "    temp1 = s1[i].flatten()\n",
    "    temp2 = s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    x.append(temp)\n",
    "x = np.array(x)\n",
    "\n",
    "data = np.hstack((x,y))\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "\n",
    "batch_size = 200\n",
    "for i in range(30000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "        print('step %d, train loss %f'%(i,loss_temp))\n",
    "    if loss_temp < 1:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print('Final:step %d, train loss %f'%(i,loss_temp))\n",
    "        print (\"Final accuracy %g\" %train_accuracy)\n",
    "        break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97654206"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "loss_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54720002"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali = np.load('E:/Alibaba German AI Challenge/data_process/sample_of_training.npy')\n",
    "accuracy.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAE/CAYAAAC5EpGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEfEhBbiIiIgPKcBFRER86P8DkNQRqVH0s3YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d1c518b6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('second_20k_vali_as_train_Adam.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用均衡分布的数据进行训练 demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (50989, 18449)\n",
      "step 0, train accuracy 0.095\n",
      "step 0, train loss 2501.947998\n",
      "step 250, train accuracy 0.325\n",
      "step 250, train loss 691.175171\n",
      "step 500, train accuracy 0.4\n",
      "step 500, train loss 464.166840\n",
      "step 750, train accuracy 0.385\n",
      "step 750, train loss 478.504944\n",
      "step 1000, train accuracy 0.415\n",
      "step 1000, train loss 383.896942\n",
      "step 1250, train accuracy 0.365\n",
      "step 1250, train loss 419.449768\n",
      "step 1500, train accuracy 0.425\n",
      "step 1500, train loss 420.824310\n",
      "step 1750, train accuracy 0.43\n",
      "step 1750, train loss 395.652863\n",
      "step 2000, train accuracy 0.43\n",
      "step 2000, train loss 363.959473\n",
      "step 2250, train accuracy 0.42\n",
      "step 2250, train loss 341.458374\n",
      "step 2500, train accuracy 0.555\n",
      "step 2500, train loss 306.630798\n",
      "step 2750, train accuracy 0.475\n",
      "step 2750, train loss 323.587952\n",
      "step 3000, train accuracy 0.48\n",
      "step 3000, train loss 302.360168\n",
      "step 3250, train accuracy 0.475\n",
      "step 3250, train loss 299.143127\n",
      "step 3500, train accuracy 0.495\n",
      "step 3500, train loss 300.170990\n",
      "step 3750, train accuracy 0.51\n",
      "step 3750, train loss 311.753876\n",
      "step 4000, train accuracy 0.59\n",
      "step 4000, train loss 258.111206\n",
      "step 4250, train accuracy 0.55\n",
      "step 4250, train loss 258.508026\n",
      "step 4500, train accuracy 0.5\n",
      "step 4500, train loss 311.390320\n",
      "step 4750, train accuracy 0.48\n",
      "step 4750, train loss 328.088348\n",
      "step 5000, train accuracy 0.475\n",
      "step 5000, train loss 316.329529\n",
      "step 5250, train accuracy 0.57\n",
      "step 5250, train loss 263.346313\n",
      "step 5500, train accuracy 0.515\n",
      "step 5500, train loss 284.621368\n",
      "step 5750, train accuracy 0.59\n",
      "step 5750, train loss 229.452347\n",
      "step 6000, train accuracy 0.595\n",
      "step 6000, train loss 238.978439\n",
      "step 6250, train accuracy 0.585\n",
      "step 6250, train loss 250.040436\n",
      "step 6500, train accuracy 0.565\n",
      "step 6500, train loss 257.015137\n",
      "step 6750, train accuracy 0.505\n",
      "step 6750, train loss 269.617218\n",
      "step 7000, train accuracy 0.64\n",
      "step 7000, train loss 213.652893\n",
      "step 7250, train accuracy 0.56\n",
      "step 7250, train loss 272.556641\n",
      "step 7500, train accuracy 0.65\n",
      "step 7500, train loss 217.420715\n",
      "step 7750, train accuracy 0.6\n",
      "step 7750, train loss 245.837555\n",
      "step 8000, train accuracy 0.6\n",
      "step 8000, train loss 240.630707\n",
      "step 8250, train accuracy 0.63\n",
      "step 8250, train loss 229.314178\n",
      "step 8500, train accuracy 0.56\n",
      "step 8500, train loss 258.536011\n",
      "step 8750, train accuracy 0.585\n",
      "step 8750, train loss 231.353897\n",
      "step 9000, train accuracy 0.595\n",
      "step 9000, train loss 239.685043\n",
      "step 9250, train accuracy 0.615\n",
      "step 9250, train loss 219.584000\n",
      "step 9500, train accuracy 0.565\n",
      "step 9500, train loss 231.586411\n",
      "step 9750, train accuracy 0.59\n",
      "step 9750, train loss 236.071762\n",
      "step 10000, train accuracy 0.585\n",
      "step 10000, train loss 240.872635\n",
      "step 10250, train accuracy 0.62\n",
      "step 10250, train loss 220.365372\n",
      "step 10500, train accuracy 0.63\n",
      "step 10500, train loss 200.533051\n",
      "step 10750, train accuracy 0.66\n",
      "step 10750, train loss 215.022614\n",
      "step 11000, train accuracy 0.68\n",
      "step 11000, train loss 210.721954\n",
      "step 11250, train accuracy 0.61\n",
      "step 11250, train loss 231.508301\n",
      "step 11500, train accuracy 0.595\n",
      "step 11500, train loss 235.371429\n",
      "step 11750, train accuracy 0.61\n",
      "step 11750, train loss 222.915359\n",
      "step 12000, train accuracy 0.645\n",
      "step 12000, train loss 198.152939\n",
      "step 12250, train accuracy 0.63\n",
      "step 12250, train loss 218.070084\n",
      "step 12500, train accuracy 0.675\n",
      "step 12500, train loss 184.892258\n",
      "step 12750, train accuracy 0.66\n",
      "step 12750, train loss 196.683670\n",
      "step 13000, train accuracy 0.635\n",
      "step 13000, train loss 203.013260\n",
      "step 13250, train accuracy 0.61\n",
      "step 13250, train loss 242.405365\n",
      "step 13500, train accuracy 0.68\n",
      "step 13500, train loss 190.536392\n",
      "step 13750, train accuracy 0.65\n",
      "step 13750, train loss 203.206253\n",
      "step 14000, train accuracy 0.67\n",
      "step 14000, train loss 250.225754\n",
      "step 14250, train accuracy 0.67\n",
      "step 14250, train loss 206.796890\n",
      "step 14500, train accuracy 0.7\n",
      "step 14500, train loss 181.381424\n",
      "step 14750, train accuracy 0.655\n",
      "step 14750, train loss 187.097839\n",
      "step 15000, train accuracy 0.69\n",
      "step 15000, train loss 190.400787\n",
      "step 15250, train accuracy 0.675\n",
      "step 15250, train loss 187.558350\n",
      "step 15500, train accuracy 0.61\n",
      "step 15500, train loss 220.053101\n",
      "step 15750, train accuracy 0.71\n",
      "step 15750, train loss 193.496216\n",
      "step 16000, train accuracy 0.67\n",
      "step 16000, train loss 204.453339\n",
      "step 16250, train accuracy 0.705\n",
      "step 16250, train loss 179.162979\n",
      "step 16500, train accuracy 0.735\n",
      "step 16500, train loss 168.343079\n",
      "step 16750, train accuracy 0.635\n",
      "step 16750, train loss 197.536606\n",
      "step 17000, train accuracy 0.715\n",
      "step 17000, train loss 162.504150\n",
      "step 17250, train accuracy 0.715\n",
      "step 17250, train loss 159.959137\n",
      "step 17500, train accuracy 0.69\n",
      "step 17500, train loss 157.129547\n",
      "step 17750, train accuracy 0.73\n",
      "step 17750, train loss 169.675552\n",
      "step 18000, train accuracy 0.7\n",
      "step 18000, train loss 168.713043\n",
      "step 18250, train accuracy 0.73\n",
      "step 18250, train loss 174.658768\n",
      "step 18500, train accuracy 0.725\n",
      "step 18500, train loss 169.415283\n",
      "step 18750, train accuracy 0.73\n",
      "step 18750, train loss 153.456467\n",
      "step 19000, train accuracy 0.685\n",
      "step 19000, train loss 179.562469\n",
      "step 19250, train accuracy 0.675\n",
      "step 19250, train loss 181.403244\n",
      "step 19500, train accuracy 0.655\n",
      "step 19500, train loss 184.268097\n",
      "step 19750, train accuracy 0.725\n",
      "step 19750, train loss 153.515213\n",
      "step 20000, train accuracy 0.745\n",
      "step 20000, train loss 145.189728\n",
      "step 20250, train accuracy 0.7\n",
      "step 20250, train loss 170.604141\n",
      "step 20500, train accuracy 0.79\n",
      "step 20500, train loss 151.715485\n",
      "step 20750, train accuracy 0.79\n",
      "step 20750, train loss 144.890976\n",
      "step 21000, train accuracy 0.78\n",
      "step 21000, train loss 144.851303\n",
      "step 21250, train accuracy 0.715\n",
      "step 21250, train loss 156.311829\n",
      "step 21500, train accuracy 0.755\n",
      "step 21500, train loss 154.785919\n",
      "step 21750, train accuracy 0.765\n",
      "step 21750, train loss 148.689774\n",
      "step 22000, train accuracy 0.705\n",
      "step 22000, train loss 170.298508\n",
      "step 22250, train accuracy 0.77\n",
      "step 22250, train loss 158.865494\n",
      "step 22500, train accuracy 0.72\n",
      "step 22500, train loss 171.397659\n",
      "step 22750, train accuracy 0.82\n",
      "step 22750, train loss 143.836853\n",
      "step 23000, train accuracy 0.78\n",
      "step 23000, train loss 156.815613\n",
      "step 23250, train accuracy 0.775\n",
      "step 23250, train loss 145.757339\n",
      "step 23500, train accuracy 0.715\n",
      "step 23500, train loss 192.925919\n",
      "step 23750, train accuracy 0.77\n",
      "step 23750, train loss 142.493912\n",
      "step 24000, train accuracy 0.735\n",
      "step 24000, train loss 146.738037\n",
      "step 24250, train accuracy 0.725\n",
      "step 24250, train loss 150.799713\n",
      "step 24500, train accuracy 0.78\n",
      "step 24500, train loss 134.493103\n",
      "step 24750, train accuracy 0.785\n",
      "step 24750, train loss 142.152481\n",
      "step 25000, train accuracy 0.72\n",
      "step 25000, train loss 169.016479\n",
      "step 25250, train accuracy 0.765\n",
      "step 25250, train loss 134.902222\n",
      "step 25500, train accuracy 0.8\n",
      "step 25500, train loss 152.161179\n",
      "step 25750, train accuracy 0.775\n",
      "step 25750, train loss 126.139786\n",
      "step 26000, train accuracy 0.765\n",
      "step 26000, train loss 138.020950\n",
      "step 26250, train accuracy 0.775\n",
      "step 26250, train loss 118.575531\n",
      "step 26500, train accuracy 0.695\n",
      "step 26500, train loss 148.555267\n",
      "step 26750, train accuracy 0.86\n",
      "step 26750, train loss 125.237694\n",
      "step 27000, train accuracy 0.845\n",
      "step 27000, train loss 108.605743\n",
      "step 27250, train accuracy 0.785\n",
      "step 27250, train loss 134.921539\n",
      "step 27500, train accuracy 0.825\n",
      "step 27500, train loss 133.037598\n",
      "step 27750, train accuracy 0.795\n",
      "step 27750, train loss 114.235733\n",
      "step 28000, train accuracy 0.8\n",
      "step 28000, train loss 116.423225\n",
      "step 28250, train accuracy 0.76\n",
      "step 28250, train loss 136.295212\n",
      "step 28500, train accuracy 0.805\n",
      "step 28500, train loss 115.910805\n",
      "step 28750, train accuracy 0.77\n",
      "step 28750, train loss 128.162140\n",
      "step 29000, train accuracy 0.8\n",
      "step 29000, train loss 135.548065\n",
      "step 29250, train accuracy 0.78\n",
      "step 29250, train loss 127.885857\n",
      "step 29500, train accuracy 0.735\n",
      "step 29500, train loss 138.035156\n",
      "step 29750, train accuracy 0.78\n",
      "step 29750, train loss 111.320900\n",
      "step 30000, train accuracy 0.78\n",
      "step 30000, train loss 128.282181\n",
      "step 30250, train accuracy 0.815\n",
      "step 30250, train loss 137.791840\n",
      "step 30500, train accuracy 0.81\n",
      "step 30500, train loss 105.711861\n",
      "step 30750, train accuracy 0.765\n",
      "step 30750, train loss 128.067657\n",
      "step 31000, train accuracy 0.8\n",
      "step 31000, train loss 120.111191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31250, train accuracy 0.84\n",
      "step 31250, train loss 114.688187\n",
      "step 31500, train accuracy 0.835\n",
      "step 31500, train loss 113.541542\n",
      "step 31750, train accuracy 0.805\n",
      "step 31750, train loss 141.345490\n",
      "step 32000, train accuracy 0.86\n",
      "step 32000, train loss 102.159286\n",
      "step 32250, train accuracy 0.775\n",
      "step 32250, train loss 120.237350\n",
      "step 32500, train accuracy 0.825\n",
      "step 32500, train loss 100.407204\n",
      "step 32750, train accuracy 0.825\n",
      "step 32750, train loss 110.903641\n",
      "step 33000, train accuracy 0.825\n",
      "step 33000, train loss 110.227005\n",
      "step 33250, train accuracy 0.855\n",
      "step 33250, train loss 101.350990\n",
      "step 33500, train accuracy 0.825\n",
      "step 33500, train loss 103.588120\n",
      "step 33750, train accuracy 0.825\n",
      "step 33750, train loss 102.337891\n",
      "step 34000, train accuracy 0.79\n",
      "step 34000, train loss 119.780388\n",
      "step 34250, train accuracy 0.855\n",
      "step 34250, train loss 114.606903\n",
      "step 34500, train accuracy 0.855\n",
      "step 34500, train loss 90.204430\n",
      "step 34750, train accuracy 0.82\n",
      "step 34750, train loss 125.883163\n",
      "step 35000, train accuracy 0.86\n",
      "step 35000, train loss 88.387428\n",
      "step 35250, train accuracy 0.835\n",
      "step 35250, train loss 102.264236\n",
      "step 35500, train accuracy 0.885\n",
      "step 35500, train loss 76.856850\n",
      "step 35750, train accuracy 0.81\n",
      "step 35750, train loss 99.735825\n",
      "step 36000, train accuracy 0.895\n",
      "step 36000, train loss 84.675262\n",
      "step 36250, train accuracy 0.84\n",
      "step 36250, train loss 90.167885\n",
      "step 36500, train accuracy 0.845\n",
      "step 36500, train loss 85.913544\n",
      "step 36750, train accuracy 0.865\n",
      "step 36750, train loss 81.286446\n",
      "step 37000, train accuracy 0.83\n",
      "step 37000, train loss 96.534187\n",
      "step 37250, train accuracy 0.875\n",
      "step 37250, train loss 83.940994\n",
      "step 37500, train accuracy 0.86\n",
      "step 37500, train loss 92.176338\n",
      "step 37750, train accuracy 0.89\n",
      "step 37750, train loss 80.160965\n",
      "step 38000, train accuracy 0.885\n",
      "step 38000, train loss 69.371712\n",
      "step 38250, train accuracy 0.875\n",
      "step 38250, train loss 77.216469\n",
      "step 38500, train accuracy 0.835\n",
      "step 38500, train loss 89.416954\n",
      "step 38750, train accuracy 0.83\n",
      "step 38750, train loss 83.469086\n",
      "step 39000, train accuracy 0.86\n",
      "step 39000, train loss 77.792397\n",
      "step 39250, train accuracy 0.875\n",
      "step 39250, train loss 81.356689\n",
      "step 39500, train accuracy 0.9\n",
      "step 39500, train loss 84.220703\n",
      "step 39750, train accuracy 0.89\n",
      "step 39750, train loss 70.232384\n",
      "step 40000, train accuracy 0.86\n",
      "step 40000, train loss 92.989769\n",
      "step 40250, train accuracy 0.825\n",
      "step 40250, train loss 110.280464\n",
      "step 40500, train accuracy 0.9\n",
      "step 40500, train loss 70.515137\n",
      "step 40750, train accuracy 0.895\n",
      "step 40750, train loss 61.057423\n",
      "step 41000, train accuracy 0.825\n",
      "step 41000, train loss 97.829414\n",
      "step 41250, train accuracy 0.875\n",
      "step 41250, train loss 71.082001\n",
      "step 41500, train accuracy 0.84\n",
      "step 41500, train loss 84.904709\n",
      "step 41750, train accuracy 0.87\n",
      "step 41750, train loss 77.212471\n",
      "step 42000, train accuracy 0.875\n",
      "step 42000, train loss 76.397324\n",
      "step 42250, train accuracy 0.865\n",
      "step 42250, train loss 99.975761\n",
      "step 42500, train accuracy 0.885\n",
      "step 42500, train loss 69.439934\n",
      "step 42750, train accuracy 0.92\n",
      "step 42750, train loss 59.634369\n",
      "step 43000, train accuracy 0.88\n",
      "step 43000, train loss 71.642212\n",
      "step 43250, train accuracy 0.89\n",
      "step 43250, train loss 67.396111\n",
      "step 43500, train accuracy 0.89\n",
      "step 43500, train loss 59.475021\n",
      "step 43750, train accuracy 0.905\n",
      "step 43750, train loss 55.938202\n",
      "step 44000, train accuracy 0.905\n",
      "step 44000, train loss 78.140038\n",
      "step 44250, train accuracy 0.87\n",
      "step 44250, train loss 93.323471\n",
      "step 44500, train accuracy 0.88\n",
      "step 44500, train loss 68.625183\n",
      "step 44750, train accuracy 0.905\n",
      "step 44750, train loss 58.031837\n",
      "step 45000, train accuracy 0.87\n",
      "step 45000, train loss 72.644783\n",
      "step 45250, train accuracy 0.895\n",
      "step 45250, train loss 75.343758\n",
      "step 45500, train accuracy 0.9\n",
      "step 45500, train loss 63.779221\n",
      "step 45750, train accuracy 0.91\n",
      "step 45750, train loss 69.708374\n",
      "step 46000, train accuracy 0.89\n",
      "step 46000, train loss 66.751625\n",
      "step 46250, train accuracy 0.91\n",
      "step 46250, train loss 60.384289\n",
      "step 46500, train accuracy 0.93\n",
      "step 46500, train loss 50.390560\n",
      "step 46750, train accuracy 0.925\n",
      "step 46750, train loss 46.527191\n",
      "step 47000, train accuracy 0.915\n",
      "step 47000, train loss 54.374443\n",
      "step 47250, train accuracy 0.905\n",
      "step 47250, train loss 54.791977\n",
      "step 47500, train accuracy 0.875\n",
      "step 47500, train loss 73.940178\n",
      "step 47750, train accuracy 0.93\n",
      "step 47750, train loss 45.551617\n",
      "step 48000, train accuracy 0.935\n",
      "step 48000, train loss 48.086292\n",
      "step 48250, train accuracy 0.95\n",
      "step 48250, train loss 62.570950\n",
      "step 48500, train accuracy 0.945\n",
      "step 48500, train loss 47.738892\n",
      "step 48750, train accuracy 0.94\n",
      "step 48750, train loss 56.709915\n",
      "step 49000, train accuracy 0.955\n",
      "step 49000, train loss 47.083828\n",
      "step 49250, train accuracy 0.94\n",
      "step 49250, train loss 52.414742\n",
      "step 49500, train accuracy 0.93\n",
      "step 49500, train loss 46.952011\n",
      "step 49750, train accuracy 0.965\n",
      "step 49750, train loss 40.506454\n",
      "step 50000, train accuracy 0.965\n",
      "step 50000, train loss 42.614887\n",
      "step 50250, train accuracy 0.945\n",
      "step 50250, train loss 43.654232\n",
      "step 50500, train accuracy 0.925\n",
      "step 50500, train loss 66.967773\n",
      "step 50750, train accuracy 0.96\n",
      "step 50750, train loss 35.573090\n",
      "step 51000, train accuracy 0.925\n",
      "step 51000, train loss 52.290459\n",
      "step 51250, train accuracy 0.95\n",
      "step 51250, train loss 41.574791\n",
      "step 51500, train accuracy 0.97\n",
      "step 51500, train loss 32.036385\n",
      "step 51750, train accuracy 0.945\n",
      "step 51750, train loss 37.481083\n",
      "step 52000, train accuracy 0.95\n",
      "step 52000, train loss 35.851803\n",
      "step 52250, train accuracy 0.97\n",
      "step 52250, train loss 36.114502\n",
      "step 52500, train accuracy 0.92\n",
      "step 52500, train loss 52.234749\n",
      "step 52750, train accuracy 0.94\n",
      "step 52750, train loss 39.714352\n",
      "step 53000, train accuracy 0.905\n",
      "step 53000, train loss 53.130585\n",
      "step 53250, train accuracy 0.93\n",
      "step 53250, train loss 44.409340\n",
      "step 53500, train accuracy 0.935\n",
      "step 53500, train loss 36.016239\n",
      "step 53750, train accuracy 0.965\n",
      "step 53750, train loss 46.709000\n",
      "step 54000, train accuracy 0.95\n",
      "step 54000, train loss 40.193993\n",
      "step 54250, train accuracy 0.945\n",
      "step 54250, train loss 32.962631\n",
      "step 54500, train accuracy 0.97\n",
      "step 54500, train loss 28.694590\n",
      "step 54750, train accuracy 0.975\n",
      "step 54750, train loss 23.381413\n",
      "step 55000, train accuracy 0.95\n",
      "step 55000, train loss 34.200321\n",
      "step 55250, train accuracy 0.975\n",
      "step 55250, train loss 26.345453\n",
      "step 55500, train accuracy 0.97\n",
      "step 55500, train loss 31.116522\n",
      "step 55750, train accuracy 0.96\n",
      "step 55750, train loss 30.382528\n",
      "step 56000, train accuracy 0.965\n",
      "step 56000, train loss 28.437944\n",
      "step 56250, train accuracy 0.97\n",
      "step 56250, train loss 29.218138\n",
      "step 56500, train accuracy 0.975\n",
      "step 56500, train loss 26.913303\n",
      "step 56750, train accuracy 0.96\n",
      "step 56750, train loss 33.170418\n",
      "step 57000, train accuracy 0.975\n",
      "step 57000, train loss 28.243034\n",
      "step 57250, train accuracy 0.965\n",
      "step 57250, train loss 28.855202\n",
      "step 57500, train accuracy 0.955\n",
      "step 57500, train loss 43.355358\n",
      "step 57750, train accuracy 0.965\n",
      "step 57750, train loss 30.643475\n",
      "step 58000, train accuracy 0.97\n",
      "step 58000, train loss 25.213587\n",
      "step 58250, train accuracy 0.97\n",
      "step 58250, train loss 23.910816\n",
      "step 58500, train accuracy 0.98\n",
      "step 58500, train loss 25.655304\n",
      "step 58750, train accuracy 0.97\n",
      "step 58750, train loss 27.872833\n",
      "step 59000, train accuracy 0.975\n",
      "step 59000, train loss 34.463711\n",
      "step 59250, train accuracy 0.95\n",
      "step 59250, train loss 33.610729\n",
      "step 59500, train accuracy 0.97\n",
      "step 59500, train loss 26.098040\n",
      "step 59750, train accuracy 0.97\n",
      "step 59750, train loss 21.768784\n",
      "step 60000, train accuracy 0.975\n",
      "step 60000, train loss 22.471706\n",
      "step 60250, train accuracy 0.995\n",
      "step 60250, train loss 23.230228\n",
      "step 60500, train accuracy 0.97\n",
      "step 60500, train loss 26.498486\n",
      "step 60750, train accuracy 0.96\n",
      "step 60750, train loss 24.435865\n",
      "step 61000, train accuracy 0.96\n",
      "step 61000, train loss 41.481770\n",
      "step 61250, train accuracy 0.975\n",
      "step 61250, train loss 18.157310\n",
      "step 61500, train accuracy 0.96\n",
      "step 61500, train loss 27.538250\n",
      "step 61750, train accuracy 0.98\n",
      "step 61750, train loss 33.555702\n",
      "step 62000, train accuracy 0.975\n",
      "step 62000, train loss 19.532280\n",
      "step 62250, train accuracy 0.97\n",
      "step 62250, train loss 23.154436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 62500, train accuracy 0.98\n",
      "step 62500, train loss 17.834454\n",
      "step 62750, train accuracy 0.985\n",
      "step 62750, train loss 19.469364\n",
      "step 63000, train accuracy 0.99\n",
      "step 63000, train loss 13.726264\n",
      "step 63250, train accuracy 0.985\n",
      "step 63250, train loss 30.752413\n",
      "step 63500, train accuracy 0.985\n",
      "step 63500, train loss 29.016226\n",
      "step 63750, train accuracy 0.985\n",
      "step 63750, train loss 17.810101\n",
      "step 64000, train accuracy 0.99\n",
      "step 64000, train loss 18.153650\n",
      "step 64250, train accuracy 0.97\n",
      "step 64250, train loss 19.629835\n",
      "step 64500, train accuracy 0.985\n",
      "step 64500, train loss 23.171654\n",
      "step 64750, train accuracy 0.99\n",
      "step 64750, train loss 12.786263\n",
      "step 65000, train accuracy 0.99\n",
      "step 65000, train loss 13.052101\n",
      "step 65250, train accuracy 0.995\n",
      "step 65250, train loss 14.060141\n",
      "step 65500, train accuracy 0.985\n",
      "step 65500, train loss 28.558401\n",
      "step 65750, train accuracy 0.985\n",
      "step 65750, train loss 18.265156\n",
      "step 66000, train accuracy 0.975\n",
      "step 66000, train loss 24.421934\n",
      "step 66250, train accuracy 0.985\n",
      "step 66250, train loss 27.938122\n",
      "step 66500, train accuracy 0.99\n",
      "step 66500, train loss 16.418962\n",
      "step 66750, train accuracy 0.99\n",
      "step 66750, train loss 11.002378\n",
      "step 67000, train accuracy 0.985\n",
      "step 67000, train loss 15.439715\n",
      "step 67250, train accuracy 0.985\n",
      "step 67250, train loss 12.681532\n",
      "step 67500, train accuracy 0.985\n",
      "step 67500, train loss 11.526389\n",
      "step 67750, train accuracy 0.99\n",
      "step 67750, train loss 11.612343\n",
      "step 68000, train accuracy 0.975\n",
      "step 68000, train loss 17.595818\n",
      "step 68250, train accuracy 0.995\n",
      "step 68250, train loss 10.190494\n",
      "step 68500, train accuracy 0.985\n",
      "step 68500, train loss 13.916505\n",
      "step 68750, train accuracy 0.99\n",
      "step 68750, train loss 25.337795\n",
      "step 69000, train accuracy 0.99\n",
      "step 69000, train loss 13.491289\n",
      "step 69250, train accuracy 0.99\n",
      "step 69250, train loss 10.126125\n",
      "step 69500, train accuracy 0.99\n",
      "step 69500, train loss 12.464053\n",
      "step 69750, train accuracy 0.99\n",
      "step 69750, train loss 12.760547\n",
      "step 70000, train accuracy 0.985\n",
      "step 70000, train loss 13.724017\n",
      "step 70250, train accuracy 1\n",
      "step 70250, train loss 12.222582\n",
      "step 70500, train accuracy 1\n",
      "step 70500, train loss 6.598807\n",
      "step 70750, train accuracy 1\n",
      "step 70750, train loss 6.941884\n",
      "step 71000, train accuracy 1\n",
      "step 71000, train loss 7.305584\n",
      "step 71250, train accuracy 0.995\n",
      "step 71250, train loss 9.564557\n",
      "step 71500, train accuracy 1\n",
      "step 71500, train loss 8.472621\n",
      "step 71750, train accuracy 1\n",
      "step 71750, train loss 9.701490\n",
      "step 72000, train accuracy 0.995\n",
      "step 72000, train loss 8.948296\n",
      "step 72250, train accuracy 1\n",
      "step 72250, train loss 8.397684\n",
      "step 72500, train accuracy 1\n",
      "step 72500, train loss 7.662931\n",
      "Final:step 72626, train loss 3.942866\n",
      "Final accuracy 1\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/data.npy'\n",
    "\n",
    "\n",
    "data = np.load(filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "\n",
    "batch_size = 200\n",
    "for i in range(100000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%250 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "        print('step %d, train loss %f'%(i,loss_temp))\n",
    "    if loss_temp < 4:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print('Final:step %d, train loss %f'%(i,loss_temp))\n",
    "        print (\"Final accuracy %g\" %train_accuracy)\n",
    "        break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAE/CAYAAAC5EpGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VeW59/HvnYEQwgwxMoRJQQUVFEScZ0Ftqz2dsKdW\ne6rYt7RvPbW20kFrj57a9lRPrdZWq9X2daLVVpxFnHAADLPMMyFMIQwBAiHD/f6xV+IGghnJZq31\n+1zXvrL2s4Z9P5Xml/WsZ61t7o6IiIiES1qqCxAREZHGU4CLiIiEkAJcREQkhBTgIiIiIaQAFxER\nCSEFuIiISAgpwEVEREJIAS4SE2a22swuTnUdItIyFOAiIiIhpAAXiTkzu8HMlpvZVjObZGY9g3Yz\ns3vNbLOZlZrZfDM7MVh3uZktNLOdZlZkZj9IbS9E4kcBLhJjZnYh8Evgy0APYA3wdLD6UuBcYBDQ\nKdimJFj3CHCju3cATgTebMWyRQTISHUBIpJS/w486u6zAMxsArDNzPoBFUAH4HhghrsvStqvAhhs\nZnPdfRuwrVWrFhGdgYvEXE8SZ90AuPsuEmfZvdz9TeB+4AFgs5k9ZGYdg02/AFwOrDGzd8zsjFau\nWyT2FOAi8bYe6FvzxsxygG5AEYC73+fuw4HBJIbSbwnaP3L3K4GjgH8BE1u5bpHYU4CLxEummbWt\neQFPAd8ws2FmlgX8NzDd3Veb2WlmdrqZZQK7gb1AtZm1MbN/N7NO7l4BlALVKeuRSEwpwEXi5WVg\nT9LrfOBnwLPABuAYYGywbUfgYRLXt9eQGFr/TbDuGmC1mZUC3yJxLV1EWpG5e6prEBERkUbSGbiI\niEgIKcBFRERCSAEuIiISQgpwERGREFKAi4iIhNAR/yjV7t27e79+/VJdhoiISKuYOXPmFnfPrW+7\nIz7A+/XrR0FBQarLEBERaRVmtqb+rTSELiIiEkoKcBERkRBSgIuIiISQAlxERCSEFOAiIiIhpAAX\nEREJIQW4iIhICCnARUREQkgBLiIiEkKxCvDCrWWsLN6V6jJERESa7Yh/lGpLOufXbwGw+u4rUlyJ\niIhI88TqDFxERCQqFOAiIiIhpAAXEREJIQW4iIhICCnARUREQkgBLiIiEkIKcBERkRBSgIuIiISQ\nAlxERCSEFOAiIiIhpAAXEREJIQW4iIhICCnARUREQkgBLiIiEkL1BriZ5ZvZW2a20MwWmNn3gvaf\nm1mRmc0JXpcn7TPBzJab2RIzG53UPtzM5gfr7jMzOzzdEhERibaGfB94JXCzu88ysw7ATDObHKy7\n193/J3ljMxsMjAWGAD2BN8xskLtXAQ8CNwDTgZeBMcArLdMVERGR+Kj3DNzdN7j7rGB5J7AI6PUp\nu1wJPO3u5e6+ClgOjDSzHkBHd5/m7g78Fbiq2T0QERGJoUZdAzezfsApJM6gAb5rZvPM7FEz6xK0\n9QIKk3ZbF7T1CpYPbBcREZFGanCAm1l74FngJncvJTEcPgAYBmwAfttSRZnZODMrMLOC4uLiljqs\niIhIZDQowM0sk0R4P+HuzwG4+yZ3r3L3auBhYGSweRGQn7R776CtKFg+sP0g7v6Qu49w9xG5ubmN\n6Y+IiEgsNGQWugGPAIvc/Z6k9h5Jm30e+DhYngSMNbMsM+sPDARmuPsGoNTMRgXH/DrwfAv1Q0RE\nJFYaMgv9LOAaYL6ZzQnafgxcbWbDAAdWAzcCuPsCM5sILCQxg318MAMd4NvAY0A2idnnmoEuIiLS\nBPUGuLu/B9R1v/bLn7LPXcBddbQXACc2pkARERE5mJ7EJiIiEkINGUKPjAuOy6Vk975UlyEiItJs\nsTsDd091BSIiIs0XqwDXo9dFRCQqYhXgIiIiURG7AHc0hi4iIuEXqwDXALqIiERFrAJcREQkKmIX\n4JqFLiIiURCrANckdBERiYpYBbiIiEhUxC7ANYQuIiJRELMA1xi6iIhEQ8wCXEREJBpiF+AaQRcR\nkSiIVYBrFrqIiERFrAJcREQkKmIX4K5p6CIiEgGxCnCNoIuISFTEKsBFRESiQgEuIiISQrEKcM1C\nFxGRqIhVgIuIiERF7AJck9BFRCQKYhXgpnnoIiISEbEKcBERkaiIXYC7noYuIiIREKsA1yx0ERGJ\nilgFuIiISFTELsA1C11ERKIgVgGuIXQREYmKWAW4iIhIVMQuwDWCLiIiURCrANeDXEREJCpiFeAi\nIiJREbsAd01DFxGRCIhXgGsEXUREIqLeADezfDN7y8wWmtkCM/te0N7VzCab2bLgZ5ekfSaY2XIz\nW2Jmo5Pah5vZ/GDdfWa6sUtERKQpGnIGXgnc7O6DgVHAeDMbDNwKTHH3gcCU4D3BurHAEGAM8Acz\nSw+O9SBwAzAweI1pwb40iAbQRUQkCuoNcHff4O6zguWdwCKgF3Al8Hiw2ePAVcHylcDT7l7u7quA\n5cBIM+sBdHT3aZ64EP3XpH1ahU73RUQkKhp1DdzM+gGnANOBPHffEKzaCOQFy72AwqTd1gVtvYLl\nA9tFRESkkRoc4GbWHngWuMndS5PXBWfULTY6bWbjzKzAzAqKi4tb6rAJGkMXEZEIaFCAm1kmifB+\nwt2fC5o3BcPiBD83B+1FQH7S7r2DtqJg+cD2g7j7Q+4+wt1H5ObmNrQvDelHix1LREQklRoyC92A\nR4BF7n5P0qpJwLXB8rXA80ntY80sy8z6k5isNiMYbi81s1HBMb+etI+IiIg0QkYDtjkLuAaYb2Zz\ngrYfA3cDE83sm8Aa4MsA7r7AzCYCC0nMYB/v7lXBft8GHgOygVeCV6vSCLqIiERBvQHu7u9x6Anc\nFx1in7uAu+poLwBObEyBLUkD6CIiEhXxehKbiIhIRMQuwPUsdBERiYJYBbgmoYuISFTEKsBFRESi\nInYBrgF0ERGJglgFuEbQRUQkKmIV4CIiIlERuwDXJHQREYmCWAW4noUuIiJREasAFxERiYrYBbhr\nHrqIiERArAJcA+giIhIVsQpwERGRqIhdgGsWuoiIREG8Alxj6CIiEhHxCnAREZGIiF2AawhdRESi\nIFYBbhpDFxGRiIhVgIuIiESFAlxERCSEYhXgehS6iIhERawCXEREJCpiF+CuaegiIhIBsQpwjaCL\niEhUxCrARUREoiJ2Aa4BdBERiYJYBbhmoYuISFRkpLqA1jR54Sa2lVWkugwREZFmi9UZuMJbRESi\nIlYBLiIiEhUKcBERkRBSgIuIiISQAlxERCSEFOAiIiIhpAAXEREJIQW4iIhICCnARUREQqjeADez\nR81ss5l9nNT2czMrMrM5wevypHUTzGy5mS0xs9FJ7cPNbH6w7j4zPdhURESkqRpyBv4YMKaO9nvd\nfVjwehnAzAYDY4EhwT5/MLP0YPsHgRuAgcGrrmOKiIhIA9Qb4O7+LrC1gce7Enja3cvdfRWwHBhp\nZj2Aju4+zd0d+CtwVVOLFhERibvmXAP/rpnNC4bYuwRtvYDCpG3WBW29guUD20VERKQJmhrgDwID\ngGHABuC3LVYRYGbjzKzAzAqKi4tb8tAiIiKR0KQAd/dN7l7l7tXAw8DIYFURkJ+0ae+grShYPrD9\nUMd/yN1HuPuI3NzcppQoIiISaU0K8OCado3PAzUz1CcBY80sy8z6k5isNsPdNwClZjYqmH3+deD5\nZtQtIiISaxn1bWBmTwHnA93NbB1wO3C+mQ0DHFgN3Ajg7gvMbCKwEKgExrt7VXCob5OY0Z4NvBK8\nREREpAnqDXB3v7qO5kc+Zfu7gLvqaC8ATmxUdSIiIlInPYlNREQkhBTgIiIiIaQAFxERCSEFuIiI\nSAjFKsCH5XdOdQkiIiItIlYBft4gPRRGRESiIVYBXiPxfSoiIiLhFasA1zeQi4hIVMQqwGvoBFxE\nRMIuVgFu6BRcRESiIVYBXkMn4CIiEnaxCnBdAxcRkaiIVYDX0Cx0EREJu1gFuE7ARUQkKmIV4DV0\n/i0iImEXqwDXNXAREYmKWAX4zvJKQPeBi4hI+MUqwP/0zkoA3l++JcWViIiINE+sArzGrLXbUl2C\niIhIs8QywPdVVae6BBERkWaJZYCLiIiEXTwDXJPYREQk5GIZ4MpvEREJu3gGuO4jExGRkItpgKe6\nAhERkeaJZYCLiIiEXSwDfPueilSXICIi0iyxDPANO/akugQREZFmiWWAm75YVEREQi6WAS4iIhJ2\nsQzwtDSdgYuISLjFMsCPy2uf6hJERESaJZYBbqYzcBERCbdYBnhex7apLkFERKRZYhngyzbtTHUJ\nIiIizRLLAH/6o8JUlyAiItIssQxwERGRsKs3wM3sUTPbbGYfJ7V1NbPJZrYs+Nklad0EM1tuZkvM\nbHRS+3Azmx+su880k0xERKTJGnIG/hgw5oC2W4Ep7j4QmBK8x8wGA2OBIcE+fzCz9GCfB4EbgIHB\n68BjioiISAPVG+Du/i6w9YDmK4HHg+XHgauS2p9293J3XwUsB0aaWQ+go7tP88SXcf81aR8RERFp\npKZeA89z9w3B8kYgL1juBSTPEFsXtPUKlg9sr5OZjTOzAjMrKC4ubmKJIiIi0dXsSWzBGbW3QC3J\nx3zI3Ue4+4jc3NyWPLSIiEgkNDXANwXD4gQ/NwftRUB+0na9g7aiYPnAdhEREWmCpgb4JODaYPla\n4Pmk9rFmlmVm/UlMVpsRDLeXmtmoYPb515P2ERERkUbKqG8DM3sKOB/obmbrgNuBu4GJZvZNYA3w\nZQB3X2BmE4GFQCUw3t2rgkN9m8SM9mzgleAlIiIiTVBvgLv71YdYddEhtr8LuKuO9gLgxEZVJyIi\nInXSk9hERERCSAEuIiISQgpwERGREFKAi4iIhJACXEREJIQU4CIiIiGkABcREQkhBbiIiEgIKcBF\nRERCSAEuIiISQgpwERGREIpVgH9lRH79G4mIiIRArAK8T7d2qS5BRESkRcQqwC8/qUft8r7K6hRW\nIiIi0jyxCvDszPTa5b+8vyqFlYiIiDRPrAK8fdtPvv78gxUlKaxERESkeWIV4MneWVqc6hJERESa\nLFYB3jYjVt0VEZEIi1WiZaTHqrsiIhJhSjQREZEQUoCLiIiEkAJcREQkhBTgIiIiIaQAFxERCSEF\nuIiISAjFOsD1PHQREQmrWAf4rc/NS3UJIiIiTRLrAH9uVlGqSxAREWmSWAe4iIhIWMU+wKurPdUl\niIiINFrsA3xe0Y5UlyAiItJosQ/wom17Ul2CiIhIo8U+wH/0rGaii4hI+MQ+wHeVV6a6BBERkUaL\nfYCLiIiEkQIcGP/kLCqq9FQ2EREJj2YFuJmtNrP5ZjbHzAqCtq5mNtnMlgU/uyRtP8HMlpvZEjMb\n3dzim2Jo704Htb00bwMzVm1NQTUiIiJN0xJn4Be4+zB3HxG8vxWY4u4DgSnBe8xsMDAWGAKMAf5g\nZukt8PmN0rdbTp3ta7eW0e/Wl3hz8aZWrkhERKTxDscQ+pXA48Hy48BVSe1Pu3u5u68ClgMjD8Pn\nf6r8rtl1ts9euw3Q41VFRCQcmhvgDrxhZjPNbFzQlufuG4LljUBesNwLKEzad13Q1qquGdWvzvbp\nwRD6i/M21LleRETkSJLRzP3PdvciMzsKmGxmi5NXurubWaOfVRr8MTAOoE+fPs0scX+d22XW2b6m\npKxFP0dERORwatYZuLsXBT83A/8kMSS+ycx6AAQ/NwebFwH5Sbv3DtrqOu5D7j7C3Ufk5uY2p8SD\ntM1s9cvuIiIiLa7JAW5mOWbWoWYZuBT4GJgEXBtsdi3wfLA8CRhrZllm1h8YCMxo6ue3hklz17Op\ndG+qyxARETlIc87A84D3zGwuiSB+yd1fBe4GLjGzZcDFwXvcfQEwEVgIvAqMd/eq5hTfVGce0+1T\n1/+9oJCyfZX836dmc/XD01qpKhERkYZr8jVwd18JDK2jvQS46BD73AXc1dTPbCl3XnUiF/72nUOu\nv+Uf89i6ex8AG3foDFxERI48sXwSW5d2berd5pevJObjle2rYkdZxX7rtuwqPyx1iYiINFQ8Azyn\n/gBPNvQXr3N3EOgvz9/AiDvfYPrKksNRmoiISIPEMsCb4o/vrKC62pk0Zz0AC9aXprgiERGJs9gG\n+G+/dNDl+3oN+PHLvLpgIwBmLV2RiIhIw8U2wHOymvsMG5i/bgdrSna3QDUiIiKN0/wUC6nzBjXv\nATF3vLCwdvnOq07kp//6mMX/NUYPihERkVYR2zPw7DYtF7Q//dfHAEwsKKxnSxERkZYR2wA/HG57\nfsGnrl9TspsH3lreStWIiEiUxTrAR/Tt0uLHPOn21+h360tsL9vHV/70IQvXlzJr7TZ2lVdyzSMz\n+M1rS9i8Uw+HERGR5ontNXD4ZCb5Z07uwb7Kal5fuKnZx9xZXgnAsF9MBuDy+6bWrsvtkJVYaPT3\ns4mIiOwv1mfgw/I7A/CflwzijiuHHPbPK96ZeILb2q2Jry7dW5GSR8GLiEgExPoM/IdjjueqU3px\nTG77Vv3cL/7xwzrbT+nTmds/O4Rh+Z1ZsH4Hg/I6YEB6mmG68VxERJLE+gw8Mz2NIT071b7//dWn\npLAamL12O1c98D7LN+/kivve40fPzuPYn7xC/wkvM/red5mxaisfrd7KPZOX1p6979xbQWFwRl9j\nd3kln/n9VBas35GKboiISCsw9yP7guyIESO8oKCgVT5r8cZSxvzv1Po3PEJ0y2lDSfCtaUPzO/Ps\nt87gvinLGJjXge8+NZvjj+7Av8afpXvTRURCxMxmuvuIerdTgO9vwfodHH90R664byqLN+5stc89\nXPI6ZjG4R0e+dd4xtG+bQa/O2aSnGR3aZvL8nCLyu7bj1D4Nn43/5uJN3DN5Kc+PP5v0NA3ri4i0\ntIYGeKyvgdelZkj91ZvOBeCtxZv5xmMfpbKkZtlUWs6m0mLeWlK8X/u9XxnKfz4zF4Anrz+d91ds\nYf32vVx8Qh6791Xyw3/MY85tl9A5+OrV95ZtIc3g+xPnsr2sgtI9FY3+VjcREWk5OgNvgJJd5XRr\nn7gFrN+tL6W0ltb0xPWnc9ax3YFP+t0+K4Nd5ZVMufk8sjLS6N2lXSpLFBGJHA2hHyYVVdUUbi1j\nQG57bvn7XP4+c12qSzrshuZ3Zm7h9jrXzb3tUkr3VvD20mIuPuEoHnt/Nd+7eCBpZrr2LiLSBArw\nVlK4tYxfv7aEAd1z+N2UZaku54jy7i0XsKu8ksvvm8rEG89gZP+uB22zZONOenXJpn0Dvx1u2aad\nPDljLTddPIg7Ji3g51cOoWPbzJYuXUQkZRTgKbCrvJJ2memYwdA7Xqd0b2WqSzpizfzpxQy/843a\n90N7d2JPRRX//fmT6NO1HRt2JB43W15ZzTG5OazfvpeTenfinF+/SeHWPfzbKb14bnYRN108kJsu\nHtRidW3bvY9FG0sZ2rsz6WkaRRCR1qcAP4KU7CpnRfFuxv2tgO1lFakuJ7TevPk8LvztO8An1+KP\nP7pD7YTDGtXVzqade+nRKbtBx91VXsmJt7/Gb754Mn+btoZ56xL3zx/dsS3TfnxRy3ZCRKQeCvAj\nUNm+Sv724Rr+7dTenHbXG/Ts1JbBPTvyxqLNqS4t1Lq3b8OWXfv46RUnsL2sgvuDb3z7zgXH8oPR\nx1Fd7cwu3M6HK7Zw2Uk9mDRnPdec0RcDsjLT+eesdfzs+QUck5tD4bY97Kusrj326ruvSFGvRCSu\nFOAhsnX3PrIy0sjJyqBwaxl5HdvSJiONeeu288S0tTxTUMjlJx3Ny/M3prrU2Jl44xlUu9O5XSbt\nszL43RvL2LBjL/d/9RR+89oS1m4tI7dDFvd8edh++01bWcLAo9rX3r3QKrV+VEh2m3Q+O7Rnq32m\niLQ8BXgEvTB3PT06tT3ks9QldV74ztl0a9+G2yctYHLwrXbH5Obw+n+ex69eXcw3z+5PXse2uDsf\nrihhXtEOxp0zgLSkh+GsLSmjrKKS44/uWOdnzFyzjYUbSrlmVN8619fc6qdRA5Fw04NcIqjmzGre\nzy/llfkbOHdQLnv2VbFzbyVXPvA+t31mMNed2Y+VW3bRrk0GZ979Zoorjo/P3v/eQW0rinfzg7/P\n5Z+zi3jo3ZW0a5NOj05tWVG8G4AB3XO4ZHAe33lqNuu2ljE3uPaeHMDrtpVRVe307ZbDFx78AOCQ\nAV6jdG+FZuaLxIDOwCNse9k+Fm4o5cxjuvPivPVsL6sgOzOdLwzvzZKNO7nmkelsDr7iVI4cY0/L\n5+mPCvnq6X14cvrag9afM7A7j1x7Gm0y9v8uopoz8H7d2vH2LRdQUVVNmhlz122nXZv02jP7t5Zs\npltOG07u3bn2S3E0217kyKEhdKmXu/PMR4V8dmhPlm7aSbXDKfmd2VtZxXWPfkRZRSVLN+3i2W+d\nyfRVJdz50qJUlyx1OKFHR8YMOZp731ha27bkzjEc99NXOfOYbnywoqS2PfkRumcd240Pg3Urf/nJ\nWb97YtJf8jPy311azMNTV/L4N0buN+xfl+pq5943lvK1UX3J69i2RfooEicKcGlxby3eTHllFeu2\n7eHOlxbx1A2jOKl3J575qJD/enFhqsuTZph44xlc/fA0vnhqb47qmMXv30zM5L9l9HGMv+DY2rP7\nV753Dif0SJzJuzvj/jaT44/uwM2XHkdFVTUri3dzxwsL+GBFCaf378ozN56Rsj6JhJUCXA4bd2fR\nhp0M7ln3ZCtI3PteuG0Pw/I74+6YGVt372PX3kp6d8nmudlFFG4t09PrQujSwXm8HkzUq3F0x7Zs\nLN170Lb/+NYZjOjXlb0VVRz/s1f5ny8N5YvDe9d53FlrtzFrzTauP2fAYam7LtXVzq9fW8J1Z/bj\n6E4aLZAjgwJcQqOq2qmq9tprulXVzr2Tl1LtziWD83h7STFjR+bzzpJi1m3bU3ufd40nrz+dr/55\neipKlwbKykijPLi/fv7PL6VD20y2l+1j2C8mH7Rt8iS+iqpqKqqqaZuRzuqS3fTqkk1WRjorinfR\nr1vOfl9pu2dfFU9MX8N/nNV/v2H+DTv20L19Fpnp+88ZAJi5ZitfePBDRg3oytPjNFogRwYFuMTO\njj0VDL3jdQBGD8njo9XbuHpkPtmZ6XRq14YX5q5nxqqtjB6Sx2sLNtVzNDnS3XjuAPZVVfOX91cD\n0CY9jX1V1Vxxcg8e+OqpQOLhSYNvew2A2T+7hOw26Tz49gquP6c/1Q6PvreK301Zxil9OvPPb58F\nwPNzilhTUsb/vWggO/dWsLeimtwOn9zPv2NP4ut087vqm/jk8FCASyxVVFVjQEYdZ1vJpi4r5vdv\nLuev/zGStpnpfPXhaXywooQ3bz6PTaXlZKQbXwrut+/ePotT+3Q+aNhYjnz9u+ewasvug9oz043s\nzPT9vq/g+5cM4p7Jn0wEnHjjGXz5T4l/A6vvvoIX562nW04WP3p2Hmu3lvHHrw3nnIHd2b6ngl6d\nE4/tvXniXGasLmHqDy+sPU5VtSdGETLTeeajtZzUq/OnXn4SUYCLtLAZq7aydNNOvpZ0H3ZFVTW/\ne2MZJ/fuRL/uORyb2x4z2FRazqhfTqFj2wzOGZTLS/M2pLByOdzyu2Zz/9WncuUD79e2TbjseDpl\nZ/Lc7CJmrNrKdy88tnZy4N++OZJzBuZSXe088NZyOmZncvqAruwoq+CBt1fw+DdOw8zYW1FFZnra\nfpcKDrSmZDd9u+XUvn9v2RZystI5JekugvnBMwZO6t2ppbveYlZt2c3MNdsOOUciThTgIkeoD1eU\nULR9T+0vqnXbyjj7V2/x1A2jOOOYbgB8sGILbdLTeHfZFu6bsozzj8vl7SXFqSxbWtl7P7qAs3/1\nFgCfG9qT8Rccy5qS3Qzu2ZEF60tJM6Oyqpr/88QsHrl2BBedkAfs/0S+P09dCVB7C+jc2y/lf99Y\nyq2XHU9WRuPv/d+wYw9HdWj7qX9QTCwoJCsjjSuH9ar3ePPX7SAnK50Bue0Zctur7N5XpScJogAX\nibynZqylV+dszh2US9m+Suat28GqLbsZNaAb67fvYVPpXi46Po+hv3h9v/3OGdidqcu2pKhqaS2L\nfjGGE257db+2687sx2MfrObLI3qzfPMuZq3dXrvuqRtGkZOVzufuT4wi3PuVoVw1rBcPvrOC1z7e\nSM/O2bzy8UaG9+3CkzeczrtLt3DDXwv4y3WnccHxR7FoQymX/W5q7fFqgri6OpExBWu20a9bO44K\nng1QXe0M+PHLQOIPlElz1wOJSxcvzVvP4x+uqb2NEaB4ZzlfePAD7v7CSTw5fS3/86Whh3wAUene\nCh5/fzXjLzj2oOcWPDl9LUN6dmRofuc69y3ZVU52m3TatUk8qHRu4Xb2VFQxakC3+v4nbzEKcBHZ\nT0VVNelmtb/Q3l6ymSE9E0OqNz0zm02l5fz3509ieN8upBmYGb+fsoxnCgpZt21P7XFu/+xg7nhB\n9/3Lp/v+JYNYtnkXLwTBXOPcQbm8u7Tho0mjh+QxKK8Da0rKakMe4IZz+nPdWf2ZurSYs47tTpuM\nNPZVVtO5XSbXPDKDOYXb+ePXhjPmxKNZUbyL/C7tMIOBP3kFgGkTLiKvYxZm+wd8v1tfon/3HJ4e\nN4r2WRkMuT0xCbI1RwYU4CLSYnaXVzJl8WY+e3KPg37h3f78x/TplsM3z+4PJP4w2F1exSWD82iT\nkUbJrnI2lu5l8YadPDF9Te1Z3/FHd2Dxxp0MyM3h1D5d+MfMda3eL5HGuOGc/jw8ddVB7S0d7kds\ngJvZGOB3QDrwZ3e/+9O2V4CLxM/qLbvp3SWbjPQ0dpRV0L5tBv+cXcSQnh059qj2ZKan8dK8DYx/\nchZ//9YZfPXhaVRUJX6XDcprz9JNu1LcA4mTd245f7+JhM11RAa4maUDS4FLgHXAR8DV7n7I8TgF\nuIjUxz3xMKD6bh/cs6+KXeWV7KuqplN24jvea9rT0iArI51pK0tYWbybFcW7eHHeeob07MRdnz+R\np2YU8oe3llNZfWSPWkpqtORZ+JEa4GcAP3f30cH7CQDu/stD7aMAF5Ew2Bc8ae7Ab4mry449FWRl\npNVOwnojezg4AAAGbElEQVR+ThFzCrcz9rQ+9O6Szdbd+0hPM15fsJGi7Xu4clgv7nppEcP7djno\nSYRyZEhFgLf294H3AgqT3q8DTm/lGkREWlxDgrtGp+z9v6/9ymG99rvtKicYGbjurP61bU+NGwXA\nD0Yf16T6SnYlvjq4W/usOtfvLq+k2p1qh+zMdF6av54Lj8sDSzxytryimuF9uzDhufkMze/MgNwc\npi7dwjMFiV/pV4/MZ+qyLfTuks20lVubVKM0TmufgX8RGOPu1wfvrwFOd/fvHLDdOGAcQJ8+fYav\nWbOm1WoUERGpUVmVGFnJSE+jJi+rPXFPfG6HLJZt2kW39m3o0Sm7xT6zoWfgDf+TsWUUAflJ73sH\nbftx94fcfYS7j8jNzW214kRERJJlpKfVzq0wM8yM9DSjd5d2ZGWkc2KvTi0a3o3R2gH+ETDQzPqb\nWRtgLDCplWsQEREJvVa9Bu7ulWb2HeA1EreRPeruC1qzBhERkSho7UlsuPvLwMut/bkiIiJR0tpD\n6CIiItICFOAiIiIhpAAXEREJIQW4iIhICCnARUREQkgBLiIiEkIKcBERkRBq9e8DbywzKwZa8mHo\n3YEtLXi8I436F27qX3hFuW+g/rWmvu5e73PEj/gAb2lmVtCQh8SHlfoXbupfeEW5b6D+HYk0hC4i\nIhJCCnAREZEQimOAP5TqAg4z9S/c1L/winLfQP074sTuGriIiEgUxPEMXEREJPRiE+BmNsbMlpjZ\ncjO7NdX1fBoze9TMNpvZx0ltXc1sspktC352SVo3IejXEjMbndQ+3MzmB+vuMzML2rPM7JmgfbqZ\n9Wvl/uWb2VtmttDMFpjZ96LSRzNra2YzzGxu0Lc7otK3A/qZbmazzezFqPXPzFYHdc0xs4II9q+z\nmf3DzBab2SIzOyMq/TOz44L/bjWvUjO7KSr9O4i7R/4FpAMrgAFAG2AuMDjVdX1KvecCpwIfJ7X9\nGrg1WL4V+FWwPDjoTxbQP+hnerBuBjAKMOAV4LKg/dvAH4PlscAzrdy/HsCpwXIHYGnQj9D3Maij\nfbCcCUwP6gt93w7o5/eBJ4EXI/jvczXQ/YC2KPXvceD6YLkN0DlK/UvqZzqwEegbxf65e2wC/Azg\ntaT3E4AJqa6rnpr7sX+ALwF6BMs9gCV19QV4LehvD2BxUvvVwJ+StwmWM0g8vMBS2NfngUui1keg\nHTALOD1KfQN6A1OAC/kkwKPUv9UcHOCR6B/QCVh14OdFpX8H9OlS4P2o9s/dYzOE3gsoTHq/LmgL\nkzx33xAsbwTyguVD9a1XsHxg+377uHslsAPodnjK/nTB8NMpJM5UI9HHYHh5DrAZmOzukelb4H+B\nHwLVSW1R6p8Db5jZTDMbF7RFpX/9gWLgL8ElkD+bWQ7R6V+yscBTwXIU+xebAI8UT/zpF/rbB8ys\nPfAscJO7lyavC3Mf3b3K3YeROFMdaWYnHrA+tH0zs88Am9195qG2CXP/AmcH//0uA8ab2bnJK0Pe\nvwwSl+cedPdTgN0khpRrhbx/AJhZG+BzwN8PXBeF/tWIS4AXAflJ73sHbWGyycx6AAQ/Nwfth+pb\nUbB8YPt++5hZBolhtZLDVnkdzCyTRHg/4e7PBc2R6qO7bwfeAsYQnb6dBXzOzFYDTwMXmtn/Izr9\nw92Lgp+bgX8CI4lO/9YB64JRIYB/kAj0qPSvxmXALHffFLyPWv+A+AT4R8BAM+sf/GU2FpiU4poa\naxJwbbB8LYnrxjXtY4OZkf2BgcCMYLio1MxGBbMnv37APjXH+iLwZvBXaasI6nkEWOTu9yStCn0f\nzSzXzDoHy9kkru0vJgJ9A3D3Ce7e2937kfj/0Zvu/jUi0j8zyzGzDjXLJK6jfkxE+ufuG4FCMzsu\naLoIWEhE+pfkaj4ZPj+wpij0LyEVF95T8QIuJzHbeQXwk1TXU0+tTwEbgAoSfzF/k8Q1linAMuAN\noGvS9j8J+rWEYKZk0D6CxC+fFcD9fPLgnrYkhpaWk5hpOaCV+3c2iSGsecCc4HV5FPoInAzMDvr2\nMXBb0B76vtXR1/P5ZBJbJPpH4k6VucFrQc3viqj0L/j8YUBB8G/0X0CXiPUvh8QZcaektsj0L/ml\nJ7GJiIiEUFyG0EVERCJFAS4iIhJCCnAREZEQUoCLiIiEkAJcREQkhBTgIiIiIaQAFxERCSEFuIiI\nSAj9f7cVvHcUUXS/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ee26f15320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('third_100k_batch_balance_50k_train_Adam.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自制验证集，修改训练loss的阈值，增加模型评估策略，利用综合指标评判模型优劣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (50989, 18449)\n",
      "step 0, train accuracy 0.0422, train loss 3390.677002\n",
      "step 200, train accuracy 0.3558, train loss 982.563293\n",
      "step 400, train accuracy 0.3815, train loss 867.350342\n",
      "step 600, train accuracy 0.396, train loss 696.924622\n",
      "step 800, train accuracy 0.4006, train loss 710.063599\n",
      "step 1000, train accuracy 0.4108, train loss 659.363525\n",
      "step 1200, train accuracy 0.4151, train loss 576.595093\n",
      "step 1400, train accuracy 0.4363, train loss 600.156067\n",
      "step 1600, train accuracy 0.4211, train loss 525.058594\n",
      "step 1800, train accuracy 0.439, train loss 500.896179\n",
      "step 2000, train accuracy 0.4482, train loss 544.752563\n",
      "step 2200, train accuracy 0.4521, train loss 500.953247\n",
      "step 2400, train accuracy 0.4671, train loss 480.814453\n",
      "step 2600, train accuracy 0.4602, train loss 439.741852\n",
      "step 2800, train accuracy 0.4787, train loss 434.308746\n",
      "step 3000, train accuracy 0.4546, train loss 460.352905\n",
      "step 3200, train accuracy 0.4678, train loss 427.282257\n",
      "step 3400, train accuracy 0.4747, train loss 475.915833\n",
      "step 3600, train accuracy 0.487, train loss 440.609192\n",
      "step 3800, train accuracy 0.4927, train loss 431.173035\n",
      "step 4000, train accuracy 0.483, train loss 464.591095\n",
      "step 4200, train accuracy 0.478, train loss 409.760925\n",
      "step 4400, train accuracy 0.4801, train loss 380.037354\n",
      "step 4600, train accuracy 0.4819, train loss 407.551941\n",
      "step 4800, train accuracy 0.4968, train loss 398.438843\n",
      "step 5000, train accuracy 0.4894, train loss 394.148346\n",
      "step 5200, train accuracy 0.5058, train loss 410.121277\n",
      "step 5400, train accuracy 0.4981, train loss 388.933685\n",
      "step 5600, train accuracy 0.5058, train loss 369.152161\n",
      "step 5800, train accuracy 0.4938, train loss 425.229431\n",
      "step 6000, train accuracy 0.5098, train loss 393.535034\n",
      "step 6200, train accuracy 0.5112, train loss 349.663086\n",
      "step 6400, train accuracy 0.5076, train loss 368.957458\n",
      "step 6600, train accuracy 0.5006, train loss 337.060699\n",
      "step 6800, train accuracy 0.5225, train loss 376.215942\n",
      "step 7000, train accuracy 0.5148, train loss 385.582520\n",
      "step 7200, train accuracy 0.5115, train loss 390.403625\n",
      "step 7400, train accuracy 0.5179, train loss 346.966888\n",
      "step 7600, train accuracy 0.5233, train loss 337.625793\n",
      "step 7800, train accuracy 0.5132, train loss 379.019470\n",
      "step 8000, train accuracy 0.519, train loss 311.561310\n",
      "step 8200, train accuracy 0.5264, train loss 344.026855\n",
      "step 8400, train accuracy 0.5269, train loss 320.261627\n",
      "step 8600, train accuracy 0.53, train loss 350.989624\n",
      "step 8800, train accuracy 0.5332, train loss 339.583496\n",
      "step 9000, train accuracy 0.5416, train loss 318.861023\n",
      "step 9200, train accuracy 0.5428, train loss 323.764343\n",
      "step 9400, train accuracy 0.5368, train loss 290.490967\n",
      "step 9600, train accuracy 0.5432, train loss 310.003235\n",
      "step 9800, train accuracy 0.5398, train loss 307.518097\n",
      "step 10000, train accuracy 0.5349, train loss 301.111389\n",
      "step 10200, train accuracy 0.5433, train loss 316.936157\n",
      "step 10400, train accuracy 0.5557, train loss 312.880005\n",
      "step 10600, train accuracy 0.5417, train loss 259.960266\n",
      "step 10800, train accuracy 0.5473, train loss 295.551727\n",
      "step 11000, train accuracy 0.5613, train loss 284.180573\n",
      "step 11200, train accuracy 0.5249, train loss 295.635834\n",
      "step 11400, train accuracy 0.5413, train loss 353.270203\n",
      "step 11600, train accuracy 0.5525, train loss 307.200317\n",
      "step 11800, train accuracy 0.5375, train loss 262.012939\n",
      "step 12000, train accuracy 0.5663, train loss 322.613892\n",
      "step 12200, train accuracy 0.5646, train loss 282.023560\n",
      "step 12400, train accuracy 0.5667, train loss 310.082642\n",
      "step 12600, train accuracy 0.5663, train loss 271.734802\n",
      "step 12800, train accuracy 0.5699, train loss 262.649658\n",
      "step 13000, train accuracy 0.5536, train loss 285.959167\n",
      "step 13200, train accuracy 0.5699, train loss 270.834106\n",
      "step 13400, train accuracy 0.5644, train loss 244.127884\n",
      "step 13600, train accuracy 0.5514, train loss 255.585800\n",
      "step 13800, train accuracy 0.5735, train loss 261.607239\n",
      "step 14000, train accuracy 0.5603, train loss 221.174744\n",
      "step 14200, train accuracy 0.5649, train loss 253.733887\n",
      "step 14400, train accuracy 0.5692, train loss 270.015076\n",
      "step 14600, train accuracy 0.5694, train loss 250.882904\n",
      "step 14800, train accuracy 0.5557, train loss 260.722290\n",
      "step 15000, train accuracy 0.5599, train loss 237.817368\n",
      "step 15200, train accuracy 0.5672, train loss 244.279480\n",
      "step 15400, train accuracy 0.572, train loss 220.091232\n",
      "step 15600, train accuracy 0.5718, train loss 296.643677\n",
      "step 15800, train accuracy 0.5637, train loss 266.058044\n",
      "step 16000, train accuracy 0.5657, train loss 230.706207\n",
      "step 16200, train accuracy 0.5761, train loss 247.591003\n",
      "step 16400, train accuracy 0.5677, train loss 250.149429\n",
      "step 16600, train accuracy 0.5744, train loss 237.770065\n",
      "step 16800, train accuracy 0.5777, train loss 219.750977\n",
      "step 17000, train accuracy 0.5717, train loss 259.205750\n",
      "step 17200, train accuracy 0.5841, train loss 218.851730\n",
      "step 17400, train accuracy 0.5755, train loss 205.426392\n",
      "step 17600, train accuracy 0.576, train loss 197.746017\n",
      "step 17800, train accuracy 0.5848, train loss 245.669159\n",
      "step 18000, train accuracy 0.5917, train loss 225.583282\n",
      "step 18200, train accuracy 0.578, train loss 223.943909\n",
      "step 18400, train accuracy 0.5797, train loss 204.924622\n",
      "step 18600, train accuracy 0.5827, train loss 224.069260\n",
      "step 18800, train accuracy 0.5816, train loss 287.181122\n",
      "step 19000, train accuracy 0.5813, train loss 229.348434\n",
      "step 19200, train accuracy 0.5816, train loss 232.767853\n",
      "step 19400, train accuracy 0.5888, train loss 254.715485\n",
      "step 19600, train accuracy 0.59, train loss 220.393890\n",
      "step 19800, train accuracy 0.5926, train loss 197.127106\n",
      "step 20000, train accuracy 0.5984, train loss 205.571533\n",
      "step 20200, train accuracy 0.6001, train loss 221.974579\n",
      "step 20400, train accuracy 0.5885, train loss 215.532532\n",
      "step 20600, train accuracy 0.5821, train loss 203.327271\n",
      "step 20800, train accuracy 0.5876, train loss 207.364990\n",
      "step 21000, train accuracy 0.5884, train loss 193.225372\n",
      "step 21200, train accuracy 0.6055, train loss 224.936554\n",
      "step 21400, train accuracy 0.5858, train loss 202.373123\n",
      "step 21600, train accuracy 0.592, train loss 175.199524\n",
      "step 21800, train accuracy 0.5876, train loss 197.682770\n",
      "step 22000, train accuracy 0.5912, train loss 158.810486\n",
      "step 22200, train accuracy 0.5934, train loss 180.167404\n",
      "step 22400, train accuracy 0.5911, train loss 190.389816\n",
      "step 22600, train accuracy 0.5895, train loss 168.596466\n",
      "step 22800, train accuracy 0.5975, train loss 194.294556\n",
      "step 23000, train accuracy 0.5929, train loss 166.672409\n",
      "step 23200, train accuracy 0.5852, train loss 164.176025\n",
      "step 23400, train accuracy 0.5924, train loss 185.257675\n",
      "step 23600, train accuracy 0.5986, train loss 172.978241\n",
      "step 23800, train accuracy 0.5971, train loss 182.416336\n",
      "step 24000, train accuracy 0.595, train loss 164.294571\n",
      "step 24200, train accuracy 0.5923, train loss 151.163437\n",
      "step 24400, train accuracy 0.5911, train loss 179.714020\n",
      "step 24600, train accuracy 0.5935, train loss 149.392365\n",
      "step 24800, train accuracy 0.5999, train loss 183.643280\n",
      "step 25000, train accuracy 0.6013, train loss 186.277557\n",
      "step 25200, train accuracy 0.6136, train loss 142.744736\n",
      "step 25400, train accuracy 0.5978, train loss 129.994843\n",
      "step 25600, train accuracy 0.5983, train loss 156.406860\n",
      "step 25800, train accuracy 0.6084, train loss 163.140549\n",
      "step 26000, train accuracy 0.5985, train loss 141.651672\n",
      "step 26200, train accuracy 0.6027, train loss 173.280579\n",
      "step 26400, train accuracy 0.6017, train loss 145.391251\n",
      "step 26600, train accuracy 0.6127, train loss 135.402206\n",
      "step 26800, train accuracy 0.6159, train loss 148.119690\n",
      "step 27000, train accuracy 0.595, train loss 173.104919\n",
      "step 27200, train accuracy 0.5987, train loss 122.996544\n",
      "step 27400, train accuracy 0.5829, train loss 162.312683\n",
      "step 27600, train accuracy 0.6148, train loss 136.065277\n",
      "step 27800, train accuracy 0.605, train loss 141.092468\n",
      "step 28000, train accuracy 0.605, train loss 128.053406\n",
      "step 28200, train accuracy 0.5918, train loss 141.675964\n",
      "step 28400, train accuracy 0.6055, train loss 128.706253\n",
      "step 28600, train accuracy 0.5994, train loss 126.505524\n",
      "step 28800, train accuracy 0.6151, train loss 143.364487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29000, train accuracy 0.603, train loss 120.818336\n",
      "step 29200, train accuracy 0.6036, train loss 113.675385\n",
      "step 29400, train accuracy 0.6006, train loss 139.493835\n",
      "step 29600, train accuracy 0.6161, train loss 137.220917\n",
      "step 29800, train accuracy 0.6053, train loss 125.121140\n",
      "step 30000, train accuracy 0.5947, train loss 111.375031\n",
      "step 30200, train accuracy 0.6135, train loss 106.353485\n",
      "step 30400, train accuracy 0.6182, train loss 141.472260\n",
      "step 30600, train accuracy 0.6146, train loss 102.579559\n",
      "step 30800, train accuracy 0.626, train loss 108.193161\n",
      "step 31000, train accuracy 0.6068, train loss 95.271271\n",
      "step 31200, train accuracy 0.6094, train loss 115.877884\n",
      "step 31400, train accuracy 0.6056, train loss 96.853867\n",
      "step 31600, train accuracy 0.6126, train loss 114.522682\n",
      "step 31800, train accuracy 0.598, train loss 85.599091\n",
      "step 32000, train accuracy 0.6214, train loss 88.462448\n",
      "step 32200, train accuracy 0.6198, train loss 101.369186\n",
      "step 32400, train accuracy 0.6052, train loss 97.390732\n",
      "step 32600, train accuracy 0.609, train loss 98.093018\n",
      "step 32800, train accuracy 0.608, train loss 90.404282\n",
      "step 33000, train accuracy 0.6191, train loss 94.377380\n",
      "step 33200, train accuracy 0.607, train loss 112.005951\n",
      "step 33400, train accuracy 0.6079, train loss 95.663643\n",
      "step 33600, train accuracy 0.6107, train loss 99.935394\n",
      "step 33800, train accuracy 0.6224, train loss 73.766357\n",
      "step 34000, train accuracy 0.6323, train loss 84.645447\n",
      "step 34200, train accuracy 0.6187, train loss 72.967422\n",
      "step 34400, train accuracy 0.6171, train loss 69.674484\n",
      "step 34600, train accuracy 0.6135, train loss 78.681656\n",
      "step 34800, train accuracy 0.6176, train loss 108.120117\n",
      "step 35000, train accuracy 0.6175, train loss 76.143494\n",
      "step 35200, train accuracy 0.6196, train loss 92.084160\n",
      "step 35400, train accuracy 0.6039, train loss 83.262222\n",
      "step 35600, train accuracy 0.5987, train loss 92.244156\n",
      "step 35800, train accuracy 0.6177, train loss 65.679344\n",
      "step 36000, train accuracy 0.6162, train loss 90.235802\n",
      "step 36200, train accuracy 0.6168, train loss 73.204483\n",
      "step 36400, train accuracy 0.6225, train loss 90.432274\n",
      "step 36600, train accuracy 0.6147, train loss 59.100044\n",
      "step 36800, train accuracy 0.6135, train loss 79.833992\n",
      "step 37000, train accuracy 0.6204, train loss 65.076401\n",
      "step 37200, train accuracy 0.6106, train loss 73.002640\n",
      "step 37400, train accuracy 0.6171, train loss 76.054779\n",
      "step 37600, train accuracy 0.6015, train loss 57.173302\n",
      "step 37800, train accuracy 0.6127, train loss 84.030930\n",
      "step 38000, train accuracy 0.6107, train loss 63.239799\n",
      "step 38200, train accuracy 0.6215, train loss 53.366848\n",
      "step 38400, train accuracy 0.6138, train loss 61.040298\n",
      "step 38600, train accuracy 0.5998, train loss 76.513092\n",
      "step 38800, train accuracy 0.606, train loss 74.600975\n",
      "step 39000, train accuracy 0.6225, train loss 49.571941\n",
      "step 39200, train accuracy 0.6182, train loss 62.884819\n",
      "step 39400, train accuracy 0.618, train loss 63.960846\n",
      "step 39600, train accuracy 0.6097, train loss 61.730736\n",
      "step 39800, train accuracy 0.6185, train loss 64.195732\n",
      "step 40000, train accuracy 0.6068, train loss 52.667854\n",
      "step 40200, train accuracy 0.6207, train loss 53.538292\n",
      "step 40400, train accuracy 0.6212, train loss 48.883171\n",
      "step 40600, train accuracy 0.6249, train loss 42.750748\n",
      "step 40800, train accuracy 0.6179, train loss 41.908428\n",
      "step 41000, train accuracy 0.6171, train loss 45.215336\n",
      "step 41200, train accuracy 0.6116, train loss 54.474884\n",
      "step 41400, train accuracy 0.6266, train loss 69.558456\n",
      "step 41600, train accuracy 0.6146, train loss 49.217827\n",
      "step 41800, train accuracy 0.6142, train loss 39.187611\n",
      "step 42000, train accuracy 0.6146, train loss 38.889324\n",
      "step 42200, train accuracy 0.6074, train loss 38.919178\n",
      "step 42400, train accuracy 0.6069, train loss 39.373341\n",
      "step 42600, train accuracy 0.6166, train loss 61.733849\n",
      "step 42800, train accuracy 0.6157, train loss 37.988716\n",
      "step 43000, train accuracy 0.6241, train loss 39.309105\n",
      "step 43200, train accuracy 0.6224, train loss 41.057407\n",
      "step 43400, train accuracy 0.6137, train loss 45.068451\n",
      "step 43600, train accuracy 0.6255, train loss 34.584869\n",
      "step 43800, train accuracy 0.6088, train loss 43.062141\n",
      "step 44000, train accuracy 0.6104, train loss 34.695713\n",
      "step 44200, train accuracy 0.6148, train loss 45.437080\n",
      "step 44400, train accuracy 0.6259, train loss 25.410557\n",
      "step 44600, train accuracy 0.6197, train loss 56.697762\n",
      "step 44800, train accuracy 0.6172, train loss 29.339016\n",
      "step 45000, train accuracy 0.6172, train loss 33.068756\n",
      "step 45200, train accuracy 0.6065, train loss 30.697193\n",
      "step 45400, train accuracy 0.6207, train loss 23.829538\n",
      "step 45600, train accuracy 0.6205, train loss 42.712631\n",
      "step 45800, train accuracy 0.6161, train loss 30.524374\n",
      "step 46000, train accuracy 0.6115, train loss 45.875214\n",
      "step 46200, train accuracy 0.6234, train loss 20.626507\n",
      "step 46400, train accuracy 0.6144, train loss 24.110065\n",
      "step 46600, train accuracy 0.6291, train loss 19.998056\n",
      "step 46800, train accuracy 0.6159, train loss 20.153179\n",
      "step 47000, train accuracy 0.6146, train loss 19.003073\n",
      "step 47200, train accuracy 0.6079, train loss 20.358242\n",
      "step 47400, train accuracy 0.621, train loss 21.989843\n",
      "step 47600, train accuracy 0.6121, train loss 19.267017\n",
      "step 47800, train accuracy 0.6247, train loss 21.903023\n",
      "step 48000, train accuracy 0.6282, train loss 27.273582\n",
      "step 48200, train accuracy 0.6167, train loss 19.847910\n",
      "step 48400, train accuracy 0.622, train loss 22.800034\n",
      "step 48600, train accuracy 0.6135, train loss 32.119789\n",
      "step 48800, train accuracy 0.6138, train loss 22.541821\n",
      "step 49000, train accuracy 0.6163, train loss 15.778468\n",
      "step 49200, train accuracy 0.6168, train loss 15.070406\n",
      "step 49400, train accuracy 0.6132, train loss 34.684578\n",
      "step 49600, train accuracy 0.6294, train loss 16.563145\n",
      "step 49800, train accuracy 0.6174, train loss 13.663038\n",
      "step 50000, train accuracy 0.6181, train loss 18.943926\n",
      "step 50200, train accuracy 0.614, train loss 21.884390\n",
      "step 50400, train accuracy 0.624, train loss 18.660706\n",
      "step 50600, train accuracy 0.6215, train loss 17.819265\n",
      "step 50800, train accuracy 0.6248, train loss 33.834747\n",
      "step 51000, train accuracy 0.6243, train loss 30.890295\n",
      "step 51200, train accuracy 0.6169, train loss 18.738056\n",
      "step 51400, train accuracy 0.6113, train loss 31.960606\n",
      "step 51600, train accuracy 0.6186, train loss 15.771016\n",
      "step 51800, train accuracy 0.6153, train loss 18.424107\n",
      "step 52000, train accuracy 0.6161, train loss 27.004307\n",
      "step 52200, train accuracy 0.6245, train loss 13.745277\n",
      "step 52400, train accuracy 0.619, train loss 19.547222\n",
      "step 52600, train accuracy 0.6171, train loss 32.383846\n",
      "step 52800, train accuracy 0.6123, train loss 18.741978\n",
      "step 53000, train accuracy 0.6228, train loss 13.211004\n",
      "step 53200, train accuracy 0.6202, train loss 13.792016\n",
      "step 53400, train accuracy 0.6239, train loss 31.506769\n",
      "step 53600, train accuracy 0.6189, train loss 15.030360\n",
      "step 53800, train accuracy 0.6156, train loss 12.608656\n",
      "step 54000, train accuracy 0.607, train loss 14.253075\n",
      "step 54200, train accuracy 0.6219, train loss 12.112761\n",
      "step 54400, train accuracy 0.6207, train loss 11.861834\n",
      "step 54600, train accuracy 0.6233, train loss 28.320875\n",
      "step 54800, train accuracy 0.6167, train loss 19.252104\n",
      "step 55000, train accuracy 0.6137, train loss 11.555560\n",
      "step 55200, train accuracy 0.6051, train loss 10.366778\n",
      "step 55400, train accuracy 0.6171, train loss 23.708214\n",
      "step 55600, train accuracy 0.6177, train loss 26.678324\n",
      "step 55800, train accuracy 0.617, train loss 8.307338\n",
      "step 56000, train accuracy 0.6241, train loss 10.584054\n",
      "step 56200, train accuracy 0.6235, train loss 10.827840\n",
      "step 56400, train accuracy 0.6119, train loss 10.632364\n",
      "step 56600, train accuracy 0.6287, train loss 26.538460\n",
      "step 56800, train accuracy 0.6237, train loss 7.268901\n",
      "step 57000, train accuracy 0.6133, train loss 8.885630\n",
      "step 57200, train accuracy 0.6131, train loss 12.580027\n",
      "step 57400, train accuracy 0.6209, train loss 7.377991\n",
      "step 57600, train accuracy 0.6214, train loss 25.809986\n",
      "step 57800, train accuracy 0.6259, train loss 7.063296\n",
      "step 58000, train accuracy 0.6285, train loss 22.557598\n",
      "step 58200, train accuracy 0.6175, train loss 9.423694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 58400, train accuracy 0.6332, train loss 7.356300\n",
      "step 58600, train accuracy 0.6133, train loss 9.851759\n",
      "step 58800, train accuracy 0.6264, train loss 22.368652\n",
      "step 59000, train accuracy 0.617, train loss 8.210310\n",
      "step 59200, train accuracy 0.6258, train loss 5.625319\n",
      "step 59400, train accuracy 0.6251, train loss 6.540655\n",
      "step 59600, train accuracy 0.6223, train loss 7.665800\n",
      "step 59800, train accuracy 0.6171, train loss 4.989734\n",
      "step 60000, train accuracy 0.6171, train loss 5.729351\n",
      "step 60200, train accuracy 0.6214, train loss 6.293388\n",
      "step 60400, train accuracy 0.6226, train loss 36.712982\n",
      "step 60600, train accuracy 0.6152, train loss 7.444570\n",
      "step 60800, train accuracy 0.6227, train loss 5.507040\n",
      "step 61000, train accuracy 0.6311, train loss 3.583932\n",
      "step 61200, train accuracy 0.6082, train loss 6.544447\n",
      "step 61400, train accuracy 0.622, train loss 21.588621\n",
      "step 61600, train accuracy 0.624, train loss 6.277761\n",
      "step 61800, train accuracy 0.6236, train loss 5.205904\n",
      "step 62000, train accuracy 0.6181, train loss 5.741501\n",
      "step 62200, train accuracy 0.614, train loss 6.474050\n",
      "step 62400, train accuracy 0.6204, train loss 7.204308\n",
      "step 62600, train accuracy 0.6325, train loss 21.151701\n",
      "step 62800, train accuracy 0.6201, train loss 3.582141\n",
      "step 63000, train accuracy 0.6284, train loss 20.859568\n",
      "step 63200, train accuracy 0.6108, train loss 5.844027\n",
      "step 63400, train accuracy 0.624, train loss 3.131728\n",
      "step 63600, train accuracy 0.6282, train loss 21.135845\n",
      "step 63800, train accuracy 0.6288, train loss 3.042327\n",
      "step 64000, train accuracy 0.6237, train loss 4.256193\n",
      "step 64200, train accuracy 0.6156, train loss 3.886658\n",
      "step 64400, train accuracy 0.6246, train loss 19.327015\n",
      "step 64600, train accuracy 0.6086, train loss 21.750164\n",
      "step 64800, train accuracy 0.6081, train loss 38.838505\n",
      "step 65000, train accuracy 0.6285, train loss 21.201738\n",
      "step 65200, train accuracy 0.6156, train loss 4.582214\n",
      "step 65400, train accuracy 0.6245, train loss 5.020948\n",
      "step 65600, train accuracy 0.6091, train loss 5.243095\n",
      "step 65800, train accuracy 0.6196, train loss 2.892247\n",
      "step 66000, train accuracy 0.6226, train loss 2.443793\n",
      "step 66200, train accuracy 0.6195, train loss 20.081718\n",
      "step 66400, train accuracy 0.6169, train loss 4.437279\n",
      "step 66600, train accuracy 0.6212, train loss 3.962978\n",
      "step 66800, train accuracy 0.6315, train loss 2.575689\n",
      "step 67000, train accuracy 0.6202, train loss 3.381956\n",
      "step 67200, train accuracy 0.6268, train loss 2.993434\n",
      "step 67400, train accuracy 0.6174, train loss 20.801695\n",
      "step 67600, train accuracy 0.6288, train loss 3.192208\n",
      "step 67800, train accuracy 0.6136, train loss 1.872683\n",
      "step 68000, train accuracy 0.6215, train loss 3.237974\n",
      "step 68200, train accuracy 0.6245, train loss 3.397250\n",
      "step 68400, train accuracy 0.6153, train loss 4.320872\n",
      "step 68600, train accuracy 0.6267, train loss 2.175147\n",
      "step 68800, train accuracy 0.6182, train loss 2.592772\n",
      "step 69000, train accuracy 0.6138, train loss 1.967459\n",
      "step 69200, train accuracy 0.6258, train loss 18.513786\n",
      "step 69400, train accuracy 0.6216, train loss 19.293713\n",
      "step 69600, train accuracy 0.6218, train loss 3.419863\n",
      "step 69800, train accuracy 0.6139, train loss 18.938093\n",
      "step 70000, train accuracy 0.6181, train loss 17.595724\n",
      "step 70200, train accuracy 0.6269, train loss 2.303331\n",
      "step 70400, train accuracy 0.6207, train loss 2.795231\n",
      "step 70600, train accuracy 0.6139, train loss 2.879021\n",
      "step 70800, train accuracy 0.6212, train loss 3.096447\n",
      "step 71000, train accuracy 0.6149, train loss 5.841496\n",
      "step 71200, train accuracy 0.616, train loss 3.403139\n",
      "step 71400, train accuracy 0.6255, train loss 2.795784\n",
      "step 71600, train accuracy 0.631, train loss 2.452925\n",
      "step 71800, train accuracy 0.6195, train loss 18.133286\n",
      "step 72000, train accuracy 0.6165, train loss 2.928782\n",
      "step 72200, train accuracy 0.6268, train loss 1.537167\n",
      "step 72400, train accuracy 0.6254, train loss 2.524248\n",
      "step 72600, train accuracy 0.6117, train loss 2.241783\n",
      "step 72800, train accuracy 0.6196, train loss 19.814173\n",
      "step 73000, train accuracy 0.624, train loss 1.713959\n",
      "step 73200, train accuracy 0.6261, train loss 18.971125\n",
      "step 73400, train accuracy 0.6282, train loss 2.345259\n",
      "step 73600, train accuracy 0.6138, train loss 6.355590\n",
      "step 73800, train accuracy 0.616, train loss 1.627767\n",
      "step 74000, train accuracy 0.6141, train loss 18.964396\n",
      "step 74200, train accuracy 0.6072, train loss 2.859567\n",
      "step 74400, train accuracy 0.6275, train loss 1.617717\n",
      "step 74600, train accuracy 0.6147, train loss 1.854212\n",
      "step 74800, train accuracy 0.6238, train loss 34.141865\n",
      "step 75000, train accuracy 0.6145, train loss 2.874342\n",
      "step 75200, train accuracy 0.6176, train loss 19.379093\n",
      "step 75400, train accuracy 0.6339, train loss 18.109215\n",
      "step 75600, train accuracy 0.6285, train loss 1.314877\n",
      "step 75800, train accuracy 0.6151, train loss 2.206804\n",
      "step 76000, train accuracy 0.6292, train loss 24.341763\n",
      "step 76200, train accuracy 0.6204, train loss 2.109955\n",
      "step 76400, train accuracy 0.6269, train loss 49.463402\n",
      "step 76600, train accuracy 0.6261, train loss 3.252225\n",
      "step 76800, train accuracy 0.6169, train loss 1.444782\n",
      "step 77000, train accuracy 0.618, train loss 1.528383\n",
      "step 77200, train accuracy 0.621, train loss 1.263125\n",
      "step 77400, train accuracy 0.6073, train loss 3.196802\n",
      "step 77600, train accuracy 0.6307, train loss 1.725953\n",
      "step 77800, train accuracy 0.6231, train loss 1.441814\n",
      "step 78000, train accuracy 0.6189, train loss 0.883013\n",
      "step 78200, train accuracy 0.6294, train loss 1.726510\n",
      "step 78400, train accuracy 0.6157, train loss 18.154224\n",
      "step 78600, train accuracy 0.6205, train loss 1.206845\n",
      "step 78800, train accuracy 0.6133, train loss 1.548499\n",
      "step 79000, train accuracy 0.6147, train loss 1.656769\n",
      "step 79200, train accuracy 0.6264, train loss 1.008448\n",
      "step 79400, train accuracy 0.6229, train loss 1.475190\n",
      "step 79600, train accuracy 0.6189, train loss 1.200726\n",
      "step 79800, train accuracy 0.6254, train loss 1.641059\n",
      "step 80000, train accuracy 0.6197, train loss 1.700397\n",
      "step 80200, train accuracy 0.6148, train loss 1.233395\n",
      "step 80400, train accuracy 0.6207, train loss 0.978881\n",
      "step 80600, train accuracy 0.6156, train loss 33.967926\n",
      "step 80800, train accuracy 0.6184, train loss 17.346292\n",
      "step 81000, train accuracy 0.6235, train loss 1.093931\n",
      "step 81200, train accuracy 0.6139, train loss 1.048763\n",
      "step 81400, train accuracy 0.6117, train loss 1.557925\n",
      "step 81600, train accuracy 0.6159, train loss 1.178815\n",
      "step 81800, train accuracy 0.6249, train loss 0.941007\n",
      "step 82000, train accuracy 0.6213, train loss 1.604693\n",
      "step 82200, train accuracy 0.6133, train loss 1.223936\n",
      "step 82400, train accuracy 0.6255, train loss 1.320961\n",
      "step 82600, train accuracy 0.615, train loss 0.974327\n",
      "step 82800, train accuracy 0.6269, train loss 1.136426\n",
      "step 83000, train accuracy 0.6298, train loss 1.419575\n",
      "step 83200, train accuracy 0.6185, train loss 2.565012\n",
      "step 83400, train accuracy 0.6088, train loss 1.538400\n",
      "step 83600, train accuracy 0.624, train loss 17.008324\n",
      "step 83800, train accuracy 0.6251, train loss 17.119774\n",
      "step 84000, train accuracy 0.6198, train loss 1.097825\n",
      "step 84200, train accuracy 0.6149, train loss 1.312192\n",
      "step 84400, train accuracy 0.6121, train loss 16.977551\n",
      "step 84600, train accuracy 0.609, train loss 0.993418\n",
      "step 84800, train accuracy 0.6179, train loss 16.838961\n",
      "step 85000, train accuracy 0.6109, train loss 1.405963\n",
      "step 85200, train accuracy 0.6235, train loss 0.903338\n",
      "step 85400, train accuracy 0.6186, train loss 0.797283\n",
      "step 85600, train accuracy 0.6306, train loss 1.058662\n",
      "step 85800, train accuracy 0.6253, train loss 17.423742\n",
      "step 86000, train accuracy 0.619, train loss 1.277274\n",
      "step 86200, train accuracy 0.6253, train loss 0.943141\n",
      "step 86400, train accuracy 0.6263, train loss 0.523556\n",
      "step 86600, train accuracy 0.62, train loss 0.913058\n",
      "step 86800, train accuracy 0.6027, train loss 1.132227\n",
      "step 87000, train accuracy 0.6319, train loss 0.698040\n",
      "step 87200, train accuracy 0.623, train loss 17.128040\n",
      "step 87400, train accuracy 0.6205, train loss 1.032886\n",
      "step 87600, train accuracy 0.6198, train loss 0.840079\n",
      "step 87800, train accuracy 0.6134, train loss 1.155790\n",
      "step 88000, train accuracy 0.6213, train loss 1.743994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 88200, train accuracy 0.6259, train loss 1.212376\n",
      "step 88400, train accuracy 0.6086, train loss 1.155725\n",
      "step 88600, train accuracy 0.6251, train loss 0.634193\n",
      "step 88800, train accuracy 0.6302, train loss 0.585067\n",
      "step 89000, train accuracy 0.6191, train loss 0.534900\n",
      "step 89200, train accuracy 0.6193, train loss 2.006028\n",
      "step 89400, train accuracy 0.6166, train loss 17.012342\n",
      "step 89600, train accuracy 0.6111, train loss 17.025372\n",
      "step 89800, train accuracy 0.6076, train loss 17.229324\n",
      "step 90000, train accuracy 0.6176, train loss 16.645624\n",
      "step 90200, train accuracy 0.6186, train loss 0.390142\n",
      "step 90400, train accuracy 0.631, train loss 17.026569\n",
      "step 90600, train accuracy 0.6242, train loss 0.912160\n",
      "step 90800, train accuracy 0.6233, train loss 0.578924\n",
      "step 91000, train accuracy 0.6193, train loss 0.499168\n",
      "step 91200, train accuracy 0.6235, train loss 16.840761\n",
      "step 91400, train accuracy 0.6173, train loss 0.438019\n",
      "step 91600, train accuracy 0.6176, train loss 0.541330\n",
      "step 91800, train accuracy 0.6291, train loss 0.762086\n",
      "step 92000, train accuracy 0.6182, train loss 0.965767\n",
      "step 92200, train accuracy 0.6208, train loss 0.988780\n",
      "step 92400, train accuracy 0.6207, train loss 0.904814\n",
      "step 92600, train accuracy 0.6161, train loss 16.637609\n",
      "step 92800, train accuracy 0.6234, train loss 1.032705\n",
      "step 93000, train accuracy 0.6111, train loss 2.046388\n",
      "step 93200, train accuracy 0.6293, train loss 0.465577\n",
      "step 93400, train accuracy 0.6242, train loss 0.300199\n",
      "step 93600, train accuracy 0.631, train loss 0.736468\n",
      "step 93800, train accuracy 0.6203, train loss 0.501542\n",
      "step 94000, train accuracy 0.6122, train loss 16.826767\n",
      "step 94200, train accuracy 0.6227, train loss 22.038542\n",
      "step 94400, train accuracy 0.628, train loss 0.844881\n",
      "step 94600, train accuracy 0.6268, train loss 0.565067\n",
      "step 94800, train accuracy 0.6178, train loss 0.650707\n",
      "step 95000, train accuracy 0.6226, train loss 0.691205\n",
      "step 95200, train accuracy 0.6173, train loss 16.509588\n",
      "step 95400, train accuracy 0.6213, train loss 16.429825\n",
      "step 95600, train accuracy 0.6198, train loss 0.425963\n",
      "step 95800, train accuracy 0.624, train loss 0.596009\n",
      "step 96000, train accuracy 0.6164, train loss 0.310288\n",
      "step 96200, train accuracy 0.6319, train loss 0.515238\n",
      "step 96400, train accuracy 0.6182, train loss 0.366931\n",
      "step 96600, train accuracy 0.6192, train loss 1.497937\n",
      "step 96800, train accuracy 0.6167, train loss 0.551836\n",
      "step 97000, train accuracy 0.6199, train loss 1.034538\n",
      "step 97200, train accuracy 0.6236, train loss 0.640603\n",
      "step 97400, train accuracy 0.6244, train loss 16.461319\n",
      "step 97600, train accuracy 0.6192, train loss 0.267051\n",
      "step 97800, train accuracy 0.6149, train loss 0.382095\n",
      "step 98000, train accuracy 0.6126, train loss 0.415394\n",
      "step 98200, train accuracy 0.6209, train loss 0.452554\n",
      "step 98400, train accuracy 0.6179, train loss 0.218446\n",
      "step 98600, train accuracy 0.6231, train loss 16.468021\n",
      "step 98800, train accuracy 0.6272, train loss 0.243436\n",
      "step 99000, train accuracy 0.622, train loss 0.375244\n",
      "step 99200, train accuracy 0.6245, train loss 0.341455\n",
      "step 99400, train accuracy 0.6198, train loss 0.488188\n",
      "step 99600, train accuracy 0.6253, train loss 0.384182\n",
      "step 99800, train accuracy 0.622, train loss 0.386014\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/data.npy'\n",
    "\n",
    "vali = np.load('E:/Alibaba German AI Challenge/data_process/sample_of_training_10k.npy')\n",
    "data = np.load(filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "Eval = []\n",
    "\n",
    "batch_size = 300\n",
    "for i in range(100000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "        Eval.append(train_accuracy)\n",
    "        print (\"step %d, train accuracy %g, train loss %f\" %(i, train_accuracy, loss_temp))\n",
    "#    if loss_temp < 1:\n",
    "#        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "#        print('Final:step %d, train loss %f'%(i,loss_temp))\n",
    "#        print (\"Final accuracy %g\" %train_accuracy)\n",
    "#        break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAE/CAYAAAB8YAsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x228bf7e5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Eval)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Eval)\n",
    "plt.title('Eval')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAE/CAYAAAC5EpGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8F9W9//HXh6wEAmEJYQlbBVQUAYm4FlGxovaKrdbi\nbZXea6utS9Uu1u22tZWqrbWtv169xWrV3lsVq3VHZXHDBQwKsgmEPQGSsGUl+/n98Z2ELyEYyPbN\nzLyfj8f3kfme2c4R8J05c+aMOecQERERf+kS6wqIiIjIkVOAi4iI+JACXERExIcU4CIiIj6kABcR\nEfEhBbiIiIgPKcBFRER8SAEuEhJmtsnMpsS6HiLSNhTgIiIiPqQAFwk5M/uemeWY2W4ze8nMBnrl\nZmZ/MLMCMys2s+Vmdry37gIzW2VmJWaWZ2Y/iW0rRMJHAS4SYmZ2NnAPcBkwANgMPO2t/gowCRgF\n9PS22eWtexS4xjmXChwPLOjAaosIEB/rCohITH0LeMw59wmAmd0G7DGzYUA1kAocAyx2zq2O2q8a\nGG1my5xze4A9HVprEdEVuEjIDSRy1Q2Ac66UyFX2IOfcAuDPwH8DBWY2y8x6eJteAlwAbDazd8zs\n1A6ut0joKcBFwm0bMLT+i5l1A/oAeQDOuQedcxOA0US60n/qlX/snJsG9ANeAGZ3cL1FQk8BLhIu\nCWaWXP8BngL+w8zGmVkS8BtgkXNuk5mdZGYnm1kCUAZUAHVmlmhm3zKzns65aqAYqItZi0RCSgEu\nEi6vAfuiPpOB/wKeA7YDRwHTvW17AI8Qub+9mUjX+u+8dVcAm8ysGPg+kXvpItKBzDkX6zqIiIjI\nEdIVuIiIiA8pwEVERHxIAS4iIuJDCnAREREfUoCLiIj4UKefSrVv375u2LBhsa6GiIhIh1iyZMlO\n51x6c9t1+gAfNmwY2dnZsa6GiIhIhzCzzc1vpS50ERERX1KAi4iI+JACXERExIcU4CIiIj6kABcR\nEfEhBbiIiIgPKcBFRER8SAEuIiLiQwpwERERHwpVgM9blc+Cz/NjXQ0REZFW6/RTqbalv7y7noS4\nLpx9TEasqyIiItIqoboCFxERCYrQBbhzsa6BiIhI64UqwA2LdRVERETaRKgCXEREJCgU4CIiIj4U\nugB36Ca4iIj4X7gCXLfARUQkIMIV4CIiIgERugDXY2QiIhIEoQpw9aCLiEhQhCrARUREgiJ0Aa4e\ndBERCYJQBbipD11ERAIiVAEuIiISFOELcPWhi4hIAIQqwPUyExERCYpQBbiIiEhQhC7ANRe6iIgE\nQagCXKPQRUQkKEIV4CIiIkGhABcREfGh0AW4XmYiIiJBEKoA1z1wEREJilAFuIiISFCELsDVgy4i\nIkEQqgDXTGwiIhIUoQpwERGRoAhdgDsNQxcRkQAIVYBrFLqIiARFqAJcREQkKEIX4OpAFxGRIGg2\nwM1ssJm9ZWarzGylmd3olfc2s7lmts772Stqn9vMLMfM1pjZeVHlE8xsubfuQTN1aouIiLTE4VyB\n1wA/ds6NBk4BrjOz0cCtwHzn3Ehgvvcdb9104DhgKvCQmcV5x3oY+B4w0vtMbcO2iIiIhEazAe6c\n2+6c+8RbLgFWA4OAacAT3mZPABd7y9OAp51zlc65jUAOMNHMBgA9nHMfuchQ8Cej9hEREZEjcET3\nwM1sGDAeWARkOOe2e6t2ABne8iBga9RuuV7ZIG+5cXmH0lNkIiISBIcd4GbWHXgOuMk5Vxy9zrui\nbrNoNLOrzSzbzLILCwvb6rDolruIiATFYQW4mSUQCe//c8497xXne93ieD8LvPI8YHDU7pleWZ63\n3Lj8IM65Wc65LOdcVnp6+uG2RUREJDQOZxS6AY8Cq51zD0StegmY4S3PAF6MKp9uZklmNpzIYLXF\nXnd7sZmd4h3zyqh9Oox60EVEJAjiD2Ob04ErgOVmttQrux24F5htZlcBm4HLAJxzK81sNrCKyAj2\n65xztd5+1wKPA12BOd6nw6gDXUREgqLZAHfOLeTQ2XfOIfaZCcxsojwbOP5IKigiIiIHC91MbBqG\nLiIiQRCqANcgdBERCYpQBbiIiEhQhC7A1YEuIiJBEKoAVw+6iIgERagCXEREJCgU4CIiIj4UugDX\nU2QiIhIEoQpwvcxERESCIlQBLiIiEhShC3CnB8lERCQAQhXg6kAXEZGgCFWAi4iIBEXoAlyj0EVE\nJAhCFeAahC4iIkERqgAXEREJitAFuLrQRUQkCEIW4OpDFxGRYAhZgIuIiARD6AJcPegiIhIEoQpw\njUIXEZGgCFWAi4iIBIUCXERExIdCF+BOz5GJiEgAhCrAdQtcRESCIlQBLiIiEhQKcBERER8KVYDr\nMTIREQmKUAW4iIhIUIQuwDUIXUREgiBUAW4ahy4iIgERqgAXEREJitAFuNPrTEREJABCFeAahS4i\nIkERqgAXEREJCgW4iIiID4UuwPUYmYiIBEGoAlz3wEVEJChCFeAiIiJBEboAVw+6iIgEQagCXDOx\niYhIUIQqwEVERIIidAHuNAxdREQCIFwBrh50EREJiGYD3MweM7MCM1sRVfZLM8szs6Xe54KodbeZ\nWY6ZrTGz86LKJ5jZcm/dg2Z6qEtERKSlDucK/HFgahPlf3DOjfM+rwGY2WhgOnCct89DZhbnbf8w\n8D1gpPdp6pjtTh3oIiISBM0GuHPuXWD3YR5vGvC0c67SObcRyAEmmtkAoIdz7iMXuQn9JHBxSyvd\nUrrkFxGRoGjNPfAbzOwzr4u9l1c2CNgatU2uVzbIW25cLiIiIi3Q0gB/GPgSMA7YDvy+zWoEmNnV\nZpZtZtmFhYVtdtyPNuxmQ2FZmx1PREQkVloU4M65fOdcrXOuDngEmOitygMGR22a6ZXlecuNyw91\n/FnOuSznXFZ6enpLqtiknaWVbXYsERGRWGpRgHv3tOt9Dagfof4SMN3MksxsOJHBaoudc9uBYjM7\nxRt9fiXwYivqLSIiEmrxzW1gZk8Bk4G+ZpYL/AKYbGbjiAzq3gRcA+CcW2lms4FVQA1wnXOu1jvU\ntURGtHcF5ngfERERaYFmA9w5d3kTxY9+wfYzgZlNlGcDxx9R7URERKRJ4ZqJTUREJCAU4CIiIj6k\nABcREfEhBbiIiIgPKcBFRER8SAEuIiLiQwpwERERH1KAi4iI+JACXERExIdCGeAV1bXNbyQiItKJ\nhTLA65yLdRVERERaJZQBLiIi4ncKcBERER9SgIuIiPiQAlxERMSHFOAiIiI+pAAXERHxIQW4iIiI\nDynARUREfCiUAa55XERExO9CGeAiIiJ+F8oA311WFesqiIiItEooA7ykoibWVRAREWmVUAb4qu3F\nsa6CiIhIq4QywPdV6QpcRET8LZQBXqdR6CIi4nOhDHARERG/U4CLiIj4UCgD3GkmFxER8blQBriI\niIjfhTLANYhNRET8LpQBXl1bF+sqiIiItEooA/yx9zfGugoiIiKtEsoA31dVG+sqiIiItEooAzw+\nLpTNFhGRAAllklmsKyAiItJKoQxwERERv1OAi4iI+FAoA1yPgYuIiN+FMsBFRET8TgEuIiLiQ6EM\ncI1CFxERvwtlgIuIiPhdKAM8MT6UzRYRkQBpNsnM7DEzKzCzFVFlvc1srpmt8372ilp3m5nlmNka\nMzsvqnyCmS331j1oZjHrye6WFB+rU4uIiLSJw7kUfRyY2qjsVmC+c24kMN/7jpmNBqYDx3n7PGRm\ncd4+DwPfA0Z6n8bHbHd9uiUCkNY1oaNPLSIi0qaaDXDn3LvA7kbF04AnvOUngIujyp92zlU65zYC\nOcBEMxsA9HDOfeScc8CTUft0mF1lVQBkb97T0acWERFpUy29GZzhnNvuLe8AMrzlQcDWqO1yvbJB\n3nLjchEREWmBVo/m8q6o23RyMzO72syyzSy7sLCwLQ8tIiISCC0N8HyvWxzvZ4FXngcMjtou0yvL\n85YblzfJOTfLOZflnMtKT09vYRVFRESCq6UB/hIww1ueAbwYVT7dzJLMbDiRwWqLve72YjM7xRt9\nfmXUPiIiInKEmn2eysyeAiYDfc0sF/gFcC8w28yuAjYDlwE451aa2WxgFVADXOecq/UOdS2REe1d\ngTneR0RERFqg2QB3zl1+iFXnHGL7mcDMJsqzgeOPqHYiIiLSJE1JJiIi4kMKcBERER9SgIuIiPiQ\nAlxERMSHFOAiIiI+pAAXERHxIQW4iIiIDynARUREfEgBLiIi4kMKcBERER9SgIuIiPiQAlxERMSH\nFOAiIiI+pAAXERHxIQW4iIiID4UqwIf2SYl1FURERNpEqAJ8wtBesa6CiIhImwhVgOP2L9bVuUNv\nJyIi0smFKsAze3VtWM7buy+GNREREWmdUAX4+Kgu9IKSyhjWREREpHVCFeAWtTxvdX7M6iEiItJa\noQpwERGRoAhtgL+5ckesqyAiItJioQrwE6Puga8vLIthTURERFonVAHeIzkh1lUQERFpE6EKcBER\nkaBQgIuIiPiQAlxERMSHQh3gpZU1sa6CiIhIi4Q6wNfsKI51FURERFok1AGu95mIiIhfhTvAleAi\nIuJToQ7wOSs0G5uIiPhTqAP8X5/mxboKIiIiLRLqAC/aV82KvKJYV0NEROSIhTrAAd5dVxjrKoiI\niByx0Ae40zg2ERHxodAHuIiIiB+FPsCL91XHugoiIiJHLPQBXllTF+sqiIiIHLHQB3idboKLiIgP\nhT7A9RiZiIj4UegD/JMte9lbXhXraoiIiByR0AX4zVNGHVQ247HFMaiJiIhIy7UqwM1sk5ktN7Ol\nZpbtlfU2s7lmts772Stq+9vMLMfM1pjZea2tfEt8IyvzoLJluUXU6sUmIiLiI21xBX6Wc26ccy7L\n+34rMN85NxKY733HzEYD04HjgKnAQ2YW1wbnPyID07o2WX7U7a91cE1ERERarj260KcBT3jLTwAX\nR5U/7ZyrdM5tBHKAie1w/hbburs81lUQERE5LK0NcAfMM7MlZna1V5bhnNvuLe8AMrzlQcDWqH1z\nvbJO4+21mhddRET8obUBfoZzbhxwPnCdmU2KXumcc0RC/oiY2dVmlm1m2YWFHReq9835vMPOJSIi\n0hqtCnDnXJ73swD4F5Eu8XwzGwDg/SzwNs8DBkftnumVNXXcWc65LOdcVnp6emuqeERKK2s67Fwi\nIiKt0eIAN7NuZpZavwx8BVgBvATM8DabAbzoLb8ETDezJDMbDowEOt3zW3vK9Ey4iIh0fq25As8A\nFprZMiJB/Kpz7nXgXuBcM1sHTPG+45xbCcwGVgGvA9c552pbU/n2MP7Xc2NdBRERkWbFt3RH59wG\nYGwT5buAcw6xz0xgZkvP2VaG9E5hi0aci4iIj4VuJjaACUN7feH6X728qoNqIiIi0jKhDPDmZl17\n7P2N1NQe+JrR5blF5BSUtGe1REREDlsoA/yaM7/U7DYj7phDVdS7wv/tzwuZ8sC77VktERGRwxbK\nAD9uYM/D2m7UnXPYuruc+99Y0841EhEROTItHsQWFjP+tpgNhWUN30sqqlm0YTdTRmd8wV4iIiLt\nK5RX4ACTjz68CWKiwxtgzC/f5LtPZrN5V9kh9hAREWl/oQ3wuy46rlX7X/HoYnYUVbRRbURERI5M\naAO8ta//3rK7nFPumc+jCzcSmfJdRESk44Q4wNsmdH/9yiqG3/YaO0sr2+R4IiIihyO0g9h6JCe0\n6fGy7p4HwLEDevDojCwGpnVt0+OLiIhEC+0VeHpqEu/8dHKbH3f19mJOu3cBf16wjuraOv7z8Y95\nedm2w9qvqLy6zesjIiLBFNoABxjapxuv/fDL7XLs+99cy8g75rDg8wJueOpTAApLKjn/T++Ru+fg\nedjP/9N7fOMvH7RLXUREJHhCHeAAowf24JP/Orfdz7Mir4gH5q5l9fZiHl24kc9y97K10QtV1uaX\ntns9REQkGKyzj6DOyspy2dnZ7X6eWe+u5zevfd7u52nsjBF9WZizs+H7pnsvbFguqajm/ZydTD1+\nQIfXS0REYsPMljjnsprbLrSD2Bo7aVjvmJw3OrwBHn57PempSfz9w00syy1qKL8sK5PfXnrQ21tF\nRCSkQt+FXm9w75RYVwGA+17/nJ88u+yA8AaYnZ1LYUklt/xzGaWVNQDkFES63M/5/dvM/nhrh9dV\nRERiR1fgnr7dkzgqvRvjBvdi0qi+3Pj00lhX6SAnzYw8qjakdwrH9O/Bd5/M5pj+qawvLOOW5z5j\naJ8U5q7Kp7iimvsuOQEzi3GNRUSkvege+BcYduurMTlvW1j2i6/Qs2sCe8qqSE2OZ8Ld85h6XH/u\nu/SEWFdNRES+gO6Bt4HXfvhlyqpq2FBYys+eWx7r6hyRguIKZn+8lZmvrW4oeyZ7K3FxxrdPHsoD\nc9cwb3UBd154LN/9cvPvR/8ipZU1VNfU0atbYmurLSIih0lX4IdpT1kV4389N9bVaBfv33o2g9K6\nUlVTx8KcQj7LLeLaySNYm1/C8YOaf3f6uF+9yd7y6gNG0IuISMvoCryN9eqWyIIfn8lVT2SzcWew\nXiV6+r0L2HjPBfz42WUNs8b9cd46AL5z2jA+3bKHtJRE3llbyJwbv0xZZQ1ZUaP293ozyG3eVUZN\nneOo9O4d3wgRkZDRFfgR2ltexc+e+4x7v34CC3N2NsyyFjY//+po/m3sQNJTkw4aK7Dp3gtZvHE3\nJw5JY9veCnp3T6R7UvO/Kxbtq6aqpo701KT2qraISKd3uFfgCvA2sHTrXl5eto0tu8uZuyo/1tXp\nUA9ePp4fNvol5idfGcX9b649oOzRGVmcc2wGFdW1lFfV0tu7X748t4j01CQyeiQx/LbXgP2T2dT/\n3dRoehEJEwV4jCzPLWJY3xQS47tw579W8OyS3FhXqVPadO+FPJu9lZ/+8zMA7rzwWO5+dXXDOoCz\n73+bDTvLuOfrY7h84pCGfTcUlnLVE9k8+/1T6dtdV+siEiyHG+CayKWNjcnsSWpyAknxccz82hgW\n/uws0lIiry79r6+O5udfHc01k1o36jsIht36akN4Aw3hDZGXvhSVV7PBG2tw2/PLKSipaFj/yHsb\n2bizjDdW7mjx+Z1z3PnCcj7L3dviY4iIxJKuwGPo3bWFVFTXcvXfl8S6Kp1et8Q4zh2dwdnHZjR0\n2Z+Q2ZPPvBnrUpPiKfFmqDv7mH786NxRXP7IR8z/0Zn065HccJz3c3by0Ns5/PnyExn/67mkJsez\n/JfnHXS+5blFJMZ34ej+qYesU3VtHSUVNQ23A0RE2oK60H3mL++s5545B75MZcapQ3niw80xqlFw\njB+SxjWTvsSf5uewensxAGcdnc5bawrp2TWBZb/4CivyivjdG2t45MosEuO7NAzM++f3T2Xc4DTi\n4w7urLr5maX869M8NvzmArp00X16EWkbCnCf2VVayeWPfMTa/FJuOHsE3zxpMJm9UiivquHqJ5fw\nb2MHsDa/lPdzdvL5jpJYVzdQbpoysuGxuTNG9OUHk4/iW39d1LD++rNGcOOUkdTUOromxgGRLvj6\nQXd/+OZYvjY+s9nzlFRUsza/lAlDe7VDK0QkKBTgAVZQUkFqUgLH/vz1WFcldG6eMoq4LvD8J3kN\n9+ghEvyPXJlFZU0tH67fxdTj+1NWVUt8FyM5IRL63/7rIhbm7GTlXefR7TAeqxORcNJELgHWLzVy\nT/fhb53IMQN6MKR3Cn97fyNjBvVk7qp8vpE1mKF9Im9Xe+jt9Tw4f10sqxsof5i3tsnyhTk7D/iF\n6trJR/HQ2+sbvs++5tSGV8fu9Z5379UtkeW5RfTrkURGj2Qqqmv53482c1R6d846pt9B59i8q4xL\nHv6QF68/nUFpXdu4ZSLiN7oCDzjnHO+sLaS61nHOMf14Zfn2g57bltj76gkDeOWz7Q3f7774eBxw\n3uiMhkF498xZzV/e2cAtU4/m2skjGratrXPc/vxyvjdpOCP67R90t3jjbi77y4e8efMkRmUcejCe\niHQu6kKXJjnn+MfiLVw0diB7y6u5/JGPePb7pzKgZ1deXJrHuaMzWLmtmIzUZC78f+/RLTGeHcUV\nzL15Eo9/sIn/W7Ql1k0Ina+fOIgVeUWszY+8/31URnf6pSbzk/OO5oRBPVmau5evP/QBx/RP5fWb\nJjF/dT5XPZHN18cP4vlP8wCY96MzGdGvO6fdM5/uyfG8efOZsWySiHwBBbi0ibo6R1VtXcN9XICK\n6lqK91Xz9ppCLpmQyXvrClm4bie9uiXyuzfWcNHYgbzkzakunccL153Oxf/9PgCv3HAGCz4v4Iaz\nRxw0092+qlqSE7ocUO6c4wf/+wn/fvIQJo1K79B6Q2Tynr6pSfRITujwc4t0NAW4xJxzjjteWMG1\nk4/ihqc+5dMtB06akjW0F9mb98SodhJt9jWn8sLSPL6ZNZhpXsj/YPJR/GzqMfx5wTqmjM5g6h/f\nA/a/va4jDbv1VUb2687cH6nnQIJPAS6djnMOM2PYra9y6YRM7v/GWPZV1ZJTUMqq7UWs3l7C6IE9\nuCVqhjbpfO67ZAx/nLeOOuf4+1Un862/LqKwpJI/TR/HkN4pjB8SeUxuZ2kldc41DLpszguf5jGo\nV1e6JsQd9Brb+ufy66fZnbsqn/49khmT2fzrbkX8RgEunVZZZQ3JCXHENTH5SWVNLUff+TqTRqVz\ny3lHc0z/VD7fUUJGj2Qm/mYe791yFimJ8fTulshnuXuZPusjrj97BL99fU3DMY5K70ZxRQ2FJZUd\n2Sw5hFlXTOD6pz5l9jWnklNQyiUnDgL2v6SmtLKGTzbv4crHFjfs0/jd8o0DvPF3kSBRgEuo1dTW\nsb2ogsG9U1i1rZgLHox0/750/enc8a8VLM8rinEN5YuM6NedOTd+mZF3zPnC7Vob4Ll7yjnjvrd4\n7genaYId6TT0MhMJtfi4LgzuHXkWfvTAHjz1vVNYc/dUTshM4+UbzuCNmybx3i1ncf1ZI1jw4zMP\nCIKN91zAl0f2BeCH54xsKL/vkjEd24gQyykobTa8AZ75eAt//3ATizfu5sIH3+N/3ll/wPqi8mrW\nF5ayeVcZzjk27SxjyeY9/PW9DTjnWLhuZ8NxmrIir4g9ZVXU1Na1uk0ibU1X4CKeZz7ewrvrdvLf\n/34iNbV1bNtbwRBvQpx6hSWVnDRzHjdPGcWNU0Y2eZz67t3xQ9IaBu6lJMZRXlXbvg2QIzJucBpL\nt+4fWHlZVibfOW04SzbvZvrEIRSWVHLavQsAuHDMAMYNTmPma6u5etKXuP2CY6mormXb3n2kJifQ\nLSmOlMT2mxersKSSTbvKOGlY73Y7h3Qe6kIXiZGyyhrMICUxnuKKahasLuDi8YMa1hftq2ZXaSVd\nE+P4Z3YuXboYT364iTdvOpO7X11FQUklF4zpzwmZaZz/p/di1xA5pInDerN40+6G78P7duOtn0zm\nobdzKNpXzZZd5UwbN5AP1u/i9guOPeAxzGiVNbU8unAj/3HacLbuKT9owp1FG3bx2zfWsHJbERXV\ndbrnHxIKcJGAqaypJb5Ll4bBf2WVNdQ5R2llDXe/sppXl29nQM9kthdF3p1+wZj+vLa85e9Ml7Y3\n5dgMBqUlN/uWwVlXTGjyNcPP/eA0xg9O466XVzJ+SC8uHj+Ix9/fyC9fXsXi28/h7bWF3PXSSk4b\n0Zf7Lx1LVW0dZhDfxUhL2f/aW+cczsGe8ioc0DUh7ojn5y+vquHFpduYftLgg+YS2FUa6TGYMLTp\nHoOF63YytE8KW3ZHfmlJT0065Hlq6xxdLDLosai8mg83RN410N627i4nLSWB1BjMPaAAFwkR5xwf\nb9rDxOGH7mJ9a00BD7y5lrumHcebK/P5x6LNFFfU8ONzR/H7uQfO8d54alcJhktOzOS5T3KbXPf8\ntafx8rJt1NY5xmamMX5IGskJcfxh7lrGDk5jzY4SrjpjOF3MKKuq4aG31/Pysm08/K0TGTckjSc+\n2MyIft25dEImY37xBiWVNQf0GNTWOapr6/jHoi386pVVDeXD+qTw9k/ParJO9U+lXHfWUfz0vGO4\n4tFFvLdu50FzEQy/7VXOHJXO4/8xsY3+S8V27gEFuIg0a+vucjJ7dcXMqK1zlFbU0DNl/xXHnrIq\nluXu5fEPNjG4VwoXjRtI96R4CkoqWbxxFxsKy5izYgejMrrTNSGOZbka3R92I/p1J6cgMu3vnRce\ny92vrj6s/Y7OSGVNfskBvzw+c/UpfHPWRwdt2z0pnqF9Uvj9ZWOJ79KFKQ+8A8CT/zmRvL372LSr\njO17Kzj/+P68uSofgLX5JazcVtxQr6+NH0RhaSU7S6oYnt6NnSWVpKcmUV1bR9/uSRz3izeA/U86\nrMsvYU95NQN6JpMU34WUpHi6t9NbBRXgItIpbNxZxvzV+ewpr+KMEem8/Nk2ThrWi5ufWQbA5RMH\n89TirTGupUjTRvbrzus3TeKo21875DZr7p5KUnzT4xxaotMGuJlNBf4ExAF/dc7d+0XbK8BFwmPz\nrjL+550NnH98f0b0686AnsmYGQXFFRRX1HDzM0tZnldEr5QEMnulHPJ5/m9MyOTZJU13FYu0h7Yc\nYNgpA9zM4oC1wLlALvAxcLlzbtWh9lGAi8jhKq+qobyqlr7dI4OinHNU1tR94SjwpxZtYUifFEor\nIyPC84sqOLp/KkP7pPBkM4PNROrFIsDb78HFpk0EcpxzGwDM7GlgGnDIABcROVwpifEHPI9tZocM\nb4Ck+Di+c/rwhu8XjR14wPpfTTv+iOsQPWraOUfe3n04B9NnfcS5ozM47ag+DO/bja8//AElFTVH\nfHyReh0d4IOA6JtducDJHVwHEZF2Ez3Hv5mR2SsyGdD7t559wHbLf3leq89VUV1LQtz+RwvziytI\njo+jZ0oC6wtL6d8jmW5J8RQUV9A9OZ7V20sYmJZMXBcjKT6OnaWVpCTG8bf3NzHr3Q1MGzeQMYN6\nMnZwGs8tyeXpj7cyaVQ6F40dyE+eXdbq+krb6ugu9EuBqc6573rfrwBOds5d32i7q4GrAYYMGTJh\n82Z1Y4ncO53dAAAGMUlEQVSISNuprKklMa5LQ09J/ZMYBuyrriUpvgtllbVU1tSSmpxASWU1vb1n\n6VduK2ZI7xS6JcWzr6r2gCc32kJn7ULPAwZHfc/0yg7gnJsFzILIPfCOqZqIiIRF9Kjx+olo6nsy\n6ie16ZnSBYiEc9fE/duPHZzWsJwYH7tXinT0mT8GRprZcDNLBKYDL3VwHURERHyvQ6/AnXM1ZnY9\n8AaRx8gec86t7Mg6iIiIBEFHd6HjnHsNOPQT8SIiItIsvQ9cRETEhxTgIiIiPqQAFxER8SEFuIiI\niA8pwEVERHxIAS4iIuJDCnAREREf6vD3gR8pMysE2nIy9L7AzjY8XiwFpS1BaQeoLZ1VUNoSlHaA\n2vJFhjrn0pvbqNMHeFszs+zDmSTeD4LSlqC0A9SWzioobQlKO0BtaQvqQhcREfEhBbiIiIgPhTHA\nZ8W6Am0oKG0JSjtAbemsgtKWoLQD1JZWC909cBERkSAI4xW4iIiI74UmwM1sqpmtMbMcM7s11vWp\nZ2aPmVmBma2IKuttZnPNbJ33s1fUutu8Nqwxs/OiyieY2XJv3YNmZl55kpk945UvMrNh7dSOwWb2\nlpmtMrOVZnajj9uSbGaLzWyZ15a7/NoW71xxZvapmb3i53Z459vk1WOpmWX7tT1mlmZm/zSzz81s\ntZmd6tN2HO39WdR/is3sJj+2xTvXzd6/+RVm9pT3/4LO2xbnXOA/QBywHvgSkAgsA0bHul5e3SYB\nJwIrosp+C9zqLd8K3Octj/bqngQM99oU561bDJwCGDAHON8rvxb4H295OvBMO7VjAHCit5wKrPXq\n68e2GNDdW04AFnn18V1bvOP/CPgH8Ipf/35FtWUT0LdRme/aAzwBfNdbTgTS/NiORm2KA3YAQ/3Y\nFmAQsBHo6n2fDXynM7elXf9AO8sHOBV4I+r7bcBtsa5XVH2GcWCArwEGeMsDgDVN1Rt4w2vbAODz\nqPLLgb9Eb+MtxxOZbMA6oE0vAuf6vS1ACvAJcLIf2wJkAvOBs9kf4L5rR9S5N3FwgPuqPUBPIkFh\nfm5HE+36CvC+X9tCJMC3Ar2987zitanTtiUsXej1fzD1cr2yzirDObfdW94BZHjLh2rHIG+5cfkB\n+zjnaoAioE/7VDvC6xYaT+TK1Zdt8bqdlwIFwFznnF/b8kfgFqAuqsyP7ajngHlmtsTMrvbK/Nae\n4UAh8Dfv1sZfzaybD9vR2HTgKW/Zd21xzuUB9wNbgO1AkXPuzc7clrAEuG+5yK9qvnlUwMy6A88B\nNznniqPX+aktzrla59w4IlewE83s+EbrO31bzOyrQIFzbsmhtvFDOxo5w/tzOR+4zswmRa/0SXvi\nidw2e9g5Nx4oI9I128An7WhgZonARcCzjdf5pS3eve1pRH7BGgh0M7NvR2/T2doSlgDPAwZHfc/0\nyjqrfDMbAOD9LPDKD9WOPG+5cfkB+5hZPJHuu13tUWkzSyAS3v/nnHveK/ZlW+o55/YCbwFT8V9b\nTgcuMrNNwNPA2Wb2vz5sRwPvKgnnXAHwL2Ai/mtPLpDr9eoA/JNIoPutHdHOBz5xzuV73/3YlinA\nRudcoXOuGngeOK0ztyUsAf4xMNLMhnu/KU4HXopxnb7IS8AMb3kGkfvJ9eXTvZGMw4GRwGKve6fY\nzE7xRjte2Wif+mNdCizwfotsU955HwVWO+ce8Hlb0s0szVvuSuRe/ud+a4tz7jbnXKZzbhiRv/ML\nnHPf9ls76plZNzNLrV8mcn9yhd/a45zbAWw1s6O9onOAVX5rRyOXs7/7vPH5/dKWLcApZpbi1eEc\nYHWnbktbDwTorB/gAiIjo9cDd8S6PlH1eorI/ZZqIr+ZX0Xknsh8YB0wD+gdtf0dXhvW4I1s9Mqz\niPzPbD3wZ/ZP0pNMpFsrh8jIyC+1UzvOINK19Bmw1Ptc4NO2nAB86rVlBfBzr9x3bYmqx2T2D2Lz\nZTuIPEWyzPusrP937Mf2AOOAbO/v2AtALz+2wztXNyJXkT2jyvzalruI/LK+Avg7kRHmnbYtmolN\nRETEh8LShS4iIhIoCnAREREfUoCLiIj4kAJcRETEhxTgIiIiPqQAFxER8SEFuIiIiA8pwEVERHzo\n/wMj7S3BSP5xCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e59d7869e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.82      0.44       157\n",
      "          1       0.53      0.45      0.49       710\n",
      "          2       0.67      0.30      0.42       858\n",
      "          3       0.36      0.48      0.41       254\n",
      "          4       0.38      0.48      0.42       492\n",
      "          5       0.56      0.45      0.50       956\n",
      "          6       0.34      0.94      0.50        80\n",
      "          7       0.77      0.43      0.55      1092\n",
      "          8       0.42      0.58      0.49       389\n",
      "          9       0.28      0.48      0.35       323\n",
      "         10       0.92      0.83      0.87      1230\n",
      "         11       0.35      0.78      0.48       264\n",
      "         12       0.50      0.81      0.62       268\n",
      "         13       0.85      0.56      0.68      1186\n",
      "         14       0.46      1.00      0.63        65\n",
      "         15       0.55      0.93      0.69       251\n",
      "         16       0.97      0.98      0.98      1425\n",
      "\n",
      "avg / total       0.68      0.62      0.63     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = y_conv.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "train_val_y = np.argmax(vali[:,-17:],axis = 1)\n",
    "pred_y = np.argmax(pred, axis = 1)\n",
    "print (classification_report(train_val_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('forth_100k_batch_balance_50k_train_Adam.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 重建CNN，减小感受野，增加网络深度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-383ac2a0c8ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'E:/Alibaba German AI Challenge/data_process/data.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mvali\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/Alibaba German AI Challenge/data_process/sample_of_training_10k.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The shape of data is '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[1;32m--> 419\u001b[1;33m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[1;31m# Try a pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;31m# We can use the fast fromfile() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;31m# This is not a real file. We have to read it the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/data.npy'\n",
    "\n",
    "vali = np.load('E:/Alibaba German AI Challenge/data_process/sample_of_training.npy')\n",
    "data = np.load(filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1_1 = weight_variable([3, 3, 18, 64])\n",
    "b_conv1_1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "w_conv1_2 = weight_variable([3, 3, 64, 64])\n",
    "b_conv1_2 = bias_variable([64])\n",
    "\n",
    "h_conv1_1 = tf.nn.relu(conv2d(x_image, w_conv1_1) + b_conv1_1)\n",
    "h_conv1_2 = tf.nn.relu(conv2d(h_conv1_1, w_conv1_2) + b_conv1_2)\n",
    "h_pool1 = max_pool_2x2(h_conv1_1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2_1 = weight_variable([3, 3, 64, 64])\n",
    "b_conv2_1 = bias_variable([64])\n",
    "\n",
    "w_conv2_2 = weight_variable([3, 3, 64, 64])\n",
    "b_conv2_2 = bias_variable([64])\n",
    "\n",
    "h_conv2_1 = tf.nn.relu(conv2d(h_pool1, w_conv2_1) + b_conv2_1)\n",
    "h_conv2_2 = tf.nn.relu(conv2d(h_conv2_1, w_conv2_2) + b_conv2_2)\n",
    "h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "\n",
    "batch_size = 1000\n",
    "for i in range(80000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%250 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g, train loss %f\" %(i,  train_accuracy, loss_temp))\n",
    "#    if loss_temp < 1:\n",
    "#        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "#        print('Final:step %d, train loss %f'%(i,loss_temp))\n",
    "#        print (\"Final accuracy %g\" %train_accuracy)\n",
    "#        break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
